{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<style>\n",
    "h1 {\n",
    "    position: sticky;\n",
    "}\n",
    "h2, h3 {\n",
    "    position: sticky;\n",
    "    top: 0;\n",
    "    background-color: white;\n",
    "    z-index: 1000;\n",
    "    padding: 10px;\n",
    "    box-shadow: 1px 2px 5px rgba(0, 0, 0, 0);\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"RNN - Recurrent Neural Network\"\n",
    "subtitle: \"Bending Time with RNNs - How AI remembers\"\n",
    "title-block-banner: \"linear-gradient(43deg, #0a26ad 0%, #c468e0 46%,rgb(236, 113, 113) 100%)\"\n",
    "embed-resources: true\n",
    "\n",
    "format: \n",
    "   \n",
    "    html: \n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        toc-depth: 5\n",
    "        number-sections: false\n",
    "        code-fold: true\n",
    "        code-tools: true\n",
    "        favicon: \"1.png\"\n",
    "        include-in-header: include.html\n",
    "        code-line-numbers: true\n",
    "        theme: flatly\n",
    "        mermaid: \n",
    "            theme: default\n",
    "            align: center\n",
    "        grid:\n",
    "            sidebar-width: 400px\n",
    "            body-width: 1300px\n",
    "            margin-width: 1px\n",
    "            gutter-width: 1.5rem\n",
    "        \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://ai4sme.aisingapore.org/wp-content/uploads/2022/06/animated1.gif' width = 700><br> RNN</img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN stands for Recurrent Neural Network, which is a type of artificial neural network that processes sequential data. RNNs are used in a variety of applications, such as speech recognition and sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why ANN can't be used in sequential data?\n",
    "Artificial Neural Networks (ANNs) struggle with sequential data due to inherent structural limitations. Here's a breakdown of the key reasons:\n",
    "\n",
    "\n",
    "##### **Reason 1. Fixed Input/Output Size Requirement**\n",
    "In real life, sequential data (text, time series, sensor readings) often has variable lengths. For example:\n",
    " e.g., \n",
    "    <table>\n",
    "    <tr>\n",
    "    <th> Sequence</th> <th>Size</th></tr>\n",
    "    <tr>\n",
    "    <td> Python is a OOPS programming language </td> <td> 6 </td> </tr>\n",
    "    <tr><td> I Love India </td><td>3</td></tr>\n",
    "    <tr><td> I am playing football </td><td>4</td></tr>\n",
    "    </table>\n",
    "\n",
    "\n",
    "* Suppose you make an ANN having the below structure.\n",
    "* It has 3 input nodes.\n",
    "\n",
    "\n",
    "   <img src = 'https://lh3.googleusercontent.com/d/13sdaMqTjeJvpar38DRKHi_FPU8gDxZIB' width=500>\n",
    "\n",
    "\n",
    "* Our first sentence contains 6 words, hence the weight metrics will be 6 * 5 structure.\n",
    "* The second sentence contains 3 words hence the weight metrics will be 3 * 5 structure.\n",
    "* The third sentence contains 4 words hence the weight metrics will be 4 * 5 structure.\n",
    "\n",
    "We can see here the structure of the input weight metrics is changing based on the input text, which is not practical for designing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Reason 2. Zero Padding Unnecessary Computation**\n",
    "\n",
    "* To solve the first issue of varying length we can use the zero padding technique.\n",
    "* First, we can count the sentence having maximum words.\n",
    "* In our case we have the first sentence having a maximum of 6 number of words.\n",
    "* So we will fix our input text size to a maximum of 6 words.\n",
    "* In the second sentence, we have a number of words, as we have fixed our input to  6 words, but we have 3 words in 2nd sentence hence we will append 3 more vectors having zero values inside it.\n",
    "* Hence it is called **zero padding**.\n",
    "\n",
    "    <img src='https://lh3.googleusercontent.com/d/1MWF7loDs7ICUG4LEXeUCvPlbpuHqr5Ry'></img>\n",
    "\n",
    "\n",
    "* The problem with zero padding is that if we have the maximum word of a sentence is 1000 words.\n",
    "* Then we will fix the input length to 1000 nodes.\n",
    "* But if we got a sentence having only 5 words then for the rest of the 995 words we have to use zero padding.\n",
    "\n",
    "Which will take extra memory and computation power, decrease the training speed of the model and  undesirable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Reason 3. Prediction Problem On Different Input Length**\n",
    "\n",
    "* In our case, we have set our input length to 6 words while training the model.\n",
    "* But while predicting suppose we got an input text having the length of 10 words, at that time our model will fail.\n",
    "* Because we have trained our model with a fixed input size of 6 words, it will not be able to predict for 10 words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Reason 4. Not Considering Sequential Information**\n",
    "\n",
    "* ANN architecture does not take into account the sequence information of the input text.\n",
    "* When we pass the input text to the ANN model it will take all the input at a time.\n",
    "* When we enter vales at a time it will be mixed up inside the network, hence the sequence information is discarded.\n",
    "* The sequence information is discarded in the ANN model.\n",
    "* Hence it is not suitable for the sequential data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Reason 5. Lack of temporal memory**\n",
    "\n",
    "ANNs process inputs independently, with no mechanism to retain information from previous steps. This makes them unsuitable for tasks requiring context, such as:\n",
    "\n",
    "- Language: The word “lie” means different things in “never tell a lie” vs. “lie down”.\n",
    "\n",
    "- Time series: Predicting stock prices requires historical trends, not just isolated data points.\n",
    "\n",
    "Example: In the sentence “The cat chased the…”, ANNs cannot retain the context of “cat” to predict “mouse” as the next word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN Forward Propagation - Step by Step\n",
    "\n",
    "In forward propagation of an RNN (Recurrent Neural Network), the network processes input sequences step by step. At each time step $t$, it takes the current input $x_t$ and the hidden state from the previous step $h_{t−1}$, applies weights and activation functions (like tanh), and computes the new hidden state $h_t$. This hidden state is then used to predict the output $y_t$. \n",
    "\n",
    "The process repeats for each time step, allowing the RNN to capture temporal dependencies in sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*1w2z832C_B6xDovwm7ypJg.jpeg'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1. Problem Setup**\n",
    "We have a dataset of sentences:\n",
    "\n",
    "| Sequence | Size |\n",
    "|----------|------|\n",
    "| \"Show is nice\" | 3 |\n",
    "| \"Show is not nice\" | 4 |\n",
    "| \"Show is worst\" | 3 |\n",
    "\n",
    "Each word is represented using **One-Hot Encoding (OHE)**.\n",
    "\n",
    "### **2. Network Architecture**\n",
    "\n",
    "- **Input Layer:** 5 neurons (each representing a one-hot encoded word)\n",
    "- **Hidden Layer:** 3 neurons (processing sequential information)\n",
    "- **Output Layer:** 1 neuron (final prediction using softmax activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%{init: {\"flowchart\": {\"nodeSpacing\": 20, \"rankSpacing\": 40, 'height':1}}}%%\n",
    "graph LR\n",
    "    subgraph Inputs\n",
    "        direction LR\n",
    "        style Inputs fill:#a7bde0,stroke:#64b5f6,font-size:30,stroke-width:2px\n",
    "        x1[x<sub>1</sub>]\n",
    "        x2[x<sub>2</sub>]\n",
    "        x3[x<sub>3</sub>]\n",
    "        x4[x<sub>4</sub>]\n",
    "        x5[x<sub>5</sub>]\n",
    "    end\n",
    "    \n",
    "    subgraph \"hidden-layer\" [\"Hidden Layer\"]\n",
    "        direction LR\n",
    "        style hidden-layer fill:#a7e0b3,stroke:#85ff9f,font-size:30,stroke-width:2px\n",
    "        h1(h<sub>1</sub>)\n",
    "        h2(h<sub>2</sub>)\n",
    "        h3(h<sub>3</sub>)\n",
    "    end\n",
    "    \n",
    "    subgraph Output\n",
    "        direction LR\n",
    "        style Output fill:#e78383,stroke:#f8cc52,font-size:30,stroke-width:2px\n",
    "        y(y<sub>1</sub>)\n",
    "    end\n",
    "\n",
    "    x1 --> |w<sub>11</sub><sup>1</sup>| h1\n",
    "    x1 --> |w<sub>12</sub><sup>1</sup>| h2\n",
    "    x1 --> |w<sub>13</sub><sup>1</sup>| h3\n",
    "    x2 --> |w<sub>21</sub><sup>1</sup>| h1\n",
    "    x2 --> |w<sub>22</sub><sup>1</sup>| h2\n",
    "    x2 --> |w<sub>23</sub><sup>1</sup>| h3\n",
    "    x3 --> |w<sub>31</sub><sup>1</sup>| h1\n",
    "    x3 --> |w<sub>32</sub><sup>1</sup>| h2\n",
    "    x3 --> |w<sub>33</sub><sup>1</sup>| h3\n",
    "    x4 --> |w<sub>41</sub><sup>1</sup>| h1\n",
    "    x4 --> |w<sub>42</sub><sup>1</sup>| h2\n",
    "    x4 --> |w<sub>43</sub><sup>1</sup>| h3\n",
    "    x5 --> |w<sub>51</sub><sup>1</sup>| h1\n",
    "    x5 --> |w<sub>52</sub><sup>1</sup>| h2\n",
    "    x5 --> |w<sub>53</sub><sup>1</sup>| h3\n",
    "\n",
    "    h1 --> |w<sub>11</sub><sup>2</sup>| y\n",
    "    h2 --> |w<sub>21</sub><sup>2</sup>| y\n",
    "    h3 --> |w<sub>31</sub><sup>2</sup>| y\n",
    "\n",
    "    y --> Out([\"Softmax Activation\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. One-Hot Encoding Representation**\n",
    "Since we have five unique words {\"Show\", \"is\", \"nice\", \"worst\", \"not\"}, each word is a 5-dimensional vector:\n",
    "\n",
    "| Word  | One-Hot Encoding |\n",
    "|--------|-----------------|\n",
    "| Show  | `[1, 0, 0, 0, 0]` |\n",
    "| is    | `[0, 1, 0, 0, 0]` |\n",
    "| nice   | `[0, 0, 1, 0, 0]` |\n",
    "| worst    | `[0, 0, 0, 1, 0]` |\n",
    "| not    | `[0, 0, 0, 0, 1]` |\n",
    "\n",
    "Each word is now represented as a **5-dimensional vector**.\n",
    "\n",
    "### **4. Defining RNN Parameters**\n",
    "- **Weight matrices:**\n",
    "  - Input-to-Hidden weights $(W_x)$: `3 × 5` matrix\n",
    "  - Hidden-to-Hidden weights $(W_h)$: `3 × 3` matrix\n",
    "  - Bias $(b)$: `3 × 1` vector\n",
    "  - Hidden-to-Output weights $(W_y)$: `1 × 3` matrix\n",
    "  - Output bias $(b_y)$: `1 × 1` scalar\n",
    "  \n",
    "#### **Weight Matrices**\n",
    "$$\n",
    "W_x = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\\\ 1.1 & 1.2 & 1.3 & 1.4 & 1.5 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "W_h = \\begin{bmatrix} 0.9 & 0.8 & 0.7 \\\\ 0.6 & 0.5 & 0.4 \\\\ 0.3 & 0.2 & 0.1 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "b = \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "W_y = \\begin{bmatrix} 0.5 & 0.6 & 0.7 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "b_y = \\begin{bmatrix} 0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **5. Forward Propagation Formula**\n",
    "For each time step `t`:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "$$\n",
    "y_t = \\text{softmax}(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "  - $x_t$ = Input word (one-hot encoded vector of shape `5 × 1`)\n",
    "  - $h_t$ = Hidden state (`3 × 1`)\n",
    "  - $y_t$ = Output (`1 × 1` scalar)\n",
    "\n",
    "### **6. Forward Propagation Calculation**\n",
    "#### **Step 1: Processing First Word \"Show\" (t = 1)**\n",
    "\n",
    "##### **1. Input Vector for \"Show\"**\n",
    "$$\n",
    "x_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **2. Detailed calculation of $(W_x \\cdot x_1)$:**\n",
    "$$\n",
    "W_x x_1 = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\\\ 1.1 & 1.2 & 1.3 & 1.4 & 1.5 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} (0.1 \\times 1) + (0.2 \\times 0) + (0.3 \\times 0) + (0.4 \\times 0) + (0.5 \\times 0) \\\\ (0.6 \\times 1) + (0.7 \\times 0) + (0.8 \\times 0) + (0.9 \\times 0) + (1.0 \\times 0) \\\\ (1.1 \\times 1) + (1.2 \\times 0) + (1.3 \\times 0) + (1.4 \\times 0) + (1.5 \\times 0) \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 0.1 \\\\ 0.6 \\\\ 1.1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **3. Detailed calculation of $W_h \\cdot h_0$:**\n",
    "\n",
    "Since $h_0$ is initialized to zeros:\n",
    "$$\n",
    "W_h h_0 = \\begin{bmatrix} 0.9 & 0.8 & 0.7 \\\\ 0.6 & 0.5 & 0.4 \\\\ 0.3 & 0.2 & 0.1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **4. calculate $z_1$:**\n",
    "$$\n",
    "z_1 = W_x x_1 + W_h h_0 + b = \\begin{bmatrix} 0.1 \\\\ 0.6 \\\\ 1.1 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix} = \\begin{bmatrix} 0.2 \\\\ 0.7 \\\\ 1.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **5. Apply tanh activation**\n",
    "$$\n",
    "h_1 = \\tanh(z_1) = \\begin{bmatrix} \\tanh(0.2) \\\\ \\tanh(0.7) \\\\ \\tanh(1.2) \\end{bmatrix} \\approx \\begin{bmatrix} 0.198 \\\\ 0.604 \\\\ 0.833 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Step 2: Processing Second Word \"is\" (t = 2)**\n",
    "##### **1. Input Vector for \"is\"**\n",
    "$$\n",
    "x_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **2. Detailed calculation of $W_x \\cdot x_2$:**\n",
    "$$\n",
    "W_x  x_2 = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\\\ 1.1 & 1.2 & 1.3 & 1.4 & 1.5 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} (0.1 \\times 0) + (0.2 \\times 1) + (0.3 \\times 0) + (0.4 \\times 0) + (0.5 \\times 0) \\\\ (0.6 \\times 0) + (0.7 \\times 1) + (0.8 \\times 0) + (0.9 \\times 0) + (1.0 \\times 0) \\\\ (1.1 \\times 0) + (1.2 \\times 1) + (1.3 \\times 0) + (1.4 \\times 0) + (1.5 \\times 0) \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 0.2 \\\\ 0.7 \\\\ 1.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **3. Detailed calculation of $W_h \\cdot h_1$:**\n",
    "$$\n",
    "W_h h_1 = \\begin{bmatrix} 0.9 & 0.8 & 0.7 \\\\ 0.6 & 0.5 & 0.4 \\\\ 0.3 & 0.2 & 0.1 \\end{bmatrix} \\begin{bmatrix} 0.198 \\\\ 0.604 \\\\ 0.833 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} (0.9 \\times 0.198) + (0.8 \\times 0.604) + (0.7 \\times 0.833) \\\\ (0.6 \\times 0.198) + (0.5 \\times 0.604) + (0.4 \\times 0.833) \\\\ (0.3 \\times 0.198) + (0.2 \\times 0.604) + (0.1 \\times 0.833) \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 0.178 + 0.483 + 0.583 \\\\ 0.119 + 0.302 + 0.333 \\\\ 0.059 + 0.121 + 0.083 \\end{bmatrix} = \\begin{bmatrix} 1.244 \\\\ 0.754 \\\\ 0.263 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **4. calculate $z_2$:**\n",
    "$$\n",
    "z_2 = W_x x_2 + W_h h_1 + b = \\begin{bmatrix} 0.2 \\\\ 0.7 \\\\ 1.2 \\end{bmatrix} + \\begin{bmatrix} 1.244 \\\\ 0.754 \\\\ 0.263 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix} = \\begin{bmatrix} 1.544 \\\\ 1.554 \\\\ 1.563 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **5. Apply tanh activation**\n",
    "$$\n",
    "h_2 = \\tanh(z_2) = \\begin{bmatrix} \\tanh(1.544) \\\\ \\tanh(1.554) \\\\ \\tanh(1.563) \\end{bmatrix} \\approx \\begin{bmatrix} 0.911 \\\\ 0.914 \\\\ 0.917 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Step 3: Processing Third Word \"nice\" (t = 3)**\n",
    "\n",
    "##### **1. Input Vector for \"nice\"**\n",
    "$$\n",
    "x_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **2. Calculate $W_x \\cdot x_3$**\n",
    "$$\n",
    "W_x x_3 = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\\\ 1.1 & 1.2 & 1.3 & 1.4 & 1.5 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 0.3 \\\\ 0.8 \\\\ 1.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **3. Calculate $W_h \\cdot h_2$**\n",
    "From **Step 2**, we have:\n",
    "$$\n",
    "h_2 = \\begin{bmatrix} 0.911 \\\\ 0.914 \\\\ 0.917 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now compute:\n",
    "$$\n",
    "W_h h_2 = \\begin{bmatrix} 0.9 & 0.8 & 0.7 \\\\ 0.6 & 0.5 & 0.4 \\\\ 0.3 & 0.2 & 0.1 \\end{bmatrix} \\begin{bmatrix} 0.911 \\\\ 0.914 \\\\ 0.917 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 2.193 \\\\ 1.371 \\\\ 0.548 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **4. Calculate $z_3$**\n",
    "Add the bias:\n",
    "$$\n",
    "z_3 = W_x x_3 + W_h h_2 + b = \\begin{bmatrix} 0.3 \\\\ 0.8 \\\\ 1.3 \\end{bmatrix} + \\begin{bmatrix} 2.193 \\\\ 1.371 \\\\ 0.548 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix} 2.593 \\\\ 2.271 \\\\ 1.948 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **5. Apply tanh activation**\n",
    "$$\n",
    "h_3 = \\tanh(z_3) = \\begin{bmatrix} \\tanh(2.593) \\\\ \\tanh(2.271) \\\\ \\tanh(1.948) \\end{bmatrix} \\approx \\begin{bmatrix} 0.989 \\\\ 0.979 \\\\ 0.961 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Step 4: Output Calculation for \"nice\"**\n",
    "\n",
    "##### **1. Calculate $W_y \\cdot h_3$**\n",
    "$$\n",
    "W_y h_3 = \\begin{bmatrix} 0.5 & 0.6 & 0.7 \\end{bmatrix} \\begin{bmatrix} 0.989 \\\\ 0.979 \\\\ 0.961 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "= 0.495 + 0.587 + 0.673 = 1.755\n",
    "$$\n",
    "\n",
    "##### **2. Add output bias**\n",
    "$$\n",
    "W_y h_3 + b_y = 1.755 + 0.2 = 1.955\n",
    "$$\n",
    "\n",
    "##### **3. Apply softmax/sigmoid activation**\n",
    "$$\n",
    "y_{\\text{final}} = \\frac{1}{1 + e^{-1.955}} \\approx 0.876\n",
    "$$\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "1. The **hidden state** is updated at each time step using the input word and the previous hidden state.\n",
    "2. The output is computed using the final hidden state and passed through a **softmax/sigmoid activation**.\n",
    "3. This process can be repeated for any number of words in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#dcdede', 'fontSize': '25px', 'textWrapWidth': 200 }, 'viewBox': '0 0 1200 1200' }}%%\n",
    "graph LR\n",
    "    subgraph \"t_0\" [\"Time step t=0\"]\n",
    "        direction TB\n",
    "        style t_0 fill:#9199e1,stroke:#999988,stroke-width:2px,font-size:25px,color:#080b2c\n",
    "        \n",
    "        subgraph inputs0 [\"Input: Show\"]\n",
    "            direction LR\n",
    "            style inputs0 fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "            x0[\"x₀ = [1,0,0,0,0]\"]\n",
    "        end\n",
    "        \n",
    "        subgraph hidden0 [\"Hidden Layer\"]\n",
    "            direction LR\n",
    "            style hidden0 fill:#5fb48b,stroke:#85ff9f,stroke-width:2px\n",
    "            h0_1(h_01)\n",
    "            h0_2(h_02)\n",
    "            h0_3(h_03)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    subgraph \"t_1\" [\"Time step t=1\"]\n",
    "        direction TB\n",
    "        style t_1 fill:#e19f91,stroke:#999999,stroke-width:2px,font-size:25px,color:#080b2c\n",
    "        \n",
    "        subgraph inputs1 [\"Input: is\"]\n",
    "            direction LR\n",
    "            style inputs1 fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "            x1[\"x₁ = [0,1,0,0,0]\"]\n",
    "        end\n",
    "        \n",
    "        subgraph hidden1 [\"Hidden Layer\"]\n",
    "            direction LR\n",
    "            style hidden1 fill:#5fb48b,stroke:#85ff9f,stroke-width:2px\n",
    "            h1_1(h_01)\n",
    "            h1_2(h_02)\n",
    "            h1_3(h_03)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    subgraph \"t_2\" [\"Time step t=2\"]\n",
    "        direction TB\n",
    "        style t_2 fill:#c891e1,stroke:#999999,stroke-width:2px,font-size:25px,color:#080b2c\n",
    "        \n",
    "        subgraph inputs2 [\"Input: nice\"]\n",
    "            direction LR\n",
    "            style inputs2 fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "            x2[\"x₂ = [0,0,1,0,0]\"]\n",
    "        end\n",
    "        \n",
    "        subgraph hidden2 [\"Hidden Layer\"]\n",
    "            direction LR\n",
    "            style hidden2 fill:#5fb48b,stroke:#85ff9f,stroke-width:2px\n",
    "            h2_1(h_01)\n",
    "            h2_2(h_02)\n",
    "            h2_3(h_03)\n",
    "        end\n",
    "        \n",
    "        subgraph output [\"Output\"]\n",
    "            direction LR\n",
    "            style output fill:#85929e,stroke:#f8cc52,stroke-width:2px\n",
    "            y_final(y)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    %% Input to hidden connections with Wx\n",
    "    x0 -.->|W_x| h0_1\n",
    "    x0 -.->|W_x| h0_2\n",
    "    x0 -.->|W_x| h0_3\n",
    "    \n",
    "    x1 -.->|W_x| h1_1\n",
    "    x1 -.->|W_x| h1_2\n",
    "    x1 -.->|W_x| h1_3\n",
    "    \n",
    "    x2 -.->|W_x| h2_1\n",
    "    x2 -.->|W_x| h2_2\n",
    "    x2 -.->|W_x| h2_3\n",
    "    \n",
    "    %% Recurrent connections with Wh (Red)\n",
    "    h0_1 ===>|W_h| h1_1\n",
    "    h0_2 ===>|W_h| h1_1\n",
    "    h0_3 ===>|W_h| h1_1\n",
    "    \n",
    "    h0_1 ===>|W_h| h1_2\n",
    "    h0_2 ===>|W_h| h1_2\n",
    "    h0_3 ===>|W_h| h1_2\n",
    "    \n",
    "    h0_1 ===>|W_h| h1_3\n",
    "    h0_2 ===>|W_h| h1_3\n",
    "    h0_3 ===>|W_h| h1_3\n",
    "    \n",
    "    h1_1 ===>|W_h| h2_1\n",
    "    h1_2 ===>|W_h| h2_1\n",
    "    h1_3 ===>|W_h| h2_1\n",
    "    \n",
    "    h1_1 ===>|W_h| h2_2\n",
    "    h1_2 ===>|W_h| h2_2\n",
    "    h1_3 ===>|W_h| h2_2\n",
    "    \n",
    "    h1_1 ===>|W_h| h2_3\n",
    "    h1_2 ===>|W_h| h2_3\n",
    "    h1_3 ===>|W_h| h2_3\n",
    "    \n",
    "    %% Hidden to output connections (Red)\n",
    "    h2_1 ==> |W_y| y_final\n",
    "    h2_2 ==> |W_y| y_final\n",
    "    h2_3 ==> |W_y| y_final\n",
    "    \n",
    "    y_final --> Out([\"Softmax Activation\"])\n",
    "    \n",
    "    %% Bias connections are implied\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_1 (SimpleRNN)    (None, 3)                 27        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31 (124.00 Byte)\n",
      "Trainable params: 31 (124.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "model1 = Sequential()\n",
    "model1.add(SimpleRNN(3, input_shape=(None, 5), activation='tanh'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "# calculating total trainable parameters\n",
    "(5*3+3)+(3*3)+(3*1+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of RNN Architecture\n",
    "\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:1400/1*GcHaABnSiuZLB9nBN2lflg.png\" width=\"90%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 One-to-One\n",
    "\n",
    "- This is the standard neural network (not sequence-based).\n",
    "- Use case: Image classification\n",
    "\n",
    "    *e.g.,* <br>\n",
    "    **Input**: An image<br> **Output**: A label like \"cat\" or \"dog\"\n",
    "\n",
    "### 3.2 One-to-Many\n",
    "\n",
    "- A single input produces a sequence of outputs.\n",
    "- Use case: Image captioning\n",
    "\n",
    "    *e.g.,* <br>\n",
    "    **Input**: An image <br>\n",
    "    **Output**: A sequence of words describing the image\n",
    "\n",
    "### 3.3 Many-to-One\n",
    "\n",
    "- A sequence of inputs leads to a single output.\n",
    "- Use case: Sentiment analysis\n",
    "\n",
    "    *e.g.,* <br>\n",
    "    **Input**: A sentence like \"This movie is great\"<br>\n",
    "    **Output**: Sentiment label such as \"Positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Many-to-Many (Synchronized)\n",
    "- Each input has a corresponding output at every time step.\n",
    "- Input and output sequences are of the same length.\n",
    "- Use case: Part-of-speech tagging\n",
    "\n",
    "    *e.g.,* <br>\n",
    "    **Input**: \"The dog barked\"<br>\n",
    "    **Output**: \"DET NOUN VERB\"\n",
    "\n",
    "### 3.5 Many-to-Many (Unaligned / Encoder-Decoder)  \n",
    "- Input and output sequences are of different lengths.\n",
    "- Uses an encoder to read input, and a decoder to generate output.\n",
    "- Use case: Machine translation\n",
    "\n",
    "    *e.g.,* <br>\n",
    "    **Input**: Let's Run.<br>\n",
    "    **Output**: Chalo bhagte hain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backpropagation Through Time (BPTT) for a Simple RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%{init: {\"theme\":\"neutral\", \"flowchart\": {\"nodeSpacing\": 30, \"rankSpacing\": 60}}}%%\n",
    "flowchart LR\n",
    "\n",
    "    %% Time Step Labels\n",
    "    T0[\"<b>t = 1</b>\"]:::time --> A1\n",
    "    T1[\"<b>t = 2</b>\"]:::time --> A2\n",
    "    T2[\"<b>t = 3</b>\"]:::time --> A3\n",
    "\n",
    "    %% Forward flow\n",
    "    A0[\"O<sub>0</sub><br><i>h₀ = 0</i>\"]:::init --> A1[\"O<sub>1</sub><br><i>a₁ = tanh(W<sub>xh</sub>x₁ + W<sub>hh</sub>h₀ + b)</i>\"]:::cell --> A2[\"O<sub>2</sub><br><i>a₂ = tanh(W<sub>xh</sub>x₂ + W<sub>hh</sub>a₁ + b)</i>\"]:::cell --> A3[\"O<sub>3</sub><br><i>a₃ = tanh(W<sub>xh</sub>x₃ + W<sub>hh</sub>a₂ + b)</i>\"]:::cell --> Y[\"ŷ = sigmoid(W<sub>hy</sub>a₃ + b)\"]:::output --> L[\"Loss<br>L(ŷ, y)\"]:::loss\n",
    "\n",
    "    %% Inputs\n",
    "    X1[\"x₁ ∈ ℝ²\"]:::input --> A1\n",
    "    X2[\"x₂ ∈ ℝ²\"]:::input --> A2\n",
    "    X3[\"x₃ ∈ ℝ²\"]:::input --> A3\n",
    "\n",
    "    %% Weight dimension notes\n",
    "    A1 -.-> Wxh[\"W<sub>xh</sub> ∈ ℝ<sup>2×2</sup>\"]:::weight\n",
    "    A1 -.-> Whh[\"W<sub>hh</sub> ∈ ℝ<sup>2×2</sup>\"]:::weight\n",
    "    A3 -.-> Why[\"W<sub>hy</sub> ∈ ℝ<sup>1×2</sup>\"]:::weight\n",
    "\n",
    "    %% Backward flow (backprop)\n",
    "    L -.-> Yb[\"∂L/∂ŷ\"]:::grad --> A3b[\"∂L/∂a₃\"]:::grad --> A2b[\"∂L/∂a₂\"]:::grad --> A1b[\"∂L/∂a₁\"]:::grad\n",
    "\n",
    "    %% Dashed arrows for gradients\n",
    "    L -.-> Yb\n",
    "    Yb -.-> A3b\n",
    "    A3b -.-> A2b\n",
    "    A2b -.-> A1b\n",
    "\n",
    "    %% Styling\n",
    "    classDef cell fill:#b2fab4,stroke:#333,stroke-width:1px;\n",
    "    classDef input fill:#f9caca,stroke:#333,stroke-width:1px;\n",
    "    classDef output fill:#cce5ff,stroke:#333,stroke-width:1px;\n",
    "    classDef loss fill:#ffd9a0,stroke:#333,stroke-width:1px;\n",
    "    classDef weight fill:#f2f2f2,stroke:#bbb,stroke-dasharray: 4 2;\n",
    "    classDef grad fill:#ffe0e0,stroke:#cc0000,stroke-dasharray: 5 3;\n",
    "    classDef time fill:#f0f0f0,stroke:#999,stroke-dasharray: 5 5;\n",
    "    classDef init fill:#eeeeee,stroke:#999,stroke-dasharray: 2 2;\n",
    "\n",
    "    class T0,T1,T2 time\n",
    "    class A0 init\n",
    "    class A1,A2,A3 cell\n",
    "    class X1,X2,X3 input\n",
    "    class Y output\n",
    "    class L loss\n",
    "    class Wxh,Whh,Why weight\n",
    "    class Yb,A3b,A2b,A1b grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%{init: {\"theme\":\"neutral\", \"flowchart\": {\"nodeSpacing\": 30, \"rankSpacing\": 60}}}%%\n",
    "flowchart LR\n",
    "    %% Time Steps\n",
    "    T0(\"at <b>t = 1</b>\"):::time --> E3{{\"O<sub>1</sub><br><i>tanh</i>\"}}:::cell\n",
    "    T1(\"at <b>t = 2</b>\"):::time --> D3{{\"O<sub>2</sub><br><i>tanh</i>\"}}:::cell\n",
    "    T2(\"at <b>t = 3</b>\"):::time --> C1{{\"<b>O<sub>3</sub></b><br><i>tanh</i>\"}}:::cell\n",
    "\n",
    "    %% Output and Loss\n",
    "    A[\"<b>L</b><br><i>(Loss)</i>\"]:::loss --> B[\"<b>y<sub>pred</sub></b><br><i>sigmoid</i>\"]:::output\n",
    "    B --> C2([\"<b>W<sub>final</sub></b><br><i>(hidden → output)</i>\"]):::weight\n",
    "    B --> BY[\"<b>b<sub>y</sub></b>\"]:::bias\n",
    "    B --> C1\n",
    "\n",
    "    %% Layer t=2\n",
    "    C1 --> D1([\"X<sub>3</sub>\"]):::input\n",
    "    C1 --> D4([\"W<sub>h</sub><br>(h → h)\"]):::weight\n",
    "    C1 --> D2([\"W<sub>input</sub><br>(x → h)\"]):::weight\n",
    "    C1 --> D3\n",
    "    C1 --> BH[\"<b>b<sub>h</sub></b>\"]:::bias\n",
    "\n",
    "    %% Layer t=1\n",
    "    D3 --> E1([\"X<sub>2</sub>\"]):::input\n",
    "    D3 --> E4([\"W<sub>h</sub><br>(h → h)\"]):::weight\n",
    "    D3 --> E2([\"W<sub>input</sub><br>(x → h)\"]):::weight\n",
    "    D3 --> E3\n",
    "    D3 --> BH2[\"b<sub>h</sub>\"]:::bias\n",
    "\n",
    "    %% Layer t=0\n",
    "    E3 --> F1([\"X<sub>1</sub>\"]):::input\n",
    "    E3 --> F2([\"W<sub>input</sub><br>(x → h)\"]):::weight\n",
    "    E3 --> F3{{\"O<sub>0</sub><br><i>h₀ = 0</i>\"}}:::init\n",
    "    E3 --> F4([\"W<sub>h</sub><br>(h → h)\"]):::weight\n",
    "    E3 --> BH3[\"b<sub>h</sub>\"]:::bias\n",
    "\n",
    "    %% Class styling\n",
    "    classDef cell fill:#b2fab4,stroke:#333,stroke-width:1px\n",
    "    classDef input fill:#f9caca,stroke:#333,stroke-width:1px\n",
    "    classDef output fill:#cce5ff,stroke:#333,stroke-width:1px\n",
    "    classDef weight fill:#e0e0e0,stroke:#333,stroke-width:1px\n",
    "    classDef loss fill:#ffd9a0,stroke:#333,stroke-width:1px\n",
    "    classDef time fill:#f0f0f0,stroke:#999,stroke-dasharray: 5 5\n",
    "    classDef init fill:#eeeeee,stroke:#999,stroke-dasharray: 2 2\n",
    "    classDef bias fill:#fff2cc,stroke:#333,stroke-width:1px\n",
    "\n",
    "    %% Individual node styles\n",
    "    style A stroke:#D50000,fill:#D50000,color:#FFFFFF\n",
    "    style B fill:#FFFFFF\n",
    "    style C2 fill:#FFE0B2\n",
    "    style D2 fill:#BBDEFB\n",
    "    style E2 stroke:#000000,fill:#BBDEFB\n",
    "    style F2 fill:#BBDEFB,stroke:#424242\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A complete derivation of the backpropagation algorithm for a simple Recurrent Neural Network (RNN), using a 3-dimensional one-hot encoded input sequence over 3 time steps.\n",
    "\n",
    "### 4.1. RNN Architecture and Parameters\n",
    "\n",
    "Let:\n",
    "- Input dimension: $d = 3$\n",
    "- Hidden size: $h = 2$\n",
    "- Output size: $1$\n",
    "- Timesteps: $T = 3$\n",
    "\n",
    "#### Parameters:\n",
    "- $W_{\\text{input}} \\in \\mathbb{R}^{2 \\times 3}$: weights from input to hidden layer\n",
    "- $W_h \\in \\mathbb{R}^{2 \\times 2}$: recurrent weights (hidden to hidden)\n",
    "- $\\mathbf{b}_h \\in \\mathbb{R}^{2 \\times 1}$: hidden bias\n",
    "- $W_{\\text{final}} \\in \\mathbb{R}^{1 \\times 2}$: weights from hidden to output\n",
    "- $b_y \\in \\mathbb{R}$: output bias\n",
    "\n",
    "\n",
    "\n",
    "### 4.2. One-Hot Encoded Input Sequence\n",
    "\n",
    "The input sequence $\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3 \\in \\mathbb{R}^3$ is one-hot encoded:\n",
    "\n",
    "| Time Step | Input Vector $\\mathbf{x}_t$ |\n",
    "|-----------|-------------------------------|\n",
    "| $t = 1$ | $\\mathbf{x}_1 = [1, 0, 0]$    |\n",
    "| $t = 2$ | $\\mathbf{x}_2 = [0, 1, 0]$    |\n",
    "| $t = 3$ | $\\mathbf{x}_3 = [0, 0, 1]$    |\n",
    "\n",
    "These represent categorical input tokens mapped to one-hot vectors.\n",
    "\n",
    "\n",
    "\n",
    "### 4.3. Forward Pass Equations\n",
    "\n",
    "- Hidden state initialization:\n",
    "\n",
    "$$\n",
    "\\mathbf{O}_0 = \\mathbf{0} \\in \\mathbb{R}^2\n",
    "$$\n",
    "\n",
    "- For each $t = 1, 2, 3$:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_t = W_{\\text{input}} \\cdot \\mathbf{x}_t + W_h \\cdot \\mathbf{O}_{t-1} + \\mathbf{b}_h\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{O}_t = \\tanh(\\mathbf{a}_t)\n",
    "$$\n",
    "\n",
    "&ensp;&ensp;&ensp;such that,\n",
    "\n",
    "&ensp;&ensp;&ensp;  $\\mathbf{a}_1 = W_{\\text{input}} \\cdot \\mathbf{x}_1 + W_h \\cdot \\mathbf{O}_{0} + \\mathbf{b}_h$\n",
    "\n",
    "&ensp;&ensp;&ensp;  $\\mathbf{O}_1 = \\tanh(\\mathbf{a}_1)$\n",
    "\n",
    "&ensp;&ensp;&ensp;  $\\mathbf{a}_2 = W_{\\text{input}} \\cdot \\mathbf{x}_2 + W_h \\cdot \\mathbf{O}_{1} + \\mathbf{b}_h$\n",
    "\n",
    "&ensp;&ensp;&ensp;  $\\mathbf{O}_2 = \\tanh(\\mathbf{a}_2)$\n",
    "\n",
    "&ensp;&ensp;&ensp;  $\\mathbf{a}_3 = W_{\\text{input}} \\cdot \\mathbf{x}_3 + W_h \\cdot \\mathbf{O}_{2} + \\mathbf{b}_h$\n",
    "\n",
    "&ensp;&ensp;&ensp;  $\\mathbf{O}_3 = \\tanh(\\mathbf{a}_3)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Output computation (only after final hidden state):\n",
    "$$\n",
    "z_f = W_{\\text{final}} \\cdot \\mathbf{O}_3 + b_y\n",
    "$$\n",
    "$$\n",
    " \\hat{y} = \\sigma(z_f) = \\sigma(W_{\\text{final}} \\cdot \\mathbf{O}_3 + b_y)\n",
    "$$\n",
    "\n",
    "where, $\\sigma(z_f) = \\frac{1}{1 + e^{-z_f}}$ is the sigmoid activation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient flow in BPTT visualization\n",
    "```{mermaid}\n",
    "%%{init: {\"theme\":\"neutral\", \"flowchart\": {\"nodeSpacing\": 30, \"rankSpacing\": 60}}}%%\n",
    "flowchart LR\n",
    "    T0[\"at <b>t = 1</b>\"] --> E3{{\"O<sub>1</sub><br><i>tanh</i>\"}}\n",
    "    T1[\"at <b>t = 2</b>\"] --> D3{{\"O<sub>2</sub><br><i>tanh</i>\"}}\n",
    "    T2[\"at <b>t = 3</b>\"] --> C1{{\"<b>O<sub>3</sub></b><br><i>tanh</i>\"}}\n",
    "    A[\"<b>L</b><br><i>(Loss)</i>\"] --> B[\"<b>y<sub>pred</sub></b><br><i>sigmoid</i>\"]\n",
    "    B --> C2([\"<b>W<sub>final</sub></b><br><i>(hidden → output)</i>\"]) & BY[\"<b>b<sub>y</sub></b>\"] & C1\n",
    "    C1 --> D1([\"X<sub>3</sub>\"]) & D2([\"W<sub>input</sub><br>(x → h)\"]) & D3 & D4([\"W<sub>h</sub><br>(h → h)\"]) & BH[\"<b>b<sub>h</sub></b>\"]\n",
    "    D3 --> E1([\"X<sub>2</sub>\"]) & E2([\"W<sub>input</sub><br>(x → h)\"]) & E3 & E4([\"W<sub>h</sub><br>(h → h)\"]) & BH2[\"b<sub>h</sub>\"]\n",
    "    E3 --> F1([\"X<sub>1</sub>\"]) & F2([\"W<sub>input</sub><br>(x → h)\"]) & F3{{\"O<sub>0</sub><br><i>h₀ = 0</i>\"}} & F4([\"W<sub>h</sub><br>(h → h)\"]) & BH3[\"b<sub>h</sub>\"]\n",
    "    A -. \"∂L/∂y<sub>pred</sub>\" .-> B\n",
    "    B -. \"∂y<sub>pred</sub>/∂W<sub>final</sub>\" .-> C2\n",
    "    B -. \"∂y<sub>pred</sub>/∂b<sub>y</sub>\" .-> BY\n",
    "    B -. \"∂y<sub>pred</sub>/∂O₃\" .-> C1\n",
    "    C1 -. \"∂O₃/∂W<sub>h</sub>\" .-> D4\n",
    "    C1 -. \"∂O₃/∂W<sub>input</sub>\" .-> D2\n",
    "    C1 -. \"∂O₃/∂b<sub>h</sub>\" .-> BH\n",
    "    C1 -. \"∂O₃/∂O₂\" .-> D3\n",
    "    D3 -. \"∂O₂/∂W<sub>h</sub>\" .-> E4\n",
    "    D3 -. \"∂O₂/∂W<sub>input</sub>\" .-> E2\n",
    "    D3 -. \"∂O₂/∂b<sub>h</sub> \".-> BH2\n",
    "    D3 -. \"∂O₂/∂O₁\" .-> E3\n",
    "    E3 -. \"∂O₁/∂W<sub>h</sub>\" .-> F4\n",
    "    E3 -. \"∂O₁/∂W<sub>input</sub>\" .-> F2\n",
    "    E3 -. \"∂O₁/∂b<sub>h</sub> \".-> BH3\n",
    "     T0:::time\n",
    "     T0:::time\n",
    "     E3:::cell\n",
    "     T1:::time\n",
    "     T1:::time\n",
    "     D3:::cell\n",
    "     T2:::time\n",
    "     T2:::time\n",
    "     C1:::cell\n",
    "     A:::loss\n",
    "     B:::output\n",
    "     C2:::weight\n",
    "     BY:::bias\n",
    "     D1:::input\n",
    "     D2:::weight\n",
    "     D4:::weight\n",
    "     BH:::bias\n",
    "     E1:::input\n",
    "     E2:::weight\n",
    "     E4:::weight\n",
    "     BH2:::bias\n",
    "     F1:::input\n",
    "     F2:::weight\n",
    "     F3:::init\n",
    "     F3:::init\n",
    "     F4:::weight\n",
    "     BH3:::bias\n",
    "    classDef cell fill:#b2fab4,stroke:#333,stroke-width:1px\n",
    "    classDef input fill:#f9caca,stroke:#333,stroke-width:1px\n",
    "    classDef output fill:#cce5ff,stroke:#333,stroke-width:1px\n",
    "    classDef weight fill:#e0e0e0,stroke:#333,stroke-width:1px\n",
    "    classDef loss fill:#ffd9a0,stroke:#333,stroke-width:1px\n",
    "    classDef time fill:#f0f0f0,stroke:#999,stroke-dasharray: 5 5\n",
    "    classDef init fill:#eeeeee,stroke:#999,stroke-dasharray: 2 2\n",
    "    classDef bias fill:#fff2cc,stroke:#333,stroke-width:1px\n",
    "    style A fill:#D50000,color:#FFFFFF\n",
    "    style B fill:#FFFFFF\n",
    "    style C2 fill:#FFE0B2\n",
    "    style D2 fill:#BBDEFB\n",
    "    style E2 fill:#BBDEFB\n",
    "    style F2 fill:#BBDEFB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Loss Function (Binary Cross Entropy)\n",
    "\n",
    "Given target $y \\in \\{0, 1\\}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
    "$$\n",
    "\n",
    "### 4.5. Backward Pass: Output Layer Gradients\n",
    "#### **4.5.1 Start by computing the gradient with respect to output:**\n",
    "\n",
    "$\\displaystyle\\Rightarrow \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} = \\frac{\\partial}{\\partial \\hat{y}} \\left( - y \\log(\\hat{y}) - (1 - y)\\log(1 - \\hat{y}) \\right)\n",
    "= -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}$\n",
    "\n",
    "$\\displaystyle\\Rightarrow \\frac{\\partial \\mathcal{L}}{\\partial z_f} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{d\\hat{y}}{dz_f} = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})} \\cdot \\hat{y}(1 - \\hat{y}) = (\\hat{y} - y)$\n",
    "\n",
    "\n",
    "#### **4.5.2 Gradient w.r.t. output layer parameters:**\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{final}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{d\\hat{y}}{dz_f} \\cdot \\frac{\\partial z_f}{\\partial W_{\\text{final}}} = \\frac{\\partial \\mathcal{L}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial W_{\\text{final}}} = (\\hat{y} - y) \\cdot \\mathbf{O}_3$\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial b_y} =  \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{d\\hat{y}}{dz_f} \\cdot \\frac{\\partial z_f}{\\partial b_y} = (\\hat{y} - y)$\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{O}_3} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{d\\hat{y}}{dz_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} = (\\hat{y} - y) \\cdot W_{\\text{final}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Full Gradient Derivations (Chain Rule)\n",
    "\n",
    "We will now expand the gradients of $\\mathcal{L}$ w.r.t. $W_{\\text{input}}$, $W_h$, and $\\mathbf{b}_h$ **fully** via chain rule for all 3 timesteps.\n",
    "\n",
    "\n",
    "#### **4.6.1. Gradient w.r.t. $W_{\\text{input}}$**\n",
    "\n",
    "##### *At $t = 3$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{input}}} \\text{ from (t=3)}  = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial W_{\\text{input}}}$\n",
    "\n",
    "$\\hspace{5.3cm} = (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\cdot \\mathbf{x}_3$\n",
    "\n",
    "\n",
    "##### *At $t = 2$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{input}}} \\text{ from (t=2)}  = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial \\mathbf{\\mathbf{O}}_2}\\cdot \\frac{\\partial \\mathbf{O}_2}{\\partial \\mathbf{a}_2}\\cdot \\frac{\\partial \\mathbf{a}_2}{\\partial W_{\\text{input}}}$\n",
    "\n",
    "$\\hspace{5.3cm} =  (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot \\mathbf{x}_2$\n",
    "\n",
    "##### *At $t = 1$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{input}}} \\text{ from (t=1)} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial \\mathbf{\\mathbf{O}}_2}\\cdot \\frac{\\partial \\mathbf{O}_2}{\\partial \\mathbf{a}_2}\\cdot \\frac{\\partial \\mathbf{a}_2}{\\partial \\mathbf{\\mathbf{O}}_1}\\cdot \\frac{\\partial \\mathbf{O}_1}{\\partial \\mathbf{a}_1}\\cdot \\frac{\\partial \\mathbf{a}_1}{\\partial W_{\\text{input}}}$ \n",
    "\n",
    "$\\hspace{5.3cm}= (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_1^2) \\cdot \\mathbf{x}_1$\n",
    "\n",
    "##### Putting all together:\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{input}}} =\n",
    "(\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\left[ \\mathbf{x}_3 + W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot \\mathbf{x}_2 +  W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_1^2) \\cdot \\mathbf{x}_1\n",
    "\\right]$\n",
    "\n",
    "##### In general form:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{input}}} = \n",
    "\\sum_{t=1}^{T} \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_T}\\cdot \\frac{{\\partial \\mathbf{O}_T}}{\\partial \\mathbf{a}_T}\\cdot \n",
    "\\left[ \n",
    "\\prod_{k=t+1}^{T} \\left( \\frac{\\partial \\mathbf{a}_k}{\\partial \\mathbf{O}_{k-1}} \\cdot \\frac{\\partial \\mathbf{O}_{k-1}}{\\partial \\mathbf{a}_{k-1}} \\right) \n",
    "\\right] \n",
    "\\cdot \\frac{\\partial \\mathbf{a}_t}{\\partial W_{\\text{input}}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **4.6.2. Gradient w.r.t. $W_{h}$**\n",
    "\n",
    "##### *At $t = 3$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{h}} \\text{ from (t=3)}  = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial W_{h}}$\n",
    "\n",
    "$\\hspace{4.7cm} = (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\cdot \\mathbf{O}_2$\n",
    "\n",
    "\n",
    "##### *At $t = 2$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{h}} \\text{ from (t=2)}  = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial \\mathbf{\\mathbf{O}}_2}\\cdot \\frac{\\partial \\mathbf{O}_2}{\\partial \\mathbf{a}_2}\\cdot \\frac{\\partial \\mathbf{a}_2}{\\partial W_{h}}$\n",
    "\n",
    "$\\hspace{4.7cm} =  (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot \\mathbf{O}_1$\n",
    "\n",
    "##### *At $t = 1$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{h}} \\text{ from (t=1)} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial \\mathbf{\\mathbf{O}}_2}\\cdot \\frac{\\partial \\mathbf{O}_2}{\\partial \\mathbf{a}_2}\\cdot \\frac{\\partial \\mathbf{a}_2}{\\partial \\mathbf{\\mathbf{O}}_1}\\cdot \\frac{\\partial \\mathbf{O}_1}{\\partial \\mathbf{a}_1}\\cdot \\frac{\\partial \\mathbf{a}_1}{\\partial W_{h}}$\n",
    "\n",
    "$\\hspace{4.7cm} = (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_1^2) \\cdot \\mathbf{O}_0$\n",
    "\n",
    "##### Putting all together:\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_{h}} =\n",
    "(\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\left[ \\mathbf{O}_2 + W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot \\mathbf{O}_1 +  W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_1^2) \\cdot \\mathbf{O}_0\n",
    "\\right]$\n",
    "\n",
    "#### In general form:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{h}} = \n",
    "\\sum_{t=1}^{T} \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_T}\\cdot \\frac{{\\partial \\mathbf{O}_T}}{\\partial \\mathbf{a}_T}\\cdot \n",
    "\\left[ \n",
    "\\prod_{k=t+1}^{T} \\left( \\frac{\\partial \\mathbf{a}_k}{\\partial \\mathbf{O}_{k-1}} \\cdot \\frac{\\partial \\mathbf{O}_{k-1}}{\\partial \\mathbf{a}_{k-1}} \\right) \n",
    "\\right] \n",
    "\\cdot \\frac{\\partial \\mathbf{a}_t}{\\partial W_{h}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **4.6.3. Gradient w.r.t. $b_{h}$**\n",
    "\n",
    "##### *At $t = 3$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial b_{h}} \\text{ from (t=3)}  = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial b_{h}}$\n",
    "\n",
    "$\\hspace{4.4cm} = (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2)$\n",
    "\n",
    "\n",
    "##### *At $t = 2$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial b_{h}} \\text{ from (t=2)}  = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial \\mathbf{\\mathbf{O}}_2}\\cdot \\frac{\\partial \\mathbf{O}_2}{\\partial \\mathbf{a}_2}\\cdot \\frac{\\partial \\mathbf{a}_2}{\\partial b_{h}}$\n",
    "\n",
    "$\\hspace{4.4cm} =  (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_2^2) \n",
    "$\n",
    "\n",
    "##### *At $t = 1$:*\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial b_{h}} \\text{ from (t=1)} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_3} \\cdot \\frac{\\partial \\mathbf{O}_3}{\\partial \\mathbf{a}_3} \\cdot \\frac{\\partial \\mathbf{a}_3}{\\partial \\mathbf{\\mathbf{O}}_2}\\cdot \\frac{\\partial \\mathbf{O}_2}{\\partial \\mathbf{a}_2}\\cdot \\frac{\\partial \\mathbf{a}_2}{\\partial \\mathbf{\\mathbf{O}}_1}\\cdot \\frac{\\partial \\mathbf{O}_1}{\\partial \\mathbf{a}_1}\\cdot \\frac{\\partial \\mathbf{a}_1}{\\partial b_{h}}$\n",
    "\n",
    "$\\hspace{4.4cm} = (\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_1^2)$\n",
    "\n",
    "##### Putting all together:\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial b_{h}} =\n",
    "(\\hat{y} - y) \\cdot W_{\\text{final}} \\cdot (1 - \\mathbf{O}_3^2) \\left[ 1 + W_h \\cdot (1 - \\mathbf{O}_2^2) +  W_h \\cdot (1 - \\mathbf{O}_2^2) \\cdot W_h \\cdot (1 - \\mathbf{O}_1^2)  \\right]$\n",
    "\n",
    "##### In general form:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_{h}} = \n",
    "\\sum_{t=1}^{T} \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_f} \\cdot \\frac{\\partial z_f}{\\partial \\mathbf{O}_T}\\cdot \\frac{{\\partial \\mathbf{O}_T}}{\\partial \\mathbf{a}_T}\\cdot \n",
    "\\left[ \n",
    "\\prod_{k=t+1}^{T} \\left( \\frac{\\partial \\mathbf{a}_k}{\\partial \\mathbf{O}_{k-1}} \\cdot \\frac{\\partial \\mathbf{O}_{k-1}}{\\partial \\mathbf{a}_{k-1}} \\right) \n",
    "\\right] \n",
    "\\cdot \\frac{\\partial \\mathbf{a}_t}{\\partial b_{h}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4.7. Final Weight Updates (Gradient Descent)\n",
    "\n",
    "Using learning rate $\\eta$, we update all parameters:\n",
    "\n",
    "$\\displaystyle\\Rightarrow W_{\\text{input}} := W_{\\text{input}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{input}}}$\n",
    "\n",
    "$\\displaystyle\\Rightarrow W_h := W_h - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_h}$\n",
    "\n",
    "$\\displaystyle\\Rightarrow \\mathbf{b}_h := \\mathbf{b}_h - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_h}$\n",
    "\n",
    "$\\displaystyle\\Rightarrow W_{\\text{final}} := W_{\\text{final}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{final}}}$\n",
    "\n",
    "$\\displaystyle\\Rightarrow b_y := b_y - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b_y}$\n",
    "\n",
    "\n",
    "This is known as **Backpropagation Through Time (BPTT) for Many-to-One RNN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. many-to-many RNN with backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%{init: {\"theme\":\"neutral\", \"flowchart\": {\"nodeSpacing\": 40, \"rankSpacing\": 70}}}%%\n",
    "flowchart LR\n",
    "\n",
    "    %% Time Step Labels\n",
    "    T0[\"<b>t = 0</b>\"]:::time --> A1\n",
    "    T1[\"<b>t = 1</b>\"]:::time --> A2\n",
    "    T2[\"<b>t = 2</b>\"]:::time --> A3\n",
    "\n",
    "    %% Forward pass: Hidden states\n",
    "    A0[\"O<sub>0</sub><br><i>h₀ = 0</i>\"]:::init --> A1[\"O<sub>1</sub><br>a₁ = tanh(...) \"]:::cell --> A2[\"O<sub>2</sub><br>a₂ = tanh(...) \"]:::cell --> A3[\"O<sub>3</sub><br>a₃ = tanh(...) \"]:::cell\n",
    "\n",
    "    %% Inputs\n",
    "    X1[\"x₁ ∈ ℝ²\"]:::input --> A1\n",
    "    X2[\"x₂ ∈ ℝ²\"]:::input --> A2\n",
    "    X3[\"x₃ ∈ ℝ²\"]:::input --> A3\n",
    "\n",
    "    %% Outputs at each time step\n",
    "    A1 --> Y1[\"ŷ₁ = sigmoid(W<sub>hy</sub>a₁)\"]:::output --> L1[\"L₁ = L(ŷ₁, y₁)\"]:::loss\n",
    "    A2 --> Y2[\"ŷ₂ = sigmoid(W<sub>hy</sub>a₂)\"]:::output --> L2[\"L₂ = L(ŷ₂, y₂)\"]:::loss\n",
    "    A3 --> Y3[\"ŷ₃ = sigmoid(W<sub>hy</sub>a₃)\"]:::output --> L3[\"L₃ = L(ŷ₃, y₃)\"]:::loss\n",
    "\n",
    "    %% Total Loss\n",
    "    L1 --> SumL[\"<b>Total Loss:</b><br>L = L₁ + L₂ + L₃\"]:::loss\n",
    "    L2 --> SumL\n",
    "    L3 --> SumL\n",
    "\n",
    "    %% Gradients flowing back\n",
    "    L1 -.-> dY1[\"∂L₁/∂ŷ₁\"]:::grad -.-> dA1[\"∂L₁/∂a₁\"]:::grad\n",
    "    L2 -.-> dY2[\"∂L₂/∂ŷ₂\"]:::grad -.-> dA2[\"∂L₂/∂a₂\"]:::grad\n",
    "    L3 -.-> dY3[\"∂L₃/∂ŷ₃\"]:::grad -.-> dA3[\"∂L₃/∂a₃\"]:::grad\n",
    "\n",
    "    %% Time-distributed gradients into hidden layers\n",
    "    dA3 -.-> A2\n",
    "    dA2 -.-> A1\n",
    "    dA1 -.-> A0\n",
    "\n",
    "    %% Styling\n",
    "    classDef cell fill:#b2fab4,stroke:#333,stroke-width:1px;\n",
    "    classDef input fill:#f9caca,stroke:#333,stroke-width:1px;\n",
    "    classDef output fill:#cce5ff,stroke:#333,stroke-width:1px;\n",
    "    classDef loss fill:#ffd9a0,stroke:#333,stroke-width:1px;\n",
    "    classDef grad fill:#ffe0e0,stroke:#cc0000,stroke-dasharray: 5 3;\n",
    "    classDef time fill:#f0f0f0,stroke:#999,stroke-dasharray: 5 5;\n",
    "    classDef init fill:#eeeeee,stroke:#999,stroke-dasharray: 2 2;\n",
    "\n",
    "    class T0,T1,T2 time\n",
    "    class A0 init\n",
    "    class A1,A2,A3 cell\n",
    "    class X1,X2,X3 input\n",
    "    class Y1,Y2,Y3 output\n",
    "    class L1,L2,L3,SumL loss\n",
    "    class dY1,dY2,dY3,dA1,dA2,dA3 grad\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let’s now expand all the equations **step-by-step** for a **many-to-many RNN** with **T = 3 time steps**.\n",
    "\n",
    "We’ll use:\n",
    "\n",
    "- Hidden size = $H$\n",
    "- Input size = $I$\n",
    "- Output size = $O$\n",
    "\n",
    "For simplicity, assume scalar output and 1D hidden states, so we can avoid matrix notation and focus on clarity.\n",
    "\n",
    "\n",
    "\n",
    "### 🔧 Assumptions (for simplicity):\n",
    "\n",
    "- Scalar hidden state and scalar outputs (you can later vectorize).\n",
    "- Inputs: $x_1, x_2, x_3 \\in \\mathbb{R}$\n",
    "- Targets: $y_1, y_2, y_3 \\in \\mathbb{R}$\n",
    "- Initial hidden state: $h_0 = 0$\n",
    "- Recurrent weight: $W_h$\n",
    "- Input weight: $W_x$\n",
    "- Output weight: $W_y$\n",
    "- Biases: $b_h, b_y$\n",
    "\n",
    "\n",
    "\n",
    "### Forward Pass (Expanded for t = 1 to 3)\n",
    "\n",
    "#### 🔹 t = 1:\n",
    "$\\displaystyle\n",
    "a_1 = W_x x_1 + W_h h_0 + b_h = W_x x_1 + b_h\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "h_1 = \\tanh(a_1)\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "z_1 = W_y h_1 + b_y \n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "\\hat{y}_1 = \\phi(z_1)\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "\\mathcal{L}_1 = \\frac{1}{2} (\\hat{y}_1 - y_1)^2\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "#### 🔹 t = 2:\n",
    "$\\displaystyle\n",
    "a_2 = W_x x_2 + W_h h_1 + b_h\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "h_2 = \\tanh(a_2)\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "z_2 = W_y h_2 + b_y \n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "\\hat{y}_2 = \\phi(z_2)\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "\\mathcal{L}_2 = \\frac{1}{2} (\\hat{y}_2 - y_2)^2\n",
    "$\n",
    "\n",
    "\n",
    "#### 🔹 t = 3:\n",
    "$\\displaystyle\n",
    "a_3 = W_x x_3 + W_h h_2 + b_h\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "h_3 = \\tanh(a_3)\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "z_3 = W_y h_3 + b_y \n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "\\hat{y}_3 = \\phi(z_3)\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "\\mathcal{L}_3 = \\frac{1}{2} (\\hat{y}_3 - y_3)^2\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 🎯 Loss\n",
    "\n",
    "Let total loss be:\n",
    "$\\displaystyle\n",
    "\\mathcal{L} = \\frac{1}{2} (\\hat{y}_1 - y_1)^2 + \\frac{1}{2} (\\hat{y}_2 - y_2)^2 + \\frac{1}{2} (\\hat{y}_3 - y_3)^2\n",
    "$\n",
    "\n",
    "\n",
    "### Backward Pass – Step-by-Step \n",
    "\n",
    "\n",
    "\n",
    "###  Step 1: Gradient w.r.t. output weights\n",
    "\n",
    "$\\displaystyle\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = (\\hat{y}_1 - y_1) \\cdot h_1 + (\\hat{y}_2 - y_2) \\cdot h_2 + (\\hat{y}_3 - y_3) \\cdot h_3\n",
    "$\n",
    "\n",
    "$\\displaystyle\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_y} = (\\hat{y}_1 - y_1) + (\\hat{y}_2 - y_2) + (\\hat{y}_3 - y_3)\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "###  Step 2: Gradient w.r.t. hidden-to-hidden weight $W_h$\n",
    "\n",
    "We will compute:\n",
    "$\\displaystyle\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\frac{\\partial \\mathcal{L}_1}{\\partial W_h} + \\frac{\\partial \\mathcal{L}_2}{\\partial W_h} + \\frac{\\partial \\mathcal{L}_3}{\\partial W_h}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "#### 🔹 From $t = 3$:\n",
    "\n",
    "$\\displaystyle\\Rightarrow\n",
    "\\frac{\\partial \\mathcal{L}_3}{\\partial W_h} = \\left[\\frac{\\partial \\mathcal{L}_3}{\\partial \\hat{y}_3} \\cdot \\frac{\\partial \\hat{y}_3}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial h_3} \\cdot \\frac{\\partial h_3}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial W_h}\\right] + \\left[\\frac{\\partial \\mathcal{L}_3}{\\partial \\hat{y}_3} \\cdot \\frac{\\partial \\hat{y}_3}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial h_3} \\cdot \\frac{\\partial h_3}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial a_2}\\cdot \\frac{\\partial a_2}{\\partial W_h}\\right] + \\left[\\frac{\\partial \\mathcal{L}_3}{\\partial \\hat{y}_3} \\cdot \\frac{\\partial \\hat{y}_3}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial h_3} \\cdot \\frac{\\partial h_3}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial W_h}\\right]\n",
    "$\n",
    "\n",
    "\n",
    "Now expand each term:\n",
    "\n",
    "- $\\displaystyle\\frac{\\partial \\mathcal{L}_3}{\\partial \\hat{y}_3} \\cdot \\frac{\\partial \\hat{y}_3}{\\partial z_3} = (\\hat{y}_3 - y_3)$\n",
    "- $\\displaystyle\\frac{\\partial \\mathcal{L}_2}{\\partial \\hat{y}_2} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial z_2} = (\\hat{y}_2 - y_2)$\n",
    "- $\\displaystyle\\frac{\\partial z_3}{\\partial h_3} = W_y$\n",
    "- $\\displaystyle\\frac{\\partial z_2}{\\partial h_2} = W_y$\n",
    "- $\\displaystyle\\frac{\\partial h_3}{\\partial a_3} = (1 - h_3^2)$\n",
    "- $\\displaystyle\\frac{\\partial h_2}{\\partial a_2} = (1 - h_2^2)$\n",
    "- $\\displaystyle\\frac{\\partial a_3}{\\partial h_2} = W_h$\n",
    "- $\\displaystyle\\frac{\\partial h_2}{\\partial a_2} = 1 - h_2^2$\n",
    "- $\\displaystyle\\frac{\\partial a_2}{\\partial h_1} = W_h$\n",
    "- $\\displaystyle\\frac{\\partial h_1}{\\partial a_1} = 1 - h_1^2$\n",
    "- $\\displaystyle\\frac{\\partial a_1}{\\partial W_h} = h_0 = 0$\n",
    "- $\\displaystyle\\frac{\\partial a_2}{\\partial W_h} = h_1$\n",
    "- $\\displaystyle\\frac{\\partial a_3}{\\partial W_h} = h_2$\n",
    "- $\\displaystyle\\frac{\\partial a_2}{\\partial W_h} = h_1$\n",
    "\n",
    "So:\n",
    "\n",
    "$\\displaystyle \\Rightarrow\\frac{\\partial \\mathcal{L}_3}{\\partial W_h} = 0 + \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot W_h \\cdot (1 - h_2^2) \\cdot h_1\\right] + \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot h_2\\right]\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 🔹 From $t = 2$:\n",
    "\n",
    "$\\displaystyle\\Rightarrow\n",
    "\\frac{\\partial \\mathcal{L}_2}{\\partial W_h} = \\left[\\frac{\\partial \\mathcal{L}_2}{\\partial \\hat{y}_2} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial a_2}  \\cdot \\frac{\\partial a_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial W_h}\\right] + \\left[\\frac{\\partial \\mathcal{L}_2}{\\partial \\hat{y}_2} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial a_2}  \\cdot \\frac{\\partial a_2}{\\partial W_h}\\right] \n",
    "$\n",
    "\n",
    "\n",
    "$\\displaystyle \\Rightarrow\\frac{\\partial \\mathcal{L}_2}{\\partial W_h} = 0 + \\left[(\\hat{y}_2 - y_2) \\cdot W_y \\cdot (1 - h_2^2) \\cdot  h_1\\right] \n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "#### 🔹 From $t = 1$:\n",
    "\n",
    "$\\displaystyle\\Rightarrow\n",
    "\\frac{\\partial \\mathcal{L}_1}{\\partial W_h} = \\frac{\\partial \\mathcal{L}_1}{\\partial \\hat{y}_1} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial a_1}  \\cdot \\frac{\\partial a_1}{\\partial W_h}\n",
    "$\n",
    "\n",
    "\n",
    "$\\displaystyle\\Rightarrow\n",
    "\\frac{\\partial \\mathcal{L}_1}{\\partial W_h} = (\\hat{y}_1 - y_1) \\cdot W_y \\cdot (1 - h_1^2) \\cdot h_0 = 0\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Final $\\frac{\\partial \\mathcal{L}}{\\partial W_h}$ Expression:\n",
    "\n",
    "$\\displaystyle \\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\left[(\\hat{y}_2 - y_2) \\cdot W_y \\cdot (1 - h_2^2) \\cdot h_1\\right] + \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot h_2 \\right] + \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot W_h \\cdot (1 - h_2^2) \\cdot h_1\\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Gradient w.r.t. $W_x$\n",
    "\n",
    "We'll expand:\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}}{\\partial W_x} = \\frac{\\partial \\mathcal{L}_1}{\\partial W_x} + \\frac{\\partial \\mathcal{L}_2}{\\partial W_x} + \\frac{\\partial \\mathcal{L}_3}{\\partial W_x}\n",
    "$\n",
    "\n",
    "#### 🔹 From $t = 3$\n",
    "\n",
    "We have:\n",
    "\n",
    "$\\displaystyle\\Rightarrow \\frac{\\partial \\mathcal{L}_3}{\\partial W_x} = \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot \\frac{\\partial a_3}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial W_x}\\right] + \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot \\frac{\\partial a_3}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial W_x} \\right] + \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot \\frac{\\partial a_3}{\\partial W_x}\\right]\n",
    "$\n",
    "\n",
    "Breakdown:\n",
    "\n",
    "- $\\frac{\\partial a_3}{\\partial h_2} = W_h$\n",
    "- $\\frac{\\partial h_2}{\\partial a_2} = 1 - h_2^2$\n",
    "- $\\frac{\\partial a_2}{\\partial h_1} = W_h$\n",
    "- $\\frac{\\partial h_1}{\\partial a_1} = 1 - h_1^2$\n",
    "- $\\frac{\\partial a_1}{\\partial W_x} = x_1$\n",
    "- $\\frac{\\partial a_2}{\\partial W_x} = x_2$\n",
    "- $\\frac{\\partial a_3}{\\partial W_x} = x_3$\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}_3}{\\partial W_x} =\\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot W_h \\cdot (1 - h_2^2) \\cdot W_h \\cdot (1 - h_1^2) \\cdot x_1\\right] + \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot W_h \\cdot (1 - h_2^2) \\cdot x_2\\right] + \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot x_3\\right]$\n",
    "\n",
    "\n",
    "\n",
    "#### 🔹 From $t = 2$\n",
    "\n",
    "$\\displaystyle\\Rightarrow\\frac{\\partial \\mathcal{L}_2}{\\partial W_x} =\\left[(\\hat{y}_2 - y_2) \\cdot W_y \\cdot (1 - h_2^2) \\cdot W_h \\cdot (1 - h_1^2) \\cdot x_1 \\right]+ \\left[(\\hat{y}_2 - y_2) \\cdot W_y \\cdot (1 - h_2^2) \\cdot x_2\\right]$\n",
    "\n",
    "\n",
    "\n",
    "#### 🔹 From $t = 1$\n",
    "\n",
    "$\\displaystyle\\Rightarrow\n",
    "\\frac{\\partial \\mathcal{L}_1}{\\partial W_x} = (\\hat{y}_1 - y_1) \\cdot W_y \\cdot (1 - h_1^2) \\cdot x_1\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Final Expression for $\\frac{\\partial \\mathcal{L}}{\\partial W_x}$\n",
    "\n",
    "$\\displaystyle\\Rightarrow\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_x} =\\left[(\\hat{y}_1 - y_1) \\cdot W_y \\cdot (1 - h_1^2) \\cdot x_1 \\right]\\\\ + \\left[(\\hat{y}_2 - y_2) \\cdot W_y \\cdot (1 - h_2^2) \\cdot x_2 \\right]\\\\+ \\left[(\\hat{y}_2 - y_2) \\cdot W_y \\cdot (1 - h_2^2) \\cdot W_h \\cdot (1 - h_1^2) \\cdot x_1 \\right]\\\\+ \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot x_3 \\right]\\\\+\\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot W_h \\cdot (1 - h_2^2) \\cdot x_2 \\right] \\\\+ \\left[(\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot W_h \\cdot (1 - h_2^2) \\cdot W_h \\cdot (1 - h_1^2) \\cdot x_1\\right]\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Gradient w.r.t. Bias $b_h$\n",
    "\n",
    "Just replace each $x_i$ with 1 in the above expressions (because $\\frac{\\partial a_t}{\\partial b_h} = 1$)\n",
    "\n",
    "So:\n",
    "\n",
    "$\\displaystyle\\Rightarrow \\frac{\\partial \\mathcal{L}}{\\partial b_h} = (\\hat{y}_1 - y_1) \\cdot W_y \\cdot (1 - h_1^2) \\\\  + (\\hat{y}_2 - y_2) \\cdot W_y \\cdot (1 - h_2^2) \\\\ + (\\hat{y}_2 - y_2) \\cdot W_y \\cdot (1 - h_2^2) \\cdot W_h \\cdot (1 - h_1^2) \\\\ + (\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\\\ + (\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot W_h \\cdot (1 - h_2^2) \\\\ + (\\hat{y}_3 - y_3) \\cdot W_y \\cdot (1 - h_3^2) \\cdot W_h \\cdot (1 - h_2^2) \\cdot W_h \\cdot (1 - h_1^2)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RNN problems and their solutions \n",
    "\n",
    "### **6.1. Vanishing Gradient Problem**\n",
    "\n",
    "#### The issue:\n",
    "During **Backpropagation Through Time (BPTT)**, gradients are calculated by chaining partial derivatives through time steps. That chain includes terms like derivatives of activation functions (e.g., `tanh' = 1 - tanh²(x)`), which are **< 1**, leading to repeated multiplication of small numbers.\n",
    "\n",
    "#### Result:\n",
    "After many time steps, these gradients **shrink exponentially**, and the network stops learning from earlier time steps — even if they're important.\n",
    "\n",
    "#### 🛠 Mitigations:\n",
    "- **LSTM / GRU**: They maintain a memory cell with **gates** to decide what to keep or forget. These architectures are designed to **carry gradients across long distances** without vanishing.\n",
    "- **ReLU-based activations** (with caution): Some variants like ReLU (instead of tanh/sigmoid) help because their derivative is 1 (or 0), so gradients don't vanish as easily.\n",
    "\n",
    "\n",
    "### **6.2. Exploding Gradient Problem**\n",
    "\n",
    "#### What happens:\n",
    "When the weights in the RNN are large or poorly initialized, the gradient can grow **exponentially** through time steps.\n",
    "\n",
    "#### Effect:\n",
    "Loss becomes NaN, weights diverge, or model doesn’t converge.\n",
    "\n",
    "#### 🛠 Mitigation:\n",
    "- **Gradient Clipping**: Limit the gradient magnitude during backprop.\n",
    "    ```python\n",
    "    import tensorflow as tf\n",
    "\n",
    "    gradients = [tf.clip_by_value(grad, clip_value_min=-1.0, clip_value_max=1.0) for grad in gradients]\n",
    "    ```\n",
    "- Keeps training stable even in long sequences.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3. Long-Term Dependencies**\n",
    "\n",
    "#### Problem:\n",
    "\n",
    "Imagine you're reading this sentence:\n",
    "\n",
    "> \"The cat, which was chased by the dog, climbed the tree because it was scared.\"\n",
    "\n",
    "To understand what \"it\" refers to, you need to remember “cat”, even though there's a lot of information in between. That’s a long-term dependency — something that happened much earlier still affects what comes later.\n",
    "\n",
    "Even if vanishing gradients are controlled, simple RNNs are still bad at remembering **information from many steps ago**. This is due to their memoryless architecture — they overwrite their hidden state at every step.\n",
    "\n",
    "Each hidden state $\\mathbf{O}_t$ is a function of:\n",
    "\n",
    "$$\n",
    "\\mathbf{O}_t = \\tanh(\\mathbf{a}_t) = \\tanh \\left[W_{\\text{input}} \\cdot \\mathbf{x}_t + W_h \\cdot \\mathbf{O}_{t-1} + \\mathbf{b}_h\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Theoretically, $\\mathbf{O}_t$ should carry forward everything important. But in practice:\n",
    "\n",
    "- Each step overwrites the hidden state.\n",
    "\n",
    "- The gradients from early steps vanish or explode during backpropagation.\n",
    "\n",
    "That means:\n",
    "\n",
    "- The model can't learn to retain old information if it spans over many time steps.\n",
    "\n",
    "- It ends up focusing mostly on recent inputs.\n",
    "\n",
    "#### 🛠 Mitigation:\n",
    "- **LSTM**: It uses a **cell state** and **forget gates**, which help it remember key information for long durations.\n",
    "- **GRU**: Similar idea, fewer gates (update + reset), more efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **6.4. Slow Training / High Complexity**\n",
    "\n",
    "#### Why it's slow:\n",
    "- **Sequential nature**: RNNs must process one step at a time. Can’t leverage parallelism across time steps.\n",
    "- **Backprop Through Time**: Costly because gradients need to flow through each time step.\n",
    "\n",
    "#### 🛠 Mitigation:\n",
    "- **Sequence bucketing**: Group similar-length sequences to reduce padding.\n",
    "- **Truncated BPTT**: Backpropagate only for a limited number of past steps.\n",
    "- **Layer Normalization / Batch Norm** in RNNs.\n",
    "- Switch to **non-recurrent models** (like Transformers) where applicable.\n",
    "\n",
    "\n",
    "\n",
    "### **6.5. Difficult to Capture Global Context**\n",
    "\n",
    "#### Example:\n",
    "In text generation, an RNN might forget who the subject was several sentences ago, leading to mismatched verb tense or pronouns.\n",
    "\n",
    "#### 🛠 Mitigation:\n",
    "- **Attention Mechanisms**: Let the model **look back** at all previous time steps with learnable weights.\n",
    "    - E.g., in seq2seq models, attention lets the decoder choose relevant encoder states.\n",
    "\n",
    "- Transformers take this to the next level by **replacing recurrence** with **multi-head self-attention**, allowing **direct access to any part of the input sequence**.\n",
    "\n",
    "\n",
    "\n",
    "### **6.6. Difficult to Train Deep RNNs**\n",
    "\n",
    "#### Why:\n",
    "Stacking multiple RNN layers makes gradient flow worse, leading to instability.\n",
    "\n",
    "#### 🛠 Mitigation:\n",
    "- **Residual connections** across layers.\n",
    "- **Layer normalization**.\n",
    "- Use **GRU/LSTM**, which inherently manage better depth handling.\n",
    "\n",
    "\n",
    "\n",
    "###  **5.7. Summary Table:**\n",
    "\n",
    "| Problem                   | Description | Solution |\n",
    "|---------------------------|-------------|----------|\n",
    "| Vanishing Gradient        | Gradients shrink through time | LSTM / GRU, ReLU, Layer Norm |\n",
    "| Exploding Gradient        | Gradients grow exponentially | Gradient Clipping |\n",
    "| Long-Term Dependency      | Can’t retain long-range info | LSTM / GRU |\n",
    "| Slow Training             | Sequential nature limits parallelism | Truncated BPTT, Bucketing |\n",
    "| Hard to Capture Context   | Loss of long-range relevance | Attention, Transformers |\n",
    "| Deep RNNs hard to train   | Gradient instability | Residuals, Layer Norm, Gated Units |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RNN code example \n",
    "\n",
    "In this, we are using imdb dataset from keras dataset library. \n",
    "\n",
    "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode the pad token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train[0] =  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65] \n",
      " y_train[0] =  1\n",
      "X_train.shape =  (25000,) \n",
      " y_train.shape =  (25000,)\n",
      "X_test.shape =  (25000,) \n",
      " y_test.shape =  (25000,)\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "#1. importing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding\n",
    "\n",
    "# Load Data\n",
    "(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Printing 1st sample and shapes\n",
    "print(\"X_train[0] = \",X_train[0][:10], \"\\n\", \"y_train[0] = \", y_train[0])\n",
    "print(\"X_train.shape = \", X_train.shape, \"\\n\", \"y_train.shape = \", y_train.shape)\n",
    "print(\"X_test.shape = \", X_test.shape, \"\\n\", \"y_test.shape = \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "#2. Checking if data is balanced or not\n",
    "\n",
    "import pandas as pd\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 50)\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "#3. Padding to make equal length\n",
    "\n",
    "import numpy as np\n",
    "max_len = 50\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n",
    "\n",
    "X_train = np.array(X_train, dtype='int32')\n",
    "y_train = np.array(y_train, dtype='int32')\n",
    "X_test = np.array(X_test, dtype='int32')\n",
    "y_test = np.array(y_test, dtype='int32')\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 50, 32)            320032    \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322145 (1.23 MB)\n",
      "Trainable params: 322145 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "#4. Preparing model\n",
    "\n",
    "max_vocab = 10000\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_vocab + 1,  # +1 for padding token\n",
    "              output_dim=32,\n",
    "              input_length = max_len, \n",
    "              mask_zero=True),  # mask_zero tells RNN to ignore padding values\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Embedding layer**\n",
    "Here, vocabulary_size = 10000, embedding_dim = 32\n",
    "```\n",
    "(10000 + 1) * 32 = 10001 * 32 = 320032\n",
    "```\n",
    "\n",
    "2. **Hidden layer**\n",
    "- Input-to-hidden weights (W_xh): (input_dim, units) = ```(32, 32) → 32 * 32 = 1024.```\n",
    "- Hidden-to-hidden weights (W_hh): (units, units) = ```(32, 32) → 32 * 32 = 1024.```\n",
    "- Bias (b_h): (units,) = ``(32,) → 32``\n",
    "\n",
    "- Total params = ```1024  + 1024  + 32 = 2080.```\n",
    "\n",
    "3. **Output layer** = \n",
    " ``32 (weights) + 1 (bias) = 33``\n",
    "\n",
    "```python\n",
    "Total trainable parameters = 320032 + 2080 + 33 = 322,145\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 26s 30ms/step - loss: 0.5095 - accuracy: 0.7376 - val_loss: 0.4301 - val_accuracy: 0.8026\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.3276 - accuracy: 0.8614 - val_loss: 0.4414 - val_accuracy: 0.8016\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.1897 - accuracy: 0.9291 - val_loss: 0.5418 - val_accuracy: 0.7896\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0772 - accuracy: 0.9743 - val_loss: 0.6882 - val_accuracy: 0.7742\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 21s 26ms/step - loss: 0.0351 - accuracy: 0.9896 - val_loss: 0.8471 - val_accuracy: 0.7860\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: False\n",
    "# 5. Train the model\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is there because val_accuracy is decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gradient Flow Visualization\n",
    "\n",
    "```{mermaid}\n",
    "%%{init: {\"theme\":\"neutral\", \"flowchart\": {\"nodeSpacing\": 15, \"rankSpacing\": 70, \"height\":1}}}%%\n",
    "graph LR\n",
    "    subgraph Inputs\n",
    "        direction LR\n",
    "        style Inputs fill:#a7bde0,stroke:#64b5f6,font-size:30,stroke-width:2px\n",
    "        x1[\"x<sub>t-1</sub> (Token ID)\"]\n",
    "        x2[\"x<sub>t</sub> (Token ID)\"]\n",
    "        x3[\"x<sub>t+1</sub> (Token ID)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph Embedding\n",
    "        direction LR\n",
    "        style Embedding fill:#d9b3e6,stroke:#ba68c8,font-size:30,stroke-width:2px\n",
    "        e1[\"e<sub>t-1</sub>\"]\n",
    "        e2[\"e<sub>t</sub>\"]\n",
    "        e3[\"e<sub>t+1</sub>\"]\n",
    "    end\n",
    "    \n",
    "    subgraph RNN\n",
    "        direction LR\n",
    "        style RNN fill:#a7e0b3,stroke:#85ff9f,font-size:30,stroke-width:2px\n",
    "        h1(\"h<sub>t-1</sub>\")\n",
    "        h2(\"h<sub>t</sub>\")\n",
    "        h3(\"h<sub>t+1</sub>\")\n",
    "    end\n",
    "    \n",
    "    subgraph Output\n",
    "        direction TB\n",
    "        style Output fill:#e78383,stroke:#f8cc52,font-size:30,stroke-width:2px\n",
    "        y(\"ŷ \\n(Sentiment)\")\n",
    "    end\n",
    "\n",
    "    %% Input to Embedding\n",
    "    x1 --> |Lookup| e1\n",
    "    x2 --> |Lookup| e2\n",
    "    x3 --> |Lookup| e3\n",
    "    \n",
    "    %% RNN Connections\n",
    "    e1 --> |W<sub>e</sub>| h1\n",
    "    h1 --> |W<sub>h</sub>| h2\n",
    "    e2 --> |W<sub>e</sub>| h2\n",
    "    h2 --> |W<sub>h</sub>| h3\n",
    "    e3 --> |W<sub>e</sub>| h3\n",
    "    \n",
    "    %% Final hidden to Output\n",
    "    h3 --> |W<sub>y</sub>| y\n",
    "    y --> Loss\n",
    "\n",
    "    %% Optional: Recurrent arrows (dashed)\n",
    "    h1 -.-> |h<sub>t-1</sub>| h2\n",
    "    h2 -.-> |h<sub>t</sub>| h3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Backpropagation in This RNN Model\n",
    "\n",
    "####  Workflow:\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - Inputs (word indices) are converted to vectors using the **Embedding layer**.\n",
    "   - These vectors are passed through the **SimpleRNN**.\n",
    "   - The output goes into a **Dense layer with sigmoid**, producing the final prediction.\n",
    "\n",
    "2. **Loss is computed** using binary cross-entropy.\n",
    "\n",
    "3. **Backward Pass (Backpropagation Through Time - BPTT):**\n",
    "   - Gradients from the loss are passed backward through the Dense → RNN → Embedding layers.\n",
    "   - Gradients flow from:\n",
    "     - Output → Dense layer weights  \n",
    "     - Dense → RNN weights (input, recurrent)  \n",
    "     - RNN → Embedding layer\n",
    "\n",
    "\n",
    "####  How Embeddings Get Updated\n",
    "\n",
    "- The **Embedding layer acts like a lookup table**: each word index corresponds to a row vector (of 32 dimensions here).\n",
    "- During backpropagation:\n",
    "  - Only the **vectors for the words present in the input** are updated.\n",
    "  - Gradients flow from the RNN into the embedding vectors.\n",
    "  - This is just like updating weights in any other layer — embeddings are trainable by default.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
