{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch Normalization: A Comprehensive Guide**\n",
    "\n",
    "#### **Why Batch Normalization?**\n",
    "Batch Normalization (BatchNorm) is a technique used to **speed up training**, **stabilize deep networks**, and **improve generalization**. It addresses the following issues:\n",
    "\n",
    "- **Internal Covariate Shift**: The distribution of hidden activations changes during training, slowing convergence.\n",
    "- **Vanishing/Exploding Gradients**: Helps maintain stable gradients by normalizing activations.\n",
    "- **Sensitivity to Initialization**: Reduces dependence on weight initialization.\n",
    "- **Regularization Effect**: Can reduce the need for dropout and other regularization techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation of Batch Normalization**\n",
    "\n",
    "Consider an input **mini-batch** $X = \\{x_1, x_2, \\dots, x_m\\}$ from an intermediate layer of a neural network. The batch normalization process follows these steps:\n",
    "\n",
    "#### **Step 1: Compute Batch Statistics**\n",
    "For a given mini-batch, compute the mean and variance:\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "$$\n",
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "$$\n",
    "where:\n",
    "- $\\mu_B$ is the batch mean.\n",
    "- $\\sigma_B^2$ is the batch variance.\n",
    "- $m$ is the batch size.\n",
    "\n",
    "#### **Step 2: Normalize the Batch**\n",
    "Normalize each input $x_i$ using:\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "where $\\epsilon$ is a small constant to prevent division by zero.\n",
    "\n",
    "#### **Step 3: Scale and Shift**\n",
    "To allow the network to learn optimal representations, introduce two trainable parameters, **scale** $\\gamma$ and **shift** $\\beta$:\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "- $\\gamma$ (scale) and $\\beta$ (shift) allow the model to recover the original distribution if needed.\n",
    "- $y_i$ is the final output after batch normalization.\n",
    "\n",
    "\n",
    "\n",
    "### **Batch Normalization in Neural Networks**\n",
    "Batch Normalization is typically applied **before** or **after activation functions** in hidden layers. Consider a fully connected neural network:\n",
    "\n",
    "1. Compute **linear transformation**: $z = Wx + b$\n",
    "2. Apply **Batch Normalization**: Normalize $z$ using batch statistics.\n",
    "3. Apply **activation function**: $a = f(y)$\n",
    "\n",
    "For Convolutional Neural Networks (CNNs), BatchNorm is applied **per channel** across spatial dimensions.\n",
    "\n",
    "\n",
    "\n",
    "### **Effect on Gradient Descent**\n",
    "Batch Normalization modifies weight updates in gradient descent by ensuring stable activations.\n",
    "\n",
    "#### **Gradient Updates with BatchNorm**\n",
    "Using the chain rule, the gradients of the loss function $L$ w.r.t. $x_i$ are modified as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\left( \\frac{\\partial L}{\\partial y_i} \\cdot \\gamma - \\frac{1}{m} \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\cdot \\gamma - \\hat{x}_i \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\cdot \\gamma \\cdot \\hat{x}_j \\right)\n",
    "$$\n",
    "\n",
    "This normalization process stabilizes weight updates, leading to faster training.\n",
    "\n",
    "\n",
    "\n",
    "### **Example: Applying BatchNorm in a Small ANN**\n",
    "\n",
    "#### **Consider a Neural Network with One Hidden Layer**\n",
    "- **Input layer**: 2 neurons\n",
    "- **Hidden layer**: 3 neurons (ReLU activation)\n",
    "- **Output layer**: 1 neuron (Sigmoid activation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Inputs\n",
    "        direction LR\n",
    "        style Inputs fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "        x1[x<sub>11</sub>]\n",
    "        x2[x<sub>12</sub>]\n",
    "    end\n",
    "    \n",
    "    subgraph \"hidden-layer\" [\"Hidden \\nLayer 1\"]\n",
    "        direction LR\n",
    "        style hidden-layer fill:#2c5c36,stroke:#85ff9f,stroke-width:2px\n",
    "        h1(b<sub>11</sub>)\n",
    "        h2(b<sub>12</sub>)\n",
    "        h3(b<sub>13</sub>)\n",
    "    end\n",
    "    \n",
    "    subgraph Output\n",
    "        direction LR\n",
    "        style Output fill:#e78383,stroke:#f8cc52,stroke-width:2px\n",
    "        y(b<sub>21</sub>)\n",
    "    end\n",
    "\n",
    "\n",
    "    x1 --> |w<sub>11</sub><sup>1</sup>| h1\n",
    "    x1 --> |w<sub>12</sub><sup>1</sup>| h2\n",
    "    x1 --> |w<sub>13</sub><sup>1</sup>| h3\n",
    "    x2 --> |w<sub>21</sub><sup>1</sup>| h1\n",
    "    x2 --> |w<sub>22</sub><sup>1</sup>| h2\n",
    "    x2 --> |w<sub>23</sub><sup>1</sup>| h3\n",
    "    h1 --> |w<sub>11</sub><sup>2</sup>| y\n",
    "    h2 --> |w<sub>21</sub><sup>2</sup>| y\n",
    "    h3 --> |w<sub>31</sub><sup>2</sup>| y\n",
    "    y --> Out([\"Prediction\"])\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Inputs\n",
    "        direction LR\n",
    "        style Inputs fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "        x1[x<sub>11</sub>]\n",
    "        x2[x<sub>12</sub>]\n",
    "    end\n",
    "    \n",
    "    subgraph \"hidden-layer\" [\"Hidden \\nLayer 1\"]\n",
    "        direction LR\n",
    "        style hidden-layer fill:#2c5c36,stroke:#85ff9f,stroke-width:2px\n",
    "        h1(b<sub>11</sub>)\n",
    "        h2(b<sub>12</sub>)\n",
    "        h3(b<sub>13</sub>)\n",
    "    end\n",
    "    \n",
    "    subgraph Output\n",
    "        direction LR\n",
    "        style Output fill:#e78383,stroke:#f8cc52,stroke-width:2px\n",
    "        y(b<sub>21</sub>)\n",
    "    end\n",
    "\n",
    "\n",
    "    x1 --> |w<sub>11</sub><sup>1</sup>| h1\n",
    "    x1 --> |w<sub>12</sub><sup>1</sup>| h2\n",
    "    x1 --> |w<sub>13</sub><sup>1</sup>| h3\n",
    "    x2 --> |w<sub>21</sub><sup>1</sup>| h1\n",
    "    x2 --> |w<sub>22</sub><sup>1</sup>| h2\n",
    "    x2 --> |w<sub>23</sub><sup>1</sup>| h3\n",
    "    h1 --> |w<sub>11</sub><sup>2</sup>| y\n",
    "    h2 --> |w<sub>21</sub><sup>2</sup>| y\n",
    "    h3 --> |w<sub>31</sub><sup>2</sup>| y\n",
    "    y --> Out([\"Prediction\"])\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[(Zbiór Danych\\nA)] --> B[Przetwarzanie]\n",
    "    subgraph Analiza Danych\n",
    "        B --> C[Analiza] --> D[Modelowanie]\n",
    "    end\n",
    "    D --> E[(Zbiór Danych\\nB)] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Forward Pass with BatchNorm**\n",
    "1. Compute linear transformation: $Z^{(1)} = W^{(1)}X + b^{(1)}$\n",
    "2. Apply Batch Normalization:\n",
    "   $$\n",
    "   \\hat{Z}^{(1)} = \\frac{Z^{(1)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "   $$\n",
    "   $$\n",
    "   Y^{(1)} = \\gamma \\hat{Z}^{(1)} + \\beta\n",
    "   $$\n",
    "3. Apply activation function: $A^{(1)} = ReLU(Y^{(1)})$\n",
    "4. Compute output layer: $\\hat{y} = \\sigma(W^{(2)} A^{(1)} + b^{(2)})$\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of Batch Normalization**\n",
    "- **Improved Training Speed**: Allows for higher learning rates without instability.\n",
    "- **Reduced Sensitivity to Initialization**: Networks train effectively even with suboptimal initial weights.\n",
    "- **Regularization Effect**: Reduces the need for dropout by controlling activation magnitudes.\n",
    "- **Better Gradient Flow**: Prevents issues related to vanishing/exploding gradients.\n",
    "- **Invariance to Input Scaling**: Model becomes less sensitive to data preprocessing choices.\n",
    "\n",
    "### **Disadvantages and Considerations**\n",
    "- **Additional Computation**: Increases computation due to mean and variance calculations.\n",
    "- **Incompatibility with Small Batch Sizes**: If the batch size is too small, estimated statistics may be unstable.\n",
    "- **Dependence on Batch Size**: Performance can vary significantly with different batch sizes.\n",
    "\n",
    "### **Alternative Normalization Techniques**\n",
    "- **Layer Normalization**: Normalizes activations across features instead of batches.\n",
    "- **Instance Normalization**: Commonly used in style transfer.\n",
    "- **Group Normalization**: Divides channels into groups and normalizes within each group.\n",
    "\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **Batch Normalization normalizes inputs** at each layer to maintain a stable distribution.\n",
    "- **Reduces dependence on initialization**, allowing deeper networks to train efficiently.\n",
    "- **Acts as a regularizer**, reducing the need for dropout.\n",
    "- **Improves gradient flow** by stabilizing updates, preventing vanishing/exploding gradients.\n",
    "\n",
    "BatchNorm is a powerful tool that significantly enhances training efficiency and generalization in deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Understanding Internal Covariate Shift**\n",
    "Internal Covariate Shift (ICS) refers to the phenomenon where the distribution of inputs to each layer in a deep neural network changes during training. This happens due to weight updates across layers, causing instability and requiring lower learning rates for convergence.\n",
    "\n",
    "Effects of ICS:\n",
    "- Slower convergence due to shifting input distributions.\n",
    "- Requires careful weight initialization to maintain stable learning.\n",
    "- Increases the risk of vanishing/exploding gradients in deep networks.\n",
    "\n",
    "Batch Normalization mitigates ICS by ensuring that each layer receives inputs with a stable distribution, enabling faster training and more robust optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How Do We Get $\\gamma$ and $\\beta$?**\n",
    "\n",
    "#### **1. Initialization**\n",
    "- $\\gamma$ is typically initialized to **1** so that the initial transformation does not scale the normalized values.\n",
    "- $\\beta$ is initialized to **0** to keep the mean centered at zero initially.\n",
    "\n",
    "#### **2. Learning Process**\n",
    "- Both $\\gamma$ and $\\beta$ are **trainable parameters**, meaning they are updated using backpropagation.\n",
    "- The gradients of the loss function w.r.t. $\\gamma$ and $\\beta$ are computed, and their values are updated using gradient descent:\n",
    "  \n",
    "  $$\n",
    "  \\gamma \\leftarrow \\gamma - \\eta \\frac{\\partial L}{\\partial \\gamma}\n",
    "  $$\n",
    "  $$\n",
    "  \\beta \\leftarrow \\beta - \\eta \\frac{\\partial L}{\\partial \\beta}\n",
    "  $$\n",
    "  \n",
    "  where $\\eta$ is the learning rate.\n",
    "\n",
    "#### **3. Effect of $\\gamma$ and $\\beta$**\n",
    "- If $\\gamma = 1$ and $\\beta = 0$, BatchNorm behaves like a standard normalization layer.\n",
    "- The model learns optimal values of $\\gamma$ and $\\beta$ that help in achieving the best representations.\n",
    "\n",
    "BatchNorm is a powerful tool that significantly enhances training efficiency and generalization in deep learning models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
