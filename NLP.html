<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>NLP</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="NLP_files/libs/clipboard/clipboard.min.js"></script>
<script src="NLP_files/libs/quarto-html/quarto.js"></script>
<script src="NLP_files/libs/quarto-html/popper.min.js"></script>
<script src="NLP_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="NLP_files/libs/quarto-html/anchor.min.js"></script>
<link href="NLP_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="NLP_files/libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="NLP_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="NLP_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="NLP_files/libs/bootstrap/bootstrap-a05fb3d2c69e2896b3ecb69b599a6d65.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">NLP</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Natural language processing (NLP) is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language.</p>
<p><img src="https://cdn.prod.website-files.com/5ec6a20095cdf182f108f666/5f22908f09f2341721cd8901_AI%20poster.png" width="40%"></p>
<section id="why-it-is-needed" class="level2">
<h2 class="anchored" data-anchor-id="why-it-is-needed">Why it is needed?</h2>
<p>Natural Language Processing (NLP) is essential because it bridges the gap between human communication and machine understanding, enabling computers to process and analyze human language effectively. Here are the key reasons why NLP is needed, along with examples:</p>
<p><strong>1. Understanding Human Language</strong><br> Computers inherently do not understand human languages, which are complex, context-dependent, and full of nuances like sarcasm, idioms, and dialects. NLP enables machines to interpret and generate human language for meaningful interaction.<br> <strong>e.g.,</strong> Virtual assistants like Alexa and Siri use NLP to interpret spoken commands and provide relevant responses.</p>
<p><strong>2. Automating Repetitive Tasks</strong><br> NLP automates tasks such as data entry, document classification, and summarization, saving time and reducing errors.<br> <strong>e.g.,</strong> Customer service chatbots handle routine queries, freeing human agents for complex issues.</p>
<p><strong>3. Analyzing Unstructured Data</strong><br> A significant portion of data is unstructured (e.g., social media posts, reviews). NLP extracts insights from this data for decision-making.<br> <strong>e.g.,</strong> Sentiment analysis helps businesses understand customer opinions from reviews or tweets.</p>
<p><strong>4. Enhancing Productivity</strong><br> NLP-powered tools streamline workflows by automating tasks like email sorting, invoice processing, or extracting key information from documents.<br> <strong>e.g.,</strong> Accounting systems use NLP to populate databases from invoices automatically.</p>
<p><strong>5. Enabling Accessibility</strong><br> NLP makes technology accessible to people with disabilities by supporting voice commands and text-to-speech systems.<br> <strong>e.g.,</strong> Screen readers for visually impaired users leverage NLP for better comprehension.</p>
<p><strong>6. Improving Communication Across Languages</strong><br> NLP facilitates real-time language translation, breaking down communication barriers globally.<br> <strong>e.g.,</strong> Google Translate uses NLP to translate text while preserving meaning and context.</p>
<p><strong>7. Driving Innovation in Specialized Fields</strong><br> NLP enables advancements in fields like healthcare (analyzing medical records) or autonomous systems (interpreting commands).<br> <strong>e.g.,</strong> Clinical applications use NLP to summarize patient records efficiently.</p>
<p><strong>8. Knowledge Graph and QA systems</strong><br> A knowledge graph (KG) is a structured network of entities (nodes) and their relationships (edges) that enables machines to understand context and meaning, making it a critical component in modern question-answering (QA) systems.<br> <strong>e.g.</strong> When asked, “Where was the painter of the Mona Lisa born?”, the KG links “Mona Lisa” → “painted by” → “Leonardo da Vinci” → “born in” → “Italy” to derive the answer</p>
<p><img src="https://assets.zilliz.com/Figure_1_Knowledge_graphs_illustration_643cec06af.png"></p>
<p>For example, a search for the film director James Cameron reveals information such as his date of birth, height, movies and TV shows he directed, previous romantic partners, TED Talks he gave</p>
<p><strong>9. Text parsing</strong><br> Text parsing, also known as syntactic analysis, is the process of analyzing text to understand its structure and meaning based on grammatical rules, separating it into smaller components for further processing.</p>
<p><img src="https://cdn.botpenguin.com/assets/website/Parsing_4bcdfead23.webp" width="40%"></p>
</section>
<section id="approaches-used-for-nlp" class="level2">
<h2 class="anchored" data-anchor-id="approaches-used-for-nlp">Approaches used for NLP</h2>
<p><strong>1. Heuristic approach</strong><br> Heuristics are mental shortcuts that allow people to solve problems and make judgments quickly and efficiently.</p>
<p>e.g., use of Regular expressions, wordnet, open mind common sense.</p>
<p><strong>2. ML approach</strong><br> Based on data. We convert the text into numbers and then apply algorithms.</p>
<p>e.g., Naive bayes, SVM, Logistic regression, LDA, Hidden markov models</p>
<p><strong>3. Deep Learning Approach</strong><br> In ML, sequential information is lost when text converted into numbers. In DL, the sequential information is preserved and also no feature generation is needed in DL unlike ML.</p>
<p>e.g., RNN, LSTM, GRU, Transformers</p>
</section>
<section id="challanges-in-nlp" class="level2">
<h2 class="anchored" data-anchor-id="challanges-in-nlp">Challanges in NLP</h2>
<p><strong>1. Ambiguity</strong><br> So much meaning of a single word or sentence is easy for us but not for machines.</p>
<p>e.g., I saw the boy on the bench with my binoculars.</p>
<p><strong>2. Contextual word</strong><br> Different meaning of the word based on the context.</p>
<p>e.g., I ran to the store because we ran out of the supplies.</p>
<p><strong>3. Colloquialisms and Slangs</strong><br> pulling leg meaning is different in our context but not for machines. Colloquialisms are a word or phrase that is used in conversation but not in formal speech or writing</p>
<p><strong>4. Synonyms</strong></p>
<p><strong>5. Tonal difference and irony</strong></p>
<p><strong>6. Speeling errors</strong></p>
<p><strong>7. Creativity</strong><br> e.g., Poems, dialogs, scripts</p>
<p><strong>8. So much languages/ diversity</strong></p>
</section>
<section id="nlp-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="nlp-pipeline">NLP Pipeline</h2>
<p>Steps followed to build an end to end NLP software. It consists of following steps:</p>
<ol type="1">
<li><p>Data acquisition</p></li>
<li><p>Text Preparation:</p>
<ul>
<li>Text cleanup <br>(like spelling mistakes or emoji removing etc.),</li>
<li>Basic preprocessing <br>(removing punctuations, stopwords and tokenization etc.)</li>
<li>Advance preprocessing <br>(chunking, Parts-of-speech or POS tagging, co-reference resolution etc.)</li>
</ul></li>
<li><p>Feature Engineering:<br> Converting words into numbers. <br> e.g., TF-IDF, Bag-of-words, word2vec</p></li>
<li><p>Modelling</p>
<ul>
<li>Model building</li>
<li>evaluation</li>
</ul></li>
<li><p>Deployment</p>
<ul>
<li>Deployment</li>
<li>Monitoring</li>
<li>Model update</li>
</ul></li>
</ol>
<p>This pipeline is mainly for ML, not for DL. Also this is not universal, e.g., this pipeline is good for sentiment analysis or text summarization but not chatbot.</p>
<p>Also, this is non-linear, i.e., we go back n forth continously depending on the results.</p>
<pre class="mermaid"><code>---
title: "1. Data Acquisition"
---
%%{init: {"flowchart": {"htmlLabels": true}}}%%
flowchart LR

    A{"&lt;b&gt;Data Acquisition&lt;/b&gt;"} --&gt; B("Available") &amp; C("Other Sources") &amp; D("No where")
    subgraph Available[" "]
        B --&gt; E["Already available in csv"] &amp; F["In the data warehouse, \nneed a data engineer to retrieve the data"] &amp; G["Less Data"]
        G --&gt; H["Data \naugmentation"]
        H --&gt; HA["Replacing some words with Synonyms"] &amp; HB["Bigram flip | e.g., [king, man] to [man, king]"] &amp; HC["Back-translate \n| Used to rearrange text \n| Converting into another lang &amp; \nthen converting back"] &amp; HD["Adding Noise"]
    end
    style A color:#000000, fill:#FFF9C4,  stroke:#000000

    subgraph OtherSources[" "]
        C --&gt; CA["&lt;p align='left'&gt;1. Public Dataset &lt;br&gt; 2. Web Scraping &lt;br&gt; 3. APIs &lt;br&gt; 4. PDF &lt;br&gt; 5. Image &lt;br&gt; 6. Audio"] 
    end
</code></pre>
<pre class="mermaid"><code>---
title: 2. Text preparation
---
%%{init: {"flowchart": {"htmlLabels": true}}}%%
flowchart LR

    A{"&lt;b&gt;Text Preparation&lt;/b&gt;"} --&gt; B(Cleaning) &amp; C(Basic preprocessing) &amp; D(Advance Preprocessing)
    subgraph Basic_preprocessing[" "]
    C --&gt; CA[Must] &amp; CC["Optional \n|Based on application"]
    CA --&gt; CAA["Tokenization \n| Sentence or Word Tokenization"]
    CC --&gt; CCA["&lt;p align='left'&gt;1. StopWords removal \n2. Word Stemming \n3. Removing Digits &amp; punctuation \n4. Lower casing \n5. Language Detection"]
    end
    subgraph Advance_preprocessing[" "]
    D --&gt; DA["&lt;p align='left'&gt;1. POS tagging\n2. Parsing\n3. Corefence resolution"]
    end

style A color:#000000, fill:#FFF9C4,  stroke:#000000</code></pre>
<pre class="mermaid"><code>flowchart TB
A{"&lt;b&gt;modeling&lt;/b&gt;"} --&gt; B["Heuristic approach \n (if less data)"] &amp; C["ML Models\n (if thik-thak data)"] &amp; D["DL\n (if much data)"] &amp; E["Cloud API \n (if andha paisa)"]
style A color:#000000, fill:#FFF9C4,  stroke:#000000</code></pre>
<div id="cell-12" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># remove html tags</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_html(data):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> re.<span class="bu">compile</span>(<span class="vs">r'&lt;.*?&gt;'</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p.sub(<span class="vs">r''</span>, data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-13" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="st">"&lt;html&gt;&lt;head&gt;Krishna.. &lt;a href='google.com'&gt; yes"</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>remove_html(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'Krishna..  yes'</code></pre>
</div>
</div>
<section id="why-remove-punctuations" class="level5">
<h5 class="anchored" data-anchor-id="why-remove-punctuations">Why remove punctuations?</h5>
<p>Punctuation removal simplifies text data, streamlining the analysis by reducing the complexity and variability within the data. Also punctuation does’t have exact meaning.</p>
<p>e.g., <code>hi!</code> and <code>hi</code> will be treated differently and will increase complexity.</p>
<div id="cell-15" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>string.punctuation</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~'</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_punc(text):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> char <span class="kw">in</span> string.punctuation:</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> text.replace(char, <span class="st">''</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="vs">r"""Removing stopwords is a common text-processing task. The words (like "is," "the," "at," etc.) usually don't contribute to the meaning"""</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>remove_punc(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-17" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>remove_punc(text)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>time1 <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(time1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.0002079010009765625</code></pre>
</div>
</div>
<div id="cell-18" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># OR</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> again_punc_remove(text):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text.translate(<span class="bu">str</span>.maketrans(<span class="st">''</span>, <span class="st">''</span>, string.punctuation))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-19" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>again_punc_remove(text)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>time2 <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(time2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.00016546249389648438</code></pre>
</div>
</div>
</section>
<section id="spell-correction" class="level3">
<h3 class="anchored" data-anchor-id="spell-correction">Spell correction</h3>
<div id="cell-21" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> textblob <span class="im">import</span> TextBlob</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-22" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>incorrect <span class="op">=</span> <span class="st">"wjo iss thiis? worng speeling "</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-23" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>TextBlob(incorrect).correct().string</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'who iss this? wrong spelling '</code></pre>
</div>
</div>
</section>
<section id="stop-words" class="level3">
<h3 class="anchored" data-anchor-id="stop-words">Stop words</h3>
<div id="cell-25" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-26" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> emoji</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-27" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>stopwords.words(<span class="st">'english'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['a',
 'about',
 'above',
 'after',
 'again',
 'against',
 'ain',
 'all',
 'am',
 'an',
 'and',
 'any',
 'are',
 'aren',
 "aren't",
 'as',
 'at',
 'be',
 'because',
 'been',
 'before',
 'being',
 'below',
 'between',
 'both',
 'but',
 'by',
 'can',
 'couldn',
 "couldn't",
 'd',
 'did',
 'didn',
 "didn't",
 'do',
 'does',
 'doesn',
 "doesn't",
 'doing',
 'don',
 "don't",
 'down',
 'during',
 'each',
 'few',
 'for',
 'from',
 'further',
 'had',
 'hadn',
 "hadn't",
 'has',
 'hasn',
 "hasn't",
 'have',
 'haven',
 "haven't",
 'having',
 'he',
 "he'd",
 "he'll",
 'her',
 'here',
 'hers',
 'herself',
 "he's",
 'him',
 'himself',
 'his',
 'how',
 'i',
 "i'd",
 'if',
 "i'll",
 "i'm",
 'in',
 'into',
 'is',
 'isn',
 "isn't",
 'it',
 "it'd",
 "it'll",
 "it's",
 'its',
 'itself',
 "i've",
 'just',
 'll',
 'm',
 'ma',
 'me',
 'mightn',
 "mightn't",
 'more',
 'most',
 'mustn',
 "mustn't",
 'my',
 'myself',
 'needn',
 "needn't",
 'no',
 'nor',
 'not',
 'now',
 'o',
 'of',
 'off',
 'on',
 'once',
 'only',
 'or',
 'other',
 'our',
 'ours',
 'ourselves',
 'out',
 'over',
 'own',
 're',
 's',
 'same',
 'shan',
 "shan't",
 'she',
 "she'd",
 "she'll",
 "she's",
 'should',
 'shouldn',
 "shouldn't",
 "should've",
 'so',
 'some',
 'such',
 't',
 'than',
 'that',
 "that'll",
 'the',
 'their',
 'theirs',
 'them',
 'themselves',
 'then',
 'there',
 'these',
 'they',
 "they'd",
 "they'll",
 "they're",
 "they've",
 'this',
 'those',
 'through',
 'to',
 'too',
 'under',
 'until',
 'up',
 've',
 'very',
 'was',
 'wasn',
 "wasn't",
 'we',
 "we'd",
 "we'll",
 "we're",
 'were',
 'weren',
 "weren't",
 "we've",
 'what',
 'when',
 'where',
 'which',
 'while',
 'who',
 'whom',
 'why',
 'will',
 'with',
 'won',
 "won't",
 'wouldn',
 "wouldn't",
 'y',
 'you',
 "you'd",
 "you'll",
 'your',
 "you're",
 'yours',
 'yourself',
 'yourselves',
 "you've"]</code></pre>
</div>
</div>
<div id="cell-28" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> emoji</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(emoji.demojize(<span class="st">'''Python is  😀😃😄😁😆'''</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Python is  :grinning_face::grinning_face_with_big_eyes::grinning_face_with_smiling_eyes::beaming_face_with_smiling_eyes::grinning_squinting_face:</code></pre>
</div>
</div>
<div id="cell-29" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> word_tokenize, sent_tokenize</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-30" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>sent1 <span class="op">=</span> <span class="st">"My email naman@samatrix.io, sent me there. Thank you."</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(word_tokenize(sent1))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sent_tokenize(sent1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['My', 'email', 'naman', '@', 'samatrix.io', ',', 'sent', 'me', 'there', '.', 'Thank', 'you', '.']
['My email naman@samatrix.io, sent me there.', 'Thank you.']</code></pre>
</div>
</div>
<div id="cell-31" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>sent2 <span class="op">=</span> <span class="st">"ohh! ride for 5km is $20.1"</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(word_tokenize(sent2))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sent_tokenize(sent2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['ohh', '!', 'ride', 'for', '5km', 'is', '$', '20.1']
['ohh!', 'ride for 5km is $20.1']</code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_trf"</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>nlp2 <span class="op">=</span> spacy.load(<span class="st">"en_core_web_lg"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-33" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(nlp(sent1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[My, email, naman@samatrix.io, ,, sent, me, there, ., Thank, you, .]</code></pre>
</div>
</div>
<div id="cell-34" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(nlp(sent2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[ohh, !, ride, for, 5, km, is, $, 20.1]</code></pre>
</div>
</div>
</section>
<section id="stemming" class="level3">
<h3 class="anchored" data-anchor-id="stemming">Stemming</h3>
<p>Used to extract root form from word.<br> NLTK has two stemmer, Porter stemmer for english and Snowball stemmer for other language.</p>
<div id="cell-36" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem.porter <span class="im">import</span> PorterStemmer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-37" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> PorterStemmer()</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stem_word(text):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">" "</span>.join([ps.stem(word) <span class="cf">for</span> word <span class="kw">in</span> text.split()])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-38" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>sample1 <span class="op">=</span> <span class="st">"walk walks walking walked ferrous ferrum ferric"</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>stem_word(sample1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'walk walk walk walk ferrou ferrum ferric'</code></pre>
</div>
</div>
<div id="cell-39" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>sample2 <span class="op">=</span> <span class="st">"All faith he had had had had no effect on an outcome of his life."</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>stem_word(sample2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'all faith he had had had had no effect on an outcom of hi life.'</code></pre>
</div>
</div>
<div id="cell-40" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>sample3 <span class="op">=</span> <span class="st">"The man the professor the student has knows studies Rome.."</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>stem_word(sample3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'the man the professor the student ha know studi rome..'</code></pre>
</div>
</div>
</section>
<section id="lemmitization" class="level3">
<h3 class="anchored" data-anchor-id="lemmitization">Lemmitization</h3>
<p>But stemming can not handle other than english, like ferrum is the root word and not ferrous but can’t handle by stemming. So, for these cases, we use Lemmitization which is little slow but can handle other language words too.</p>
<p>In cases where we want to show output to user, we use lemmitization, and where we don’t need to show output to user, we use stemming.</p>
<p>so, <strong>Lemmitization</strong> extracts proper root form from words.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>’ ’</th>
<th>Stemming</th>
<th>Lemmitization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Type</strong></td>
<td>Based on algorithms</td>
<td>Search in wordnet data.</td>
</tr>
<tr class="even">
<td><strong>Speed</strong></td>
<td>fast</td>
<td>slow</td>
</tr>
<tr class="odd">
<td><strong>Goal</strong></td>
<td>Simplify and standardize words</td>
<td>Reduce words to their root form</td>
</tr>
<tr class="even">
<td><strong>Method</strong></td>
<td>Remove common suffixes from the end of words</td>
<td>Consider word meaning and context</td>
</tr>
<tr class="odd">
<td><strong>Output</strong></td>
<td>Word stem that may not be meaningful</td>
<td>Normalized word form that can be found in a dictionary</td>
</tr>
</tbody>
</table>
<p>WordNet is a large lexical database of English, developed by Princeton University. It’s not a traditional dataset, but rather a linguistic resource that groups words into sets of cognitive synonyms (called synsets), each expressing a distinct concept.</p>
<p><strong>Key characteristics of WordNet:</strong></p>
<ul>
<li>Semantic Relationships: It captures relationships between words</li>
<li>Hierarchical Structure: Words are organized in a semantic network</li>
<li>Multiple Meanings: It accounts for different senses of words</li>
<li>Linguistic Information: Provides definitions, examples, and semantic connections</li>
</ul>
<p><strong>Main components:</strong></p>
<ul>
<li><p>Synsets: Groups of synonymous words/concepts</p></li>
<li><p>Semantic Relations:</p></li>
<li><ul>
<li>Hypernyms: More general words (e.g., “animal” is a hypernym of “dog”)</li>
</ul></li>
<li><ul>
<li>Hyponyms: More specific words (e.g., “retriever” is a hyponym of “dog”)</li>
</ul></li>
<li><ul>
<li>Meronyms: Part-whole relationships</li>
</ul></li>
<li><ul>
<li>Antonyms: Opposite meanings</li>
</ul></li>
</ul>
<p><strong>Use cases:</strong></p>
<p>Natural Language Processing, Semantic analysis, Machine translation, Information retrieval, Sentiment analysis, Text classification</p>
<div id="cell-42" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> wordnet <span class="im">as</span> wn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-43" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> wordnet <span class="im">as</span> wn</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_wordnet_details(word):</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Comprehensively print WordNet details for a given word</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co">        word (str): Word to explore in WordNet</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get all synsets for the word</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    synsets <span class="op">=</span> wn.synsets(word)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"===== WordNet Details for '</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">' ====="</span>)</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total number of synsets: </span><span class="sc">{</span><span class="bu">len</span>(synsets)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, synset <span class="kw">in</span> <span class="bu">enumerate</span>(synsets, <span class="dv">1</span>):</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Synset </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>synset<span class="sc">.</span>name()<span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Basic Information</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Lemmas: </span><span class="sc">{</span>[lemma.name() <span class="cf">for</span> lemma <span class="kw">in</span> synset.lemmas()]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Definition: </span><span class="sc">{</span>synset<span class="sc">.</span>definition()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Part of Speech: </span><span class="sc">{</span>synset<span class="sc">.</span>pos()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Semantic Relationships</span></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">  Semantic Relationships:"</span>)</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hypernyms (more general terms)</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>        hypernyms <span class="op">=</span> synset.hypernyms()</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hypernyms:</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"  Hypernyms:"</span>)</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> hyp <span class="kw">in</span> hypernyms:</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"    - </span><span class="sc">{</span>hyp<span class="sc">.</span>name()<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>hyp<span class="sc">.</span>definition()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hyponyms (more specific terms)</span></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>        hyponyms <span class="op">=</span> synset.hyponyms()</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> hyponyms:</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"  Hyponyms:"</span>)</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> hypo <span class="kw">in</span> hyponyms[:<span class="dv">5</span>]:  <span class="co"># Limit to first 5</span></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"    - </span><span class="sc">{</span>hypo<span class="sc">.</span>name()<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>hypo<span class="sc">.</span>definition()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Antonyms</span></span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a>        antonyms <span class="op">=</span> []</span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> lemma <span class="kw">in</span> synset.lemmas():</span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> lemma.antonyms():</span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>                antonyms.extend([ant.name() <span class="cf">for</span> ant <span class="kw">in</span> lemma.antonyms()])</span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> antonyms:</span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"  Antonyms:"</span>)</span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> ant <span class="kw">in</span> antonyms:</span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"    - </span><span class="sc">{</span>ant<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Examples</span></span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a>        examples <span class="op">=</span> synset.examples()</span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> examples:</span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">  Examples:"</span>)</span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> example <span class="kw">in</span> examples:</span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"    - </span><span class="sc">{</span>example<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Examples to demonstrate</span></span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a>words_to_explore <span class="op">=</span> [<span class="st">'dog'</span>,<span class="st">'beautiful'</span>]</span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Print details for each word</span></span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words_to_explore:</span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a>    print_wordnet_details(word)</span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>===== WordNet Details for 'dog' =====
Total number of synsets: 8

--- Synset 1: dog.n.01 ---
Lemmas: ['dog', 'domestic_dog', 'Canis_familiaris']
Definition: a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds
Part of Speech: n

  Semantic Relationships:
  Hypernyms:
    - domestic_animal.n.01: any of various animals that have been tamed and made fit for a human environment
    - canine.n.02: any of various fissiped mammals with nonretractile claws and typically long muzzles
  Hyponyms:
    - cur.n.01: an inferior dog or one of mixed breed
    - pug.n.01: small compact smooth-coated breed of Asiatic origin having a tightly curled tail and broad flat wrinkled muzzle
    - dalmatian.n.02: a large breed having a smooth white coat with black or brown spots; originated in Dalmatia
    - pooch.n.01: informal terms for dogs
    - lapdog.n.01: a dog small and tame enough to be held in the lap

  Examples:
    - the dog barked all night

--- Synset 2: frump.n.01 ---
Lemmas: ['frump', 'dog']
Definition: a dull unattractive unpleasant girl or woman
Part of Speech: n

  Semantic Relationships:
  Hypernyms:
    - unpleasant_woman.n.01: a woman who is an unpleasant person

  Examples:
    - she got a reputation as a frump
    - she's a real dog

--- Synset 3: dog.n.03 ---
Lemmas: ['dog']
Definition: informal term for a man
Part of Speech: n

  Semantic Relationships:
  Hypernyms:
    - chap.n.01: a boy or man

  Examples:
    - you lucky dog

--- Synset 4: cad.n.01 ---
Lemmas: ['cad', 'bounder', 'blackguard', 'dog', 'hound', 'heel']
Definition: someone who is morally reprehensible
Part of Speech: n

  Semantic Relationships:
  Hypernyms:
    - villain.n.01: a wicked or evil person; someone who does evil deliberately
  Hyponyms:
    - perisher.n.01: bounder

  Examples:
    - you dirty dog

--- Synset 5: frank.n.02 ---
Lemmas: ['frank', 'frankfurter', 'hotdog', 'hot_dog', 'dog', 'wiener', 'wienerwurst', 'weenie']
Definition: a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll
Part of Speech: n

  Semantic Relationships:
  Hypernyms:
    - sausage.n.01: highly seasoned minced meat stuffed in casings
  Hyponyms:
    - vienna_sausage.n.01: short slender frankfurter usually with ends cut off

--- Synset 6: pawl.n.01 ---
Lemmas: ['pawl', 'detent', 'click', 'dog']
Definition: a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward
Part of Speech: n

  Semantic Relationships:
  Hypernyms:
    - catch.n.06: a restraint that checks the motion of something

--- Synset 7: andiron.n.01 ---
Lemmas: ['andiron', 'firedog', 'dog', 'dog-iron']
Definition: metal supports for logs in a fireplace
Part of Speech: n

  Semantic Relationships:
  Hypernyms:
    - support.n.10: any device that bears the weight of another thing

  Examples:
    - the andirons were too hot to touch

--- Synset 8: chase.v.01 ---
Lemmas: ['chase', 'chase_after', 'trail', 'tail', 'tag', 'give_chase', 'dog', 'go_after', 'track']
Definition: go after with the intent to catch
Part of Speech: v

  Semantic Relationships:
  Hypernyms:
    - pursue.v.02: follow in or as if in pursuit
  Hyponyms:
    - run_down.v.07: pursue until captured
    - quest.v.02: search the trail of (game)
    - tree.v.03: chase an animal up a tree
    - hound.v.01: pursue or chase relentlessly

  Examples:
    - The policeman chased the mugger down the alley
    - the dog chased the rabbit

==================================================

===== WordNet Details for 'beautiful' =====
Total number of synsets: 2

--- Synset 1: beautiful.a.01 ---
Lemmas: ['beautiful']
Definition: delighting the senses or exciting intellectual or emotional admiration
Part of Speech: a

  Semantic Relationships:
  Antonyms:
    - ugly

  Examples:
    - a beautiful child
    - beautiful country
    - a beautiful painting
    - a beautiful theory
    - a beautiful party

--- Synset 2: beautiful.s.02 ---
Lemmas: ['beautiful']
Definition: (of weather) highly enjoyable
Part of Speech: s

  Semantic Relationships:

  Examples:
    - what a beautiful day

==================================================
</code></pre>
</div>
</div>
<div id="cell-44" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> word_tokenize, sent_tokenize</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer, PorterStemmer</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>lemmatizer <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>stemmer <span class="op">=</span> PorterStemmer()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-45" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">"The striped bats are hanging on their feet for best"</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> nltk.word_tokenize(sentence)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>stemmed_words <span class="op">=</span> [stemmer.stem(word) <span class="cf">for</span> word <span class="kw">in</span> words]</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="co"># function to get parts of speech</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_wordnet_pos(word):</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    tag <span class="op">=</span> nltk.pos_tag([word])[<span class="dv">0</span>][<span class="dv">1</span>][<span class="dv">0</span>].upper()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-46" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer <span class="im">as</span> wnl</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wnl().lemmatize(<span class="st">'dogs'</span>))</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wnl().lemmatize(<span class="st">'churches'</span>))</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wnl().lemmatize(<span class="st">'ships'</span>))</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wnl().lemmatize(<span class="st">'aardwolves'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dog
church
ship
aardwolf</code></pre>
</div>
</div>
<div id="cell-47" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>sentence2 <span class="op">=</span> <span class="st">"John's big idea isn't all that bad. He sleeps the most. loved love loving"</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>punctuations <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="st">"?.!:,"</span>]</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>sentence_words <span class="op">=</span> word_tokenize(sentence2)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> sentence_words:</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> punctuations:</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>        sentence_words.remove(word)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sentence_words)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Word'</span><span class="sc">:20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Lemma'</span><span class="sc">:20}</span><span class="ss">"</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> sentence_words:</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:20}</span><span class="ss"> </span><span class="sc">{</span>lemmatizer<span class="sc">.</span>lemmatize(word, pos<span class="op">=</span><span class="st">'v'</span>)<span class="sc">:20}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['John', "'s", 'big', 'idea', 'is', "n't", 'all', 'that', 'bad', 'He', 'sleeps', 'the', 'most', 'loved', 'love', 'loving']
Word                 Lemma               
John                 John                
's                   's                  
big                  big                 
idea                 idea                
is                   be                  
n't                  n't                 
all                  all                 
that                 that                
bad                  bad                 
He                   He                  
sleeps               sleep               
the                  the                 
most                 most                
loved                love                
love                 love                
loving               love                </code></pre>
</div>
</div>
<div id="cell-48" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_wordnet_pos(word):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    tag <span class="op">=</span> nltk.pos_tag(word_tokenize(word), tagset<span class="op">=</span><span class="st">'universal'</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tag</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>get_wordnet_pos(sentence2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('John', 'NOUN'),
 ("'s", 'PRT'),
 ('big', 'ADJ'),
 ('idea', 'NOUN'),
 ('is', 'VERB'),
 ("n't", 'ADV'),
 ('all', 'DET'),
 ('that', 'DET'),
 ('bad', 'ADJ'),
 ('.', '.'),
 ('He', 'PRON'),
 ('sleeps', 'VERB'),
 ('the', 'DET'),
 ('most', 'ADV'),
 ('.', '.'),
 ('loved', 'VERB'),
 ('love', 'ADP'),
 ('loving', 'VERB')]</code></pre>
</div>
</div>
<p>Que. What is feature extraction from text?<br> Ans. Converting text into numbers. It is also called text representation / text vectorization.</p>
<p><strong>1. Corpus</strong></p>
<ul>
<li><p>A corpus (plural: corpora) is a large collection of texts (documents) used for linguistic analysis or NLP tasks.</p></li>
<li><p>Example:<br> News Corpus: A collection of news articles from various publishers.<br> Medical Corpus: A set of medical research papers.<br> Wikipedia Corpus: A dataset containing all Wikipedia pages.</p></li>
<li><p>Example Corpus (Simple Set of Documents)<br> Document 1: “Apple is a popular fruit.”<br> Document 2: “Apple Inc.&nbsp;is a technology company.”<br> Document 3: “Bananas and apples are fruits.”</p></li>
</ul>
<p><strong>2. Vocabulary</strong></p>
<ul>
<li><p>The vocabulary of a corpus is the set of unique words it contains.</p></li>
<li><p>Example: For the three documents above, the vocabulary is:<br> {“Apple”, “is”, “a”, “popular”, “fruit”, “Inc.”, “technology”, “company”, “Bananas”, “and”, “apples”, “are”, “fruits”}</p></li>
<li><p>Vocabulary does not store duplicates—only unique words from the corpus.</p></li>
</ul>
<p><strong>3. Word</strong></p>
<ul>
<li><p>A word is an individual unit in the vocabulary.</p></li>
<li><p>Example: “Apple” appears in two different meanings (fruit vs.&nbsp;company).<br> “is” is a common word appearing multiple times.</p></li>
</ul>
<p><strong>4. Document</strong></p>
<ul>
<li><p>A document is a single text or a part of a corpus. It can be a sentence, paragraph, or an entire article.</p></li>
<li><p>Example: Single Document: “Apple is a popular fruit.”</p></li>
<li><p>Multiple Documents in a Corpus:<br> [“Apple is a popular fruit.”, “Apple Inc.&nbsp;is a technology company.”, “Bananas and apples are fruits.”]</p></li>
</ul>
</section>
<section id="ohe-one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="ohe-one-hot-encoding">OHE ( One hot encoding)</h3>
<p><strong>Fayde</strong> - 1. Samajh me aati hai</p>
<p><strong>Nuksan</strong> - 1. Sparsity (so many zeroes) 2. as the sentence length changes, input size changes, and ML algos can’t work with varying inputs. 3. <strong>OOV(out of vocabulary) problem</strong> - Word other than words defined inside vocab can not be vectorized. 4. <strong>No semantic meaning</strong> - When we convert text into numbers, our core idea is to convert in that way so that their semantic meaning also visible in the numbers, e.g., Shoes = 1, run = 0.8, fish = 3<br> because shoes and run are somewhere similar than fish. But One hot encoding can not retain this relation, it will treat each word equal. <br> e.g., male = [0,1,0], female = [0,0,1], book = [1,0,0]</p>
</section>
</section>
<section id="bag-of-words-bow-nlp-concept" class="level1">
<h1>Bag of Words (BoW) – NLP Concept</h1>
<section id="what-is-bag-of-words" class="level2">
<h2 class="anchored" data-anchor-id="what-is-bag-of-words">📖 What is Bag of Words?</h2>
<p><strong>Bag of Words (BoW)</strong> is a <strong>text representation technique</strong> used in <strong>Natural Language Processing (NLP)</strong>. It converts text into numerical form by counting word occurrences, while <strong>ignoring grammar and word order</strong>.</p>
<section id="how-bag-of-words-works" class="level3">
<h3 class="anchored" data-anchor-id="how-bag-of-words-works">🔹 How Bag of Words Works:</h3>
<ol type="1">
<li><strong>Create a Vocabulary</strong> – Identify all unique words in the corpus.</li>
<li><strong>Count Word Occurrences</strong> – Count how many times each word appears in each document.</li>
<li><strong>Convert to a Vector</strong> – Represent each document mas a numerical vector.</li>
</ol>
</section>
</section>
<section id="example-of-bag-of-words" class="level2">
<h2 class="anchored" data-anchor-id="example-of-bag-of-words">📝 Example of Bag of Words</h2>
<section id="corpus-collection-of-documents" class="level3">
<h3 class="anchored" data-anchor-id="corpus-collection-of-documents"><strong>Corpus (Collection of Documents)</strong></h3>
<ul>
<li>Document 1: “Apple is a fruit”</li>
<li>Document 2: “I like Apple and Banana”</li>
<li>Document 3: “Banana is a yellow fruit”</li>
</ul>
</section>
<section id="step-1-create-vocabulary" class="level3">
<h3 class="anchored" data-anchor-id="step-1-create-vocabulary"><strong>Step 1: Create Vocabulary</strong></h3>
<p>[“Apple”, “is”, “a”, “fruit”, “I”, “like”, “and”, “Banana”, “yellow”]</p>
</section>
<section id="step-2-word-count-in-each-document" class="level3">
<h3 class="anchored" data-anchor-id="step-2-word-count-in-each-document"><strong>Step 2: Word Count in Each Document</strong></h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>Doc 1</th>
<th>Doc 2</th>
<th>Doc 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Apple</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>is</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>a</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>fruit</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>I</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>like</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>and</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>Banana</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>yellow</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</section>
<section id="step-3-convert-to-vector-representation" class="level3">
<h3 class="anchored" data-anchor-id="step-3-convert-to-vector-representation"><strong>Step 3: Convert to Vector Representation</strong></h3>
<p>Each document is now represented as a vector based on word frequency.</p>
<ul>
<li>Doc 1 → [1, 1, 1, 1, 0, 0, 0, 0, 0]</li>
<li>Doc 2 → [1, 0, 0, 0, 1, 1, 1, 1, 0]</li>
<li>Doc 3 → [0, 1, 1, 1, 0, 0, 0, 1, 1]</li>
</ul>
</section>
</section>
<section id="advantages-of-bag-of-words" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-bag-of-words">✅ Advantages of Bag of Words</h2>
<ul>
<li><strong>Simple &amp; Effective</strong> – Works well for text classification tasks.</li>
<li><strong>Useful for Machine Learning</strong> – Applied in spam detection, sentiment analysis, and information retrieval.</li>
<li><strong>OOV problem solved</strong></li>
<li><strong>Fixed Input size</strong></li>
</ul>
</section>
<section id="limitations-of-bag-of-words" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-bag-of-words">❌ Limitations of Bag of Words</h2>
<ul>
<li><strong>Ignores word order</strong> (e.g., “I love NLP” and “NLP love I” are treated the same).</li>
<li><strong>Sparse representation</strong> (creates large vectors for big vocabularies).</li>
<li><strong>Does not capture meaning</strong> (e.g., “good” and “not good” may be treated similarly).</li>
<li><strong>not treating out of vocabulary words</strong> it don’t throw error for OOV but still not know how to deal with it.</li>
</ul>
</section>
<section id="alternative-approaches" class="level2">
<h2 class="anchored" data-anchor-id="alternative-approaches">🔥 Alternative Approaches</h2>
<ul>
<li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> – Weighs words based on importance.</li>
<li><strong>Word Embeddings (Word2Vec, GloVe, BERT)</strong> – Captures semantic meaning.</li>
</ul>
<div id="cell-54" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample corpus</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">"Apple is a Apple fruit"</span>, <span class="st">"I like an Apple of and Banana"</span>, <span class="st">"The Banana is a yellow fruit"</span>, <span class="st">"an an an"</span>]</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize CountVectorizer</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer()</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform the corpus into BoW representation</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Get feature names (vocabulary)</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> vectorizer.get_feature_names_out()</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocabulary count: "</span>, vectorizer.vocabulary_)</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="co"># sorting </span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> <span class="bu">sorted</span>(vectorizer.vocabulary_.values())</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>new_dict <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> vals:</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> vectorizer.vocabulary_.keys():</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> vectorizer.vocabulary_[j] <span class="op">==</span> i:</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>            new_dict[j] <span class="op">=</span> i</span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sorted: "</span>,new_dict)        </span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert BoW matrix to array and display</span></span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Bag of Words Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Vocabulary count:  {'apple': 2, 'is': 5, 'fruit': 4, 'like': 6, 'an': 0, 'of': 7, 'and': 1, 'banana': 3, 'the': 8, 'yellow': 9}
sorted:  {'an': 0, 'and': 1, 'apple': 2, 'banana': 3, 'fruit': 4, 'is': 5, 'like': 6, 'of': 7, 'the': 8, 'yellow': 9}

Bag of Words Matrix:
 [[0 0 2 0 1 1 0 0 0 0]
 [1 1 1 1 0 0 1 1 0 0]
 [0 0 0 1 1 1 0 0 1 1]
 [3 0 0 0 0 0 0 0 0 0]]</code></pre>
</div>
</div>
<div id="cell-55" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>vectorizer.transform([<span class="st">"I don't like apple."</span>]).toarray()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>array([[0, 0, 1, 0, 0, 0,
        1, 0, 0, 0]])</code></pre>
</div>
</div>
<p>Stopwords like a are automatically removed.</p>
</section>
</section>
<section id="n-grams-and-bag-of-n-grams" class="level1">
<h1>📚 N-grams and Bag of N-grams</h1>
<section id="what-are-n-grams" class="level2">
<h2 class="anchored" data-anchor-id="what-are-n-grams">What are N-grams?</h2>
<p>An <strong>n-gram</strong> is a contiguous sequence of <strong>n items</strong> (usually words or characters) from a given text.</p>
<ul>
<li><strong>Unigram (n = 1)</strong>: <code>"I love NLP"</code> → <code>["I", "love", "NLP"]</code></li>
<li><strong>Bigram (n = 2)</strong>: <code>"I love NLP"</code> → <code>[("I", "love"), ("love", "NLP")]</code></li>
<li><strong>Trigram (n = 3)</strong>: <code>"I love NLP"</code> → <code>[("I", "love", "NLP")]</code></li>
</ul>
</section>
<section id="what-is-a-bag-of-n-grams" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-bag-of-n-grams">What is a Bag of N-grams?</h2>
<p>Just like <strong>Bag of Words (BoW)</strong> counts word frequencies, the <strong>Bag of N-grams</strong> model counts <strong>n-gram frequencies</strong>.</p>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">🧾 Example</h3>
<p>Given two sentences:</p>
<p>S1: “I love NLP” S2: “I love deep learning”</p>
<p>Using <strong>bigrams</strong>:</p>
<ul>
<li>From S1: <code>("I", "love"), ("love", "NLP")</code></li>
<li>From S2: <code>("I", "love"), ("love", "deep"), ("deep", "learning")</code></li>
</ul>
</section>
<section id="bag-of-bigrams" class="level3">
<h3 class="anchored" data-anchor-id="bag-of-bigrams">📊 Bag of Bigrams:</h3>
<p><code>alcctnxrntqcxpkwlraqqktfyypzjmsfbuowcsgfgyhcdgpwutkzkmmubngqivvujjmurxwjjnnfgcttwoti</code></p>
</section>
</section>
<section id="why-use-n-grams" class="level2">
<h2 class="anchored" data-anchor-id="why-use-n-grams">🔹 Why Use N-grams?</h2>
<p>✅ Capture local context (e.g., “deep learning”, “New York”)<br>
✅ Better than single words in many NLP tasks<br>
⚠️ Larger <code>n</code> leads to more features → sparse vectors → higher dimensionality</p>
<section id="code-example-python" class="level4">
<h4 class="anchored" data-anchor-id="code-example-python">🧪 Code Example (Python)</h4>
<div id="cell-59" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample text corpus</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love NLP"</span>,</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love deep learning"</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a bigram vectorizer</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>))    <span class="co">## combination of (1,1) and (2,2) and (3,3)</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Get feature names</span></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bigrams:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocabulary count: "</span>, vectorizer.vocabulary_)</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to array</span></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frequency Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bigrams: ['deep' 'deep learning'
 'learning' 'love'
 'love deep'
 'love deep learning'
 'love nlp' 'nlp']
Vocabulary count:  {'love': 3, 'nlp': 7, 'love nlp': 6, 'deep': 0, 'learning': 2, 'love deep': 4, 'deep learning': 1, 'love deep learning': 5}
Frequency Matrix:
 [[0 0 0 1 0 0 1 1]
 [1 1 1 1 1 1 0 0]]</code></pre>
</div>
</div>
<div id="cell-60" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This movie is very good"</span>,</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This movie is not good"</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a bigram vectorizer</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))    <span class="co">## combination of (1,1) and (2,2) and (3,3)</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get feature names</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bigrams:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocabulary count: "</span>, vectorizer.vocabulary_)</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to array</span></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frequency Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bigrams: ['good' 'is' 'movie' 'not' 'this' 'very']
Vocabulary count:  {'this': 4, 'movie': 2, 'is': 1, 'very': 5, 'good': 0, 'not': 3}
Frequency Matrix:
 [[1 1 1 0 1 1]
 [1 1 1 1 1 0]]</code></pre>
</div>
</div>
<p>Using Unigram,</p>
<ul>
<li><p>Vectors are quite similar.</p></li>
<li><p>Only difference: very in sentence 1 vs not in sentence 2.</p></li>
<li><p>This means cosine distance or Euclidean distance between them is small.</p></li>
</ul>
<div id="cell-62" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Bigrams:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocabulary:"</span>, vectorizer.vocabulary_)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frequency Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bigrams: ['is not' 'is very' 'movie is' 'not good' 'this movie' 'very good']
Vocabulary: {'this movie': 4, 'movie is': 2, 'is very': 1, 'very good': 5, 'is not': 0, 'not good': 3}
Frequency Matrix:
 [[0 1 1 0 1 1]
 [1 0 1 1 1 0]]</code></pre>
</div>
</div>
<p>Using Bigram, - Now the vectors differ more significantly.</p>
<ul>
<li><p>Bigram is very and very good only exist in sentence 1.</p></li>
<li><p>Bigram is not and not good only exist in sentence 2.</p></li>
<li><p>Higher discriminative power than unigrams.</p></li>
</ul>
<div id="cell-64" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trigrams:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocabulary:"</span>, vectorizer.vocabulary_)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frequency Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Trigrams: ['is not good' 'is very good' 'movie is not' 'movie is very' 'this movie is']
Vocabulary: {'this movie is': 4, 'movie is very': 3, 'is very good': 1, 'movie is not': 2, 'is not good': 0}
Frequency Matrix:
 [[0 1 0 1 1]
 [1 0 1 0 1]]</code></pre>
</div>
</div>
<p>Using Trigrams, - Even sharper contrast than bigrams.</p>
<ul>
<li><p>No shared trigrams between the two sentences.</p></li>
<li><p>The vectors are now much farther apart.</p></li>
</ul>
<div id="cell-66" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Combined N-grams (1, 2, 3)</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All N-grams:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frequency Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All N-grams: ['good' 'is' 'is not' 'is not good' 'is very' 'is very good' 'movie' 'movie is'
 'movie is not' 'movie is very' 'not' 'not good' 'this' 'this movie' 'this movie is'
 'very' 'very good']
Frequency Matrix:
 [[1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1]
 [1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0]]</code></pre>
</div>
</div>
<p>This approach mixes all the advantages of different levels of granularity.</p>
<div id="cell-68" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Corpus</span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This movie is very good"</span>,</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"This movie is not good"</span></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to process n-grams and visualize</span></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_and_visualize(ngram_range, title):</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Vectorization</span></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>    vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>ngram_range)</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> vectorizer.fit_transform(corpus).toarray()</span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cosine Similarity</span></span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>    similarity <span class="op">=</span> cosine_similarity(X)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">=== </span><span class="sc">{</span>title<span class="sc">}</span><span class="ss"> ==="</span>)</span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Feature Names:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Vectors:</span><span class="ch">\n</span><span class="st">"</span>, X)</span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Cosine Similarity: </span><span class="sc">{</span>similarity<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PCA for 2D projection</span></span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a>    X_pca <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb68-31"><a href="#cb68-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-32"><a href="#cb68-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot</span></span>
<span id="cb68-33"><a href="#cb68-33" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb68-34"><a href="#cb68-34" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"</span><span class="sc">{</span>title<span class="sc">}</span><span class="ss"> (Similarity: </span><span class="sc">{</span>similarity<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb68-35"><a href="#cb68-35" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X_pca[:, <span class="dv">0</span>], X_pca[:, <span class="dv">1</span>], c<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'red'</span>])</span>
<span id="cb68-36"><a href="#cb68-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, txt <span class="kw">in</span> <span class="bu">enumerate</span>(corpus):</span>
<span id="cb68-37"><a href="#cb68-37" aria-hidden="true" tabindex="-1"></a>        plt.annotate(<span class="ss">f"S</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>, (X_pca[i, <span class="dv">0</span>], X_pca[i, <span class="dv">1</span>]), fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb68-38"><a href="#cb68-38" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"PC1"</span>)</span>
<span id="cb68-39"><a href="#cb68-39" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"PC2"</span>)</span>
<span id="cb68-40"><a href="#cb68-40" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb68-41"><a href="#cb68-41" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb68-42"><a href="#cb68-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-43"><a href="#cb68-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Run for unigram, bigram, trigram</span></span>
<span id="cb68-44"><a href="#cb68-44" aria-hidden="true" tabindex="-1"></a>process_and_visualize((<span class="dv">1</span>, <span class="dv">1</span>), <span class="st">"Unigram Representation"</span>)</span>
<span id="cb68-45"><a href="#cb68-45" aria-hidden="true" tabindex="-1"></a>process_and_visualize((<span class="dv">2</span>, <span class="dv">2</span>), <span class="st">"Bigram Representation"</span>)</span>
<span id="cb68-46"><a href="#cb68-46" aria-hidden="true" tabindex="-1"></a>process_and_visualize((<span class="dv">3</span>, <span class="dv">3</span>), <span class="st">"Trigram Representation"</span>)</span>
<span id="cb68-47"><a href="#cb68-47" aria-hidden="true" tabindex="-1"></a>process_and_visualize((<span class="dv">1</span>, <span class="dv">3</span>), <span class="st">"Combined ngrams Representation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Unigram Representation ===
Feature Names: ['good' 'is' 'movie' 'not' 'this' 'very']
Vectors:
 [[1 1 1 0 1 1]
 [1 1 1 1 1 0]]
Cosine Similarity: 0.8000</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NLP_files/figure-html/cell-41-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Bigram Representation ===
Feature Names: ['is not' 'is very' 'movie is' 'not good' 'this movie' 'very good']
Vectors:
 [[0 1 1 0 1 1]
 [1 0 1 1 1 0]]
Cosine Similarity: 0.5000</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NLP_files/figure-html/cell-41-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Trigram Representation ===
Feature Names: ['is not good' 'is very good' 'movie is not' 'movie is very' 'this movie is']
Vectors:
 [[0 1 0 1 1]
 [1 0 1 0 1]]
Cosine Similarity: 0.3333</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NLP_files/figure-html/cell-41-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Combined ngrams Representation ===
Feature Names: ['good' 'is' 'is not' 'is not good' 'is very' 'is very good' 'movie' 'movie is'
 'movie is not' 'movie is very' 'not' 'not good' 'this' 'this movie' 'this movie is'
 'very' 'very good']
Vectors:
 [[1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1]
 [1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0]]
Cosine Similarity: 0.5833</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NLP_files/figure-html/cell-41-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="advantages" class="level2">
<h2 class="anchored" data-anchor-id="advantages">✅ Advantages</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 69%">
</colgroup>
<thead>
<tr class="header">
<th>Benefit</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>📏 Simple and Intuitive</td>
<td>Easy to understand and implement</td>
</tr>
<tr class="even">
<td>🧠 Captures Local Context</td>
<td>Bigrams/trigrams can model short phrases and common word combinations</td>
</tr>
<tr class="odd">
<td>🎯 Improves Model Performance</td>
<td>Often boosts accuracy over just unigrams in traditional models</td>
</tr>
<tr class="even">
<td>📊 Great for Statistical Models</td>
<td>Works well with Naive Bayes, SVM, Logistic Regression</td>
</tr>
<tr class="odd">
<td>🔍 Useful for Pattern Detection</td>
<td>Can detect things like “not good”, “very bad”, “New York”</td>
</tr>
<tr class="even">
<td>⚡ Fast &amp; Scalable</td>
<td>Easy to vectorize and use in sparse matrix form</td>
</tr>
</tbody>
</table>
</section>
<section id="disadvantages" class="level2">
<h2 class="anchored" data-anchor-id="disadvantages">⚠️ Disadvantages</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 69%">
</colgroup>
<thead>
<tr class="header">
<th>Drawback</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>🧱 Data Sparsity</td>
<td>Higher n-grams = more combinations = more zeroes in vector space</td>
</tr>
<tr class="even">
<td>🧠 No Understanding of Meaning</td>
<td>N-grams are based on surface text, not semantics (e.g., “not good” ≠ “bad”)</td>
</tr>
<tr class="odd">
<td>📏 Context Window is Fixed</td>
<td>Only captures local context, not long-range dependencies</td>
</tr>
<tr class="even">
<td>🧹 Vocabulary Explosion</td>
<td>As <code>n</code> increases, vocab size grows rapidly → more memory usage</td>
</tr>
<tr class="odd">
<td>🔄 Not Robust to Word Reordering</td>
<td>“dog bites man” ≠ “man bites dog” → N-gram doesn’t generalize well</td>
</tr>
<tr class="even">
<td>🚫 Ignores Grammar</td>
<td>Doesn’t consider POS tags, sentence structure, or syntax</td>
</tr>
</tbody>
</table>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">📌 Summary</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>N</th>
<th>Use When…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>You want a basic BoW model</td>
</tr>
<tr class="even">
<td>2</td>
<td>You want to capture short phrases</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Phrases are important, and you can afford sparsity</td>
</tr>
<tr class="even">
<td>4+</td>
<td>Only for large corpora or specific tasks</td>
</tr>
</tbody>
</table>
<p>Combine unigrams + bigrams (<code>ngram_range=(1, 2)</code>) for a good balance between context and sparsity.</p>
<div id="cell-70" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">'dog bites man'</span>, <span class="st">'man bites dog'</span>]</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All N-grams:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frequency Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cosine similarity: "</span>, cosine_similarity(X)[<span class="dv">0</span>, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All N-grams: ['bites' 'dog' 'man']
Frequency Matrix:
 [[1 1 1]
 [1 1 1]]
Cosine similarity:  1.0000000000000002</code></pre>
</div>
</div>
<div id="cell-71" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">'dog bites man'</span>, <span class="st">'man bites dog'</span>]</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All N-grams:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frequency Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cosine similarity: "</span>, cosine_similarity(X)[<span class="dv">0</span>, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All N-grams: ['bites dog' 'bites man' 'dog bites' 'man bites']
Frequency Matrix:
 [[0 1 1 0]
 [1 0 0 1]]
Cosine similarity:  0.0</code></pre>
</div>
</div>
<div id="cell-72" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [<span class="st">'dog bites man'</span>, <span class="st">'man bites dog'</span>]</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All N-grams:"</span>, vectorizer.get_feature_names_out())</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Frequency Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cosine similarity: "</span>, cosine_similarity(X)[<span class="dv">0</span>, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All N-grams: ['bites' 'bites dog' 'bites man' 'dog' 'dog bites' 'man' 'man bites']
Frequency Matrix:
 [[1 0 1 1 1 1 0]
 [1 1 0 1 0 1 1]]
Cosine similarity:  0.6</code></pre>
</div>
</div>
</section>
</section>
<section id="tf-idf-term-frequencyinverse-document-frequency" class="level1">
<h1>📊 TF-IDF (Term Frequency–Inverse Document Frequency)</h1>
<p>TF-IDF is a popular technique to convert <strong>text to numerical features</strong>. It helps us weigh how <strong>important</strong> a word is in a document <strong>relative to the entire corpus</strong>.</p>
<section id="what-is-tf-idf" class="level2">
<h2 class="anchored" data-anchor-id="what-is-tf-idf">🔹 1. What is TF-IDF?</h2>
<p>It combines two metrics:</p>
<ul>
<li><strong>TF (Term Frequency)</strong>: How often a word appears in a document.</li>
<li><strong>IDF (Inverse Document Frequency)</strong>: How rare the word is across all documents.</li>
</ul>
<p><span class="math display">\[
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
\]</span></p>
</section>
<section id="term-frequency-tf" class="level2">
<h2 class="anchored" data-anchor-id="term-frequency-tf">🔹 2. Term Frequency (TF)</h2>
<p><span class="math display">\[
\text{TF}(t, d) = \frac{\text{Count of term } t \text{ in } d}{\text{Total terms in } d}
\]</span></p>
</section>
<section id="inverse-document-frequency-idf" class="level2">
<h2 class="anchored" data-anchor-id="inverse-document-frequency-idf">🔹 3. Inverse Document Frequency (IDF)</h2>
<p><span class="math display">\[
\text{IDF}(t) = \log \left(\frac{N}{1 + \text{df}(t)}\right)
\]</span></p>
<p>Where: - <code>N</code> = Total number of documents - <code>df(t)</code> = Number of documents containing the term <code>t</code></p>
<blockquote class="blockquote">
<p>Smoothing (<code>+1</code>) avoids division by zero.</p>
</blockquote>
</section>
<section id="example-1" class="level2">
<h2 class="anchored" data-anchor-id="example-1">🔹 4. Example</h2>
<p>Suppose we have 3 documents:</p>
<p>D1: “I love machine learning”</p>
<p>D2: “I love deep learning”</p>
<p>D3: “I love pizza”</p>
<ul>
<li>TF(“learning”, D1) = 1/4 = 0.25<br>
</li>
<li>“learning” appears in 2 documents → IDF = log(3 / 2) ≈ 0.176<br>
</li>
<li>TF-IDF = 0.25 × 0.176 ≈ 0.044</li>
</ul>
<p>Compare with:</p>
<ul>
<li>TF(“pizza”, D3) = 1/4 = 0.25<br>
</li>
<li>IDF(“pizza”) = log(3 / 1) ≈ 0.477<br>
</li>
<li>TF-IDF ≈ 0.119</li>
</ul>
<p>👉 “pizza” is more unique → higher TF-IDF</p>
</section>
<section id="advantages-1" class="level2">
<h2 class="anchored" data-anchor-id="advantages-1">Advantages</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th>Benefit</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Filters out common words</td>
<td>Words like “the”, “is” get low scores</td>
</tr>
<tr class="even">
<td>Highlights uniqueness</td>
<td>Emphasizes words that are rare and relevant</td>
</tr>
<tr class="odd">
<td>Fast and lightweight</td>
<td>Efficient even on large datasets</td>
</tr>
<tr class="even">
<td>Works well with ML models</td>
<td>Great for SVM, Naive Bayes, Logistic Regression</td>
</tr>
</tbody>
</table>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Limitation</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>No word order</td>
<td>Cannot capture phrases like “not good”</td>
</tr>
<tr class="even">
<td>Sparse representation</td>
<td>Large vocab = large, sparse matrices</td>
</tr>
<tr class="odd">
<td>No semantic meaning</td>
<td>Can’t tell “excellent” ≠ “good”</td>
</tr>
<tr class="even">
<td>more dimensions</td>
<td>if vocab is big</td>
</tr>
</tbody>
</table>
<div id="cell-74" class="cell" data-execution_count="229">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love machine learning deep learning"</span>,</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love deep learning"</span>,</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love pizza"</span></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(corpus)</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Features:"</span>, <span class="bu">dict</span>(<span class="bu">sorted</span>(vectorizer.vocabulary_.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>])))</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TF-IDF Matrix:</span><span class="ch">\n</span><span class="st">"</span>, X.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Features: {'deep': 0, 'learning': 1, 'love': 2, 'machine': 3, 'pizza': 4}
TF-IDF Matrix:
 [[0.36930805 0.73861611 0.28680065 0.48559571 0.        ]
 [0.61980538 0.61980538 0.48133417 0.         0.        ]
 [0.         0.         0.50854232 0.         0.861037  ]]</code></pre>
</div>
</div>
<div id="cell-75" class="cell" data-execution_count="222">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for I love machine learning deep learning</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> idff(n, df):</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.log((n<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span>(df<span class="op">+</span><span class="dv">1</span>))<span class="op">+</span><span class="dv">1</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>tfidfs <span class="op">=</span> [<span class="dv">1</span><span class="op">/</span><span class="dv">6</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">2</span>),<span class="dv">2</span><span class="op">/</span><span class="dv">6</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">2</span>), <span class="dv">1</span><span class="op">/</span><span class="dv">6</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">1</span><span class="op">/</span><span class="dv">6</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">1</span>),<span class="dv">0</span>]</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Before Normalization:"</span>, np.<span class="bu">round</span>(tfidfs, <span class="dv">4</span>))</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>l2norm <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(np.power(np.array(tfidfs), <span class="dv">2</span>)))</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L2 norm:"</span>, l2norm)</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After normalization:"</span>, tfidfs<span class="op">/</span>l2norm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before Normalization: [0.2146 0.4292 0.1667 0.2822 0.    ]
L2 norm: 0.5811237431239642
After normalization: [0.36930805 0.73861611 0.28680065 0.48559571 0.        ]</code></pre>
</div>
</div>
<div id="cell-76" class="cell" data-execution_count="224">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for I love deep learning</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>tfidfs <span class="op">=</span> [<span class="fl">.25</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">2</span>), <span class="fl">0.25</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">2</span>), <span class="fl">0.25</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">3</span>),<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Before Normalization:"</span>, np.<span class="bu">round</span>(tfidfs, <span class="dv">4</span>))</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>l2norm <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(np.power(np.array(tfidfs), <span class="dv">2</span>)))</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L2 norm:"</span>, l2norm)</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After normalization:"</span>, tfidfs<span class="op">/</span>l2norm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before Normalization: [0.3219 0.3219 0.25   0.     0.    ]
L2 norm: 0.5193896802634936
After normalization: [0.61980538 0.61980538 0.48133417 0.         0.        ]</code></pre>
</div>
</div>
<div id="cell-77" class="cell" data-execution_count="219">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for I love pizza</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>tfidfs <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.33</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">3</span>),<span class="dv">0</span>, <span class="fl">.33</span><span class="op">*</span>idff(<span class="dv">3</span>,<span class="dv">1</span>)]</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Before Normalization:"</span>, np.<span class="bu">round</span>(tfidfs, <span class="dv">4</span>))</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>l2norm <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(np.power(np.array(tfidfs), <span class="dv">2</span>)))</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L2 norm:"</span>, l2norm)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After normalization:"</span>, tfidfs<span class="op">/</span>l2norm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before Normalization: [0.     0.     0.33   0.     0.5587]
L2 norm: 0.6489135451981629
After normalization: [0.         0.         0.50854232 0.         0.861037  ]</code></pre>
</div>
</div>
<div id="cell-78" class="cell" data-execution_count="238">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vectorizer.idf_)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vectorizer.get_feature_names_out())</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>([idff(<span class="dv">3</span>,<span class="dv">2</span>), idff(<span class="dv">3</span>, <span class="dv">2</span>), idff(<span class="dv">3</span>, <span class="dv">3</span>), idff(<span class="dv">3</span>,<span class="dv">1</span>), idff(<span class="dv">3</span>,<span class="dv">1</span>)], <span class="dv">8</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1.28768207 1.28768207 1.         1.69314718 1.69314718]
['deep' 'learning' 'love' 'machine' 'pizza']
[1.28768207 1.28768207 1.         1.69314718 1.69314718]</code></pre>
</div>
</div>
</section>
</section>
<section id="why-does-tf-idf-use-log-in-idf-but-not-in-tf" class="level1">
<h1>🤔 Why Does TF-IDF Use <code>log</code> in IDF but Not in TF?</h1>
<section id="what-is-tf-and-idf" class="level2">
<h2 class="anchored" data-anchor-id="what-is-tf-and-idf">🔹 What is TF and IDF?</h2>
<section id="tf-term-frequency" class="level3">
<h3 class="anchored" data-anchor-id="tf-term-frequency">✅ TF (Term Frequency)</h3>
<p><span class="math display">\[
\text{TF}(t, d) = \frac{\text{Count of term } t \text{ in document } d}{\text{Total number of terms in } d}
\]</span></p>
<ul>
<li>Local to the document</li>
<li>Already a small number (e.g., 0.25 if the word occurs once in a 4-word document)</li>
<li><strong>No need to apply log</strong></li>
</ul>
</section>
<section id="idf-inverse-document-frequency" class="level3">
<h3 class="anchored" data-anchor-id="idf-inverse-document-frequency">✅ IDF (Inverse Document Frequency)</h3>
<p><span class="math display">\[
\text{IDF}(t) = \log \left( \frac{1 + N}{1 + \text{df}(t)} \right) + 1
\]</span></p>
<p>Where: - <code>N</code> = Total number of documents - <code>df(t)</code> = Number of documents containing the term <code>t</code></p>
<ul>
<li>Helps penalize common words like “the”, “and”</li>
<li><strong>Logarithm is used to compress large variations</strong></li>
</ul>
</section>
</section>
<section id="example-what-happens-without-log" class="level2">
<h2 class="anchored" data-anchor-id="example-what-happens-without-log">🔍 Example: What Happens Without <code>log</code>?</h2>
<p>Assume <code>N = 10,000</code> documents:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 39%">
<col style="width: 18%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Document Frequency (<code>df</code>)</th>
<th>Raw IDF</th>
<th>Log IDF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“the”</td>
<td>9999</td>
<td>10000 / 9999 ≈ 1.0001</td>
<td>log(10000 / 9999) ≈ <strong>0.0001</strong></td>
</tr>
<tr class="even">
<td>“AI”</td>
<td>100</td>
<td>10000 / 100 = 100</td>
<td>log(10000 / 100) = <strong>2</strong></td>
</tr>
<tr class="odd">
<td>“chatgpt”</td>
<td>10</td>
<td>10000 / 10 = 1000</td>
<td>log(10000 / 10) = <strong>3</strong></td>
</tr>
</tbody>
</table>
<ul>
<li>Without log, “chatgpt” gets <strong>1000× more weight</strong> than “AI”<br>
</li>
<li>With log, it’s only <strong>1.5× more</strong> — much more stable!</li>
</ul>
</section>
<section id="why-log-the-intuition" class="level2">
<h2 class="anchored" data-anchor-id="why-log-the-intuition">📉 Why Log? The Intuition</h2>
<section id="tf-is-already-small" class="level3">
<h3 class="anchored" data-anchor-id="tf-is-already-small">TF is already small:</h3>
<ul>
<li>Values like 0.1 or 0.2 — no risk of explosion</li>
<li>No need to compress the scale</li>
</ul>
</section>
<section id="idf-can-explode" class="level3">
<h3 class="anchored" data-anchor-id="idf-can-explode">IDF can explode:</h3>
<ul>
<li>Without log, rare terms can dominate</li>
<li>Log <strong>compresses</strong> large values to keep weights manageable</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="what-are-word-embeddings-in-nlp" class="level1">
<h1>What Are Word Embeddings in NLP?</h1>
<p><strong>Word embeddings</strong> are <strong>vector representations</strong> of words that capture their <strong>semantic meaning</strong>.</p>
<p>Unlike one-hot encoding (which treats each word as independent), embeddings place words in a <strong>continuous vector space</strong> where <strong>similar words are close together</strong>.</p>
<section id="why-do-we-need-them" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-need-them">🔍 Why Do We Need Them?</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Problem with Traditional Methods</th>
<th>Solution via Embeddings</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>One-hot vectors are sparse</td>
<td>Dense vectors (e.g., 100–300 dims)</td>
</tr>
<tr class="even">
<td>No semantic relationships</td>
<td>Similar meanings → similar vectors</td>
</tr>
<tr class="odd">
<td>High memory usage</td>
<td>Lower dimensional representation</td>
</tr>
</tbody>
</table>
</section>
<section id="example-2" class="level2">
<h2 class="anchored" data-anchor-id="example-2">💡 Example</h2>
<section id="one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="one-hot-encoding">One-hot encoding:</h3>
<p>[“cat”, “dog”, “apple”]</p>
<p>“cat” = [1, 0, 0]</p>
<p>“dog” = [0, 1, 0]</p>
<p>➡️ All words are <strong>equally distant</strong>, even if related.</p>
</section>
<section id="word-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="word-embeddings">Word Embeddings:</h3>
<p>“cat” = [0.20, 0.45, …, 0.11]</p>
<p>“dog” = [0.21, 0.47, …, 0.09]</p>
<p>“apple”= [0.88, 0.11, …, 0.76]</p>
<p>➡️ <code>cosine_similarity(cat, dog) &gt; cosine_similarity(cat, apple)</code></p>
<p>Word embeddings can be categorized based on how they’re generated and whether they are <strong>context-free</strong> or <strong>contextual</strong>.</p>
</section>
</section>
<section id="static-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="static-word-embeddings">🔹 1. Static Word Embeddings</h2>
<ul>
<li>Each word has a <strong>single fixed vector</strong>, regardless of context.</li>
<li>Doesn’t capture polysemy (e.g., “bank” as river vs.&nbsp;financial institution)</li>
</ul>
<section id="popular-models" class="level3">
<h3 class="anchored" data-anchor-id="popular-models">📦 Popular Models:</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 44%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Word2Vec</strong></td>
<td>Predictive (Neural)</td>
<td>Trains using Skip-Gram or CBOW to predict context</td>
</tr>
<tr class="even">
<td><strong>GloVe</strong></td>
<td>Count-based (Matrix Factorization)</td>
<td>Uses global word co-occurrence statistics</td>
</tr>
<tr class="odd">
<td><strong>FastText</strong></td>
<td>Predictive with subword info</td>
<td>Like Word2Vec, but considers character n-grams</td>
</tr>
<tr class="even">
<td><strong>TF-IDF</strong></td>
<td>Statistical (Non-NN)</td>
<td>Not an embedding model per se, but dense</td>
</tr>
</tbody>
</table>
</section>
<section id="summary-1" class="level3">
<h3 class="anchored" data-anchor-id="summary-1">🔸 Summary:</h3>
<ul>
<li>📌 Fast, lightweight</li>
<li>📉 No contextual understanding</li>
<li>💾 Pretrained vectors available</li>
</ul>
</section>
</section>
<section id="contextual-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="contextual-word-embeddings">🔹 2. Contextual Word Embeddings</h2>
<ul>
<li>Word vectors <strong>change depending on the sentence context</strong></li>
<li>Solves the polysemy problem</li>
</ul>
<section id="popular-models-1" class="level3">
<h3 class="anchored" data-anchor-id="popular-models-1">📦 Popular Models:</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 41%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Architecture</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>ELMo</strong></td>
<td>BiLSTM-based</td>
<td>Embeddings from deep bidirectional LSTM</td>
</tr>
<tr class="even">
<td><strong>BERT</strong></td>
<td>Transformer (masked LM)</td>
<td>Contextualized word vectors using attention</td>
</tr>
<tr class="odd">
<td><strong>RoBERTa</strong>, <strong>ALBERT</strong>, <strong>DistilBERT</strong></td>
<td>Transformer variants</td>
<td>Improve or optimize BERT architecture</td>
</tr>
<tr class="even">
<td><strong>GPT, GPT-2/3/4</strong></td>
<td>Decoder-only Transformers</td>
<td>More generative, can also extract embeddings</td>
</tr>
</tbody>
</table>
</section>
<section id="summary-2" class="level3">
<h3 class="anchored" data-anchor-id="summary-2">🔸 Summary:</h3>
<ul>
<li>✅ Captures word meaning in <strong>context</strong></li>
<li>🔍 Powerful for NER, QA, Sentiment Analysis</li>
<li>💻 Requires more compute, but <strong>state-of-the-art</strong> in NLP</li>
</ul>
</section>
</section>
<section id="quick-comparison-table" class="level2">
<h2 class="anchored" data-anchor-id="quick-comparison-table">✅ Quick Comparison Table</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Static Embeddings</th>
<th>Contextual Embeddings</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Context-aware</td>
<td>❌ No</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td>Vector size</td>
<td>Fixed</td>
<td>Fixed (but varies per model)</td>
</tr>
<tr class="odd">
<td>Training</td>
<td>Shallow NN / Co-occurrence</td>
<td>Deep Transformers</td>
</tr>
<tr class="even">
<td>Example Models</td>
<td>Word2Vec, GloVe, FastText</td>
<td>BERT, ELMo, GPT</td>
</tr>
<tr class="odd">
<td>Pretrained</td>
<td>✅ Widely available</td>
<td>✅ Widely available</td>
</tr>
</tbody>
</table>
</section>
<section id="key-properties" class="level2">
<h2 class="anchored" data-anchor-id="key-properties">✨ Key Properties</h2>
<ul>
<li><strong>Semantic similarity</strong>: “king” and “queen” are close in vector space</li>
<li><strong>Vector arithmetic</strong>: <span class="math display">\[
\text{vec}("king") - \text{vec}("man") + \text{vec}("woman") \approx \text{vec}("queen")
\]</span></li>
<li><strong>Dimensionality</strong>: Typically 100–300 dimensions</li>
<li><strong>Pretrained embeddings</strong>: Can load via <code>gensim</code>, <code>spacy</code>, <code>transformers</code>, etc.</li>
</ul>
</section>
<section id="advantages-2" class="level2">
<h2 class="anchored" data-anchor-id="advantages-2">✅ Advantages</h2>
<ul>
<li>Captures <strong>semantic meaning and relationships</strong></li>
<li>Improves performance of ML and deep learning models</li>
<li>Can use <strong>pretrained vectors</strong> (e.g., <code>GloVe</code>, <code>Word2Vec</code>, <code>FastText</code>)</li>
<li><strong>Compact</strong>: low dimensional and dense vector and fast compute than sparse methods</li>
</ul>
</section>
<section id="limitations-1" class="level2">
<h2 class="anchored" data-anchor-id="limitations-1">⚠️ Limitations</h2>
<ul>
<li><strong>Static embeddings</strong> (like Word2Vec, GloVe) don’t consider <strong>context</strong>
<ul>
<li>E.g., “bank” (river) vs “bank” (money) → same vector</li>
</ul></li>
<li>Need large data to train from scratch</li>
<li>Contextual embeddings (like BERT) solve this, but are heavier</li>
</ul>
<section id="google-news-word2vec-model-300-dimensional" class="level3">
<h3 class="anchored" data-anchor-id="google-news-word2vec-model-300-dimensional"><strong>Google News Word2Vec Model (300-dimensional)</strong></h3>
<section id="overview" class="level4">
<h4 class="anchored" data-anchor-id="overview">Overview</h4>
<ul>
<li>Developed by <strong>Google</strong> using the <strong>Word2Vec</strong> algorithm.</li>
<li>Trained on <strong>Google News dataset</strong> (~100 billion words).</li>
<li>Each word is represented as a <strong>300-dimensional vector</strong>.</li>
</ul>
</section>
<section id="key-characteristics" class="level4">
<h4 class="anchored" data-anchor-id="key-characteristics">📌 Key Characteristics</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Feature</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Type</td>
<td>Word2Vec (Skip-Gram with Negative Sampling)</td>
</tr>
<tr class="even">
<td>Dimensions</td>
<td>300</td>
</tr>
<tr class="odd">
<td>Dataset</td>
<td>Google News (100B words)</td>
</tr>
<tr class="even">
<td>Vocabulary Size</td>
<td>~3 million unique words/phrases</td>
</tr>
<tr class="odd">
<td>Format</td>
<td>Binary <code>.bin</code> file (300d vectors)</td>
</tr>
<tr class="even">
<td>Pretrained?</td>
<td>✅ Yes</td>
</tr>
</tbody>
</table>
<div id="cell-82" class="cell" data-execution_count="171">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(api.BASE_DIR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>/media/naman/NewVolume/Ubuntu/temp/gensim_data</code></pre>
</div>
</div>
<div id="cell-83" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>wv <span class="op">=</span> api.load(<span class="st">'word2vec-google-news-300'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-84" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using a pretrained model</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec, KeyedVectors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-85" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KeyedVectors.load_word2vec_format(<span class="st">"/media/naman/NewVolume/Ubuntu/temp/gensim_data/word2vec-google-news-300/word2vec-google-news-300.gz"</span>, binary<span class="op">=</span><span class="va">True</span>, limit<span class="op">=</span><span class="dv">500000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-86" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>model[<span class="st">'cricket'</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(300,)</code></pre>
</div>
</div>
<div id="cell-87" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(<span class="st">'Soccer'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('soccer', 0.7622618079185486),
 ('Football', 0.7229189276695251),
 ('Lacrosse', 0.7092606425285339),
 ('Youth_Soccer', 0.6637541651725769),
 ('Volleyball', 0.6585472226142883),
 ('Basketball', 0.6463674306869507),
 ('Softball', 0.6419784426689148),
 ('Baseball', 0.6195278167724609),
 ('SOCCER', 0.6015186905860901),
 ('Soccer_Federation', 0.5930034518241882)]</code></pre>
</div>
</div>
<div id="cell-88" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>model.similarity(<span class="st">'football'</span>, <span class="st">'basketball'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.66824675</code></pre>
</div>
</div>
<div id="cell-89" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>model.similarity(<span class="st">'football'</span>,<span class="st">'rugby'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.56163174</code></pre>
</div>
</div>
<div id="cell-90" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>model.doesnt_match([<span class="st">'python'</span>,<span class="st">'chair'</span>,<span class="st">'algorithm'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'chair'</code></pre>
</div>
</div>
<div id="cell-91" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>vec <span class="op">=</span> model[<span class="st">'king'</span>] <span class="op">-</span> model[<span class="st">'man'</span>] <span class="op">+</span> model[<span class="st">'women'</span>]</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>model.most_similar(vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('king', 0.6478992700576782),
 ('queen', 0.535493791103363),
 ('women', 0.5233659148216248),
 ('kings', 0.5162314772605896),
 ('queens', 0.4995364248752594),
 ('princes', 0.46233269572257996),
 ('monarch', 0.45280295610427856),
 ('monarchy', 0.4293173849582672),
 ('crown_prince', 0.42302510142326355),
 ('concubines', 0.4224807024002075)]</code></pre>
</div>
</div>
<div id="cell-92" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>vec <span class="op">=</span> model[<span class="st">'Hollywood'</span>] <span class="op">-</span> model[<span class="st">'USA'</span>] <span class="op">+</span> model[<span class="st">'India'</span>]</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>model.most_similar(vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('Hollywood', 0.7214100956916809),
 ('Bollywood', 0.6288692355155945),
 ('Hindi_cinema', 0.5506490468978882),
 ('tinseltown', 0.5477021932601929),
 ('Bhojpuri_films', 0.5466169118881226),
 ('Kollywood', 0.542259156703949),
 ('tinsel_town', 0.5404236316680908),
 ('Hindi_films', 0.5321155786514282),
 ('Kannada_cinema', 0.5287744402885437),
 ('Malayalam_cinema', 0.5259954333305359)]</code></pre>
</div>
</div>
<div id="cell-93" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [[<span class="st">"i"</span>, <span class="st">"love"</span>, <span class="st">"nlp"</span>], [<span class="st">"nlp"</span>, <span class="st">"is"</span>, <span class="st">"fun"</span>]]</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences, vector_size<span class="op">=</span><span class="dv">50</span>, window<span class="op">=</span><span class="dv">2</span>, min_count<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>vector <span class="op">=</span> model.wv[<span class="st">"nlp"</span>]</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>similar_words <span class="op">=</span> model.wv.most_similar(<span class="st">"nlp"</span>)</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(similar_words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('is', 0.1267007291316986), ('fun', 0.042373016476631165), ('love', -0.01447527389973402), ('i', -0.11821285635232925)]</code></pre>
</div>
</div>
<div id="cell-94" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(model[<span class="st">'Hockey'</span>] <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>model[<span class="st">'stick'</span>] <span class="op">+</span> <span class="fl">1.6</span><span class="op">*</span>model[<span class="st">'Racquet'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('Racquet', 0.737747311592102),
 ('Racquet_Club', 0.5148431062698364),
 ('Racquetball', 0.478988379240036),
 ('Hockey', 0.4638073444366455),
 ('Tennis', 0.4620804190635681),
 ('Roller_Hockey', 0.45459386706352234),
 ('Water_Polo', 0.43689554929733276),
 ('Indoor_Tennis', 0.4363187849521637),
 ('Fitness', 0.43017417192459106),
 ('Varsity_Tennis', 0.42938631772994995)]</code></pre>
</div>
</div>
</section>
</section>
<section id="using-another-model-fasttext-by-facebook-created-by-facebook-ai-research-fasttext-extends-word2vec-by-treating-each-word-as-composed-of-character-n-grams.-this-helps-with-handling-out-of-vocabulary-words-and-morphologically-rich-languages." class="level3">
<h3 class="anchored" data-anchor-id="using-another-model-fasttext-by-facebook-created-by-facebook-ai-research-fasttext-extends-word2vec-by-treating-each-word-as-composed-of-character-n-grams.-this-helps-with-handling-out-of-vocabulary-words-and-morphologically-rich-languages.">using Another model <strong>FastText</strong> by facebook: Created by Facebook AI Research, FastText extends Word2Vec by treating each word as composed of character n-grams. This helps with handling out-of-vocabulary words and morphologically rich languages.</h3>
<div id="cell-96" class="cell">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fasttext</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> fasttext.load_model(<span class="st">'/media/naman/NewVolume/Ubuntu/temp/fasttext/cc.en.300.bin'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.</code></pre>
</div>
</div>
<div id="cell-97" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>word_vector <span class="op">=</span> model.get_word_vector(<span class="st">'hello'</span>)</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>word_vector.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(300,)</code></pre>
</div>
</div>
<div id="cell-98" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>model.get_nearest_neighbors(<span class="st">'hello'</span>, k<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[(0.7143728733062744, 'hellow'),
 (0.7095366716384888, 'hello.'),
 (0.703833818435669, 'hi'),
 (0.6944323182106018, 'hullo'),
 (0.6912142038345337, 'hello-')]</code></pre>
</div>
</div>
<div id="cell-99" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Analogy queries (similar to Word2Vec's king - man + woman = queen)</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>analogies <span class="op">=</span> model.get_analogies(<span class="st">'berlin'</span>, <span class="st">'germany'</span>, <span class="st">'france'</span>)</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a>analogies</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[(0.7303731441497803, 'paris'),
 (0.6408537030220032, 'france.'),
 (0.6393311023712158, 'avignon'),
 (0.6316676139831543, 'paris.'),
 (0.5895596742630005, 'montpellier'),
 (0.5884554386138916, 'rennes'),
 (0.5850598812103271, 'grenoble'),
 (0.5832924246788025, 'london'),
 (0.5806092619895935, 'strasbourg'),
 (0.574320375919342, 'Paris.')]</code></pre>
</div>
</div>
<div id="cell-100" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fasttext</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> fasttext.load_model(<span class="st">'/media/naman/NewVolume/Ubuntu/temp/fasttext/wiki-news-300d-1M.vec'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>

<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)

Cell <span class="ansi-green-fg">In[1], line 2</span>

<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">fasttext</span>

<span class="ansi-green-fg">----&gt; 2</span> model2 <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">fasttext</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">load_model</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">/media/naman/NewVolume/Ubuntu/temp/fasttext/wiki-news-300d-1M.vec</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">)</span>



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/fasttext/FastText.py:441</span>, in <span class="ansi-cyan-fg">load_model</span><span class="ansi-blue-fg">(path)</span>

<span class="ansi-green-fg ansi-bold">    439</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Load a model given a filepath and return a model object."""</span>

<span class="ansi-green-fg ansi-bold">    440</span> eprint(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.</span><span style="color:rgb(175,0,0)">"</span>)

<span class="ansi-green-fg">--&gt; 441</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">_FastText</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model_path</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">path</span><span class="ansi-yellow-bg">)</span>



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/fasttext/FastText.py:98</span>, in <span class="ansi-cyan-fg">_FastText.__init__</span><span class="ansi-blue-fg">(self, model_path, args)</span>

<span class="ansi-green-fg ansi-bold">     96</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>f <span style="color:rgb(98,98,98)">=</span> fasttext<span style="color:rgb(98,98,98)">.</span>fasttext()

<span class="ansi-green-fg ansi-bold">     97</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> model_path <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:

<span class="ansi-green-fg">---&gt; 98</span>     <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">f</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">loadModel</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model_path</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-green-fg ansi-bold">     99</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_words <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-green-fg ansi-bold">    100</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_labels <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>



<span class="ansi-red-fg">ValueError</span>: /media/naman/NewVolume/Ubuntu/temp/fasttext/wiki-news-300d-1M.vec has wrong file format!</pre>
</div>
</div>
</div>
<div id="cell-101" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> KeyedVectors.load_word2vec_format(<span class="st">'/media/naman/NewVolume/Ubuntu/temp/fasttext/wiki-news-300d-1M.vec'</span>, binary<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>

<span class="ansi-red-fg">UnicodeDecodeError</span>                        Traceback (most recent call last)

Cell <span class="ansi-green-fg">In[2], line 2</span>

<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">gensim</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">models</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> KeyedVectors

<span class="ansi-green-fg">----&gt; 2</span> model3 <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">KeyedVectors</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">load_word2vec_format</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">/media/naman/NewVolume/Ubuntu/temp/fasttext/wiki-news-300d-1M.vec</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">binary</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">True</span><span class="ansi-yellow-bg">)</span>



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1719</span>, in <span class="ansi-cyan-fg">KeyedVectors.load_word2vec_format</span><span class="ansi-blue-fg">(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)</span>

<span class="ansi-green-fg ansi-bold">   1672</span> <span style="color:rgb(175,0,255)">@classmethod</span>

<span class="ansi-green-fg ansi-bold">   1673</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">load_word2vec_format</span>(

<span class="ansi-green-fg ansi-bold">   1674</span>         <span style="color:rgb(0,135,0)">cls</span>, fname, fvocab<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>, binary<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>, encoding<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">utf8</span><span style="color:rgb(175,0,0)">'</span>, unicode_errors<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">strict</span><span style="color:rgb(175,0,0)">'</span>,

<span class="ansi-green-fg ansi-bold">   1675</span>         limit<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>, datatype<span style="color:rgb(98,98,98)">=</span>REAL, no_header<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>,

<span class="ansi-green-fg ansi-bold">   1676</span>     ):

<span class="ansi-green-fg ansi-bold">   1677</span> <span style="color:rgb(188,188,188)">    </span><span style="font-style:italic;color:rgb(175,0,0)">"""Load KeyedVectors from a file produced by the original C word2vec-tool format.</span>

<span class="ansi-green-fg ansi-bold">   1678</span> 

<span class="ansi-green-fg ansi-bold">   1679</span> <span style="font-style:italic;color:rgb(175,0,0)">    Warnings</span>

<span class="ansi-green-fg">   (...)</span>

<span class="ansi-green-fg ansi-bold">   1717</span> 

<span class="ansi-green-fg ansi-bold">   1718</span> <span style="font-style:italic;color:rgb(175,0,0)">    """</span>

<span class="ansi-green-fg">-&gt; 1719</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">_load_word2vec_format</span><span class="ansi-yellow-bg">(</span>

<span class="ansi-green-fg ansi-bold">   1720</span> <span class="ansi-yellow-bg">        </span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">cls</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">fname</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">fvocab</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">fvocab</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">binary</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">binary</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">encoding</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">encoding</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">unicode_errors</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">unicode_errors</span><span class="ansi-yellow-bg">,</span>

<span class="ansi-green-fg ansi-bold">   1721</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">limit</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">limit</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">datatype</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">datatype</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">no_header</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">no_header</span><span class="ansi-yellow-bg">,</span>

<span class="ansi-green-fg ansi-bold">   1722</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">)</span>



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/gensim/models/keyedvectors.py:2065</span>, in <span class="ansi-cyan-fg">_load_word2vec_format</span><span class="ansi-blue-fg">(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)</span>

<span class="ansi-green-fg ansi-bold">   2062</span> kv <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">cls</span>(vector_size, vocab_size, dtype<span style="color:rgb(98,98,98)">=</span>datatype)

<span class="ansi-green-fg ansi-bold">   2064</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> binary:

<span class="ansi-green-fg">-&gt; 2065</span>     <span class="ansi-yellow-bg">_word2vec_read_binary</span><span class="ansi-yellow-bg">(</span>

<span class="ansi-green-fg ansi-bold">   2066</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">fin</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">kv</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">counts</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">vocab_size</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">vector_size</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">datatype</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">unicode_errors</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">binary_chunk_size</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">encoding</span>

<span class="ansi-green-fg ansi-bold">   2067</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">)</span>

<span class="ansi-green-fg ansi-bold">   2068</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:

<span class="ansi-green-fg ansi-bold">   2069</span>     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1960</span>, in <span class="ansi-cyan-fg">_word2vec_read_binary</span><span class="ansi-blue-fg">(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)</span>

<span class="ansi-green-fg ansi-bold">   1958</span> new_chunk <span style="color:rgb(98,98,98)">=</span> fin<span style="color:rgb(98,98,98)">.</span>read(binary_chunk_size)

<span class="ansi-green-fg ansi-bold">   1959</span> chunk <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> new_chunk

<span class="ansi-green-fg">-&gt; 1960</span> processed_words, chunk <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_add_bytes_to_kv</span><span class="ansi-yellow-bg">(</span>

<span class="ansi-green-fg ansi-bold">   1961</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">kv</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">counts</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">chunk</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">vocab_size</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">vector_size</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">datatype</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">unicode_errors</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">encoding</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-green-fg ansi-bold">   1962</span> tot_processed_words <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> processed_words

<span class="ansi-green-fg ansi-bold">   1963</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">len</span>(new_chunk) <span style="color:rgb(98,98,98)">&lt;</span> binary_chunk_size:



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1939</span>, in <span class="ansi-cyan-fg">_add_bytes_to_kv</span><span class="ansi-blue-fg">(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)</span>

<span class="ansi-green-fg ansi-bold">   1936</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> i_space <span style="color:rgb(98,98,98)">==</span> <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">1</span> <span style="font-weight:bold;color:rgb(175,0,255)">or</span> (<span style="color:rgb(0,135,0)">len</span>(chunk) <span style="color:rgb(98,98,98)">-</span> i_vector) <span style="color:rgb(98,98,98)">&lt;</span> bytes_per_vector:

<span class="ansi-green-fg ansi-bold">   1937</span>     <span style="font-weight:bold;color:rgb(0,135,0)">break</span>

<span class="ansi-green-fg">-&gt; 1939</span> word <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">chunk</span><span class="ansi-yellow-bg">[</span><span class="ansi-yellow-bg">start</span><span class="ansi-yellow-bg">:</span><span class="ansi-yellow-bg">i_space</span><span class="ansi-yellow-bg">]</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">decode</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">encoding</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">errors</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">unicode_errors</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-green-fg ansi-bold">   1940</span> <span style="font-style:italic;color:rgb(95,135,135)"># Some binary files are reported to have obsolete new line in the beginning of word, remove it</span>

<span class="ansi-green-fg ansi-bold">   1941</span> word <span style="color:rgb(98,98,98)">=</span> word<span style="color:rgb(98,98,98)">.</span>lstrip(<span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>)



<span class="ansi-red-fg">UnicodeDecodeError</span>: 'utf-8' codec can't decode byte 0xa1 in position 0: invalid start byte</pre>
</div>
</div>
</div>
<div id="cell-102" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> fasttext.load_facebook_vectors(<span class="st">'/media/naman/NewVolume/Ubuntu/temp/fasttext/wiki-news-300d-1M.vec'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>

<span class="ansi-red-fg">NotImplementedError</span>                       Traceback (most recent call last)

Cell <span class="ansi-green-fg">In[6], line 1</span>

<span class="ansi-green-fg">----&gt; 1</span> model2 <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">fasttext</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">load_facebook_vectors</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">/media/naman/NewVolume/Ubuntu/temp/fasttext/wiki-news-300d-1M.vec</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">)</span>



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/gensim/models/fasttext.py:784</span>, in <span class="ansi-cyan-fg">load_facebook_vectors</span><span class="ansi-blue-fg">(path, encoding)</span>

<span class="ansi-green-fg ansi-bold">    731</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">load_facebook_vectors</span>(path, encoding<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">utf-8</span><span style="color:rgb(175,0,0)">'</span>):

<span class="ansi-green-fg ansi-bold">    732</span> <span style="color:rgb(188,188,188)">    </span><span style="font-style:italic;color:rgb(175,0,0)">"""Load word embeddings from a model saved in Facebook's native fasttext `.bin` format.</span>

<span class="ansi-green-fg ansi-bold">    733</span> 

<span class="ansi-green-fg ansi-bold">    734</span> <span style="font-style:italic;color:rgb(175,0,0)">    Notes</span>

<span class="ansi-green-fg">   (...)</span>

<span class="ansi-green-fg ansi-bold">    782</span> 

<span class="ansi-green-fg ansi-bold">    783</span> <span style="font-style:italic;color:rgb(175,0,0)">    """</span>

<span class="ansi-green-fg">--&gt; 784</span>     full_model <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_load_fasttext_format</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">path</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">encoding</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">encoding</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">full_model</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">False</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-green-fg ansi-bold">    785</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> full_model<span style="color:rgb(98,98,98)">.</span>wv



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/gensim/models/fasttext.py:808</span>, in <span class="ansi-cyan-fg">_load_fasttext_format</span><span class="ansi-blue-fg">(model_file, encoding, full_model)</span>

<span class="ansi-green-fg ansi-bold">    789</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Load the input-hidden weight matrix from Facebook's native fasttext `.bin` output files.</span>

<span class="ansi-green-fg ansi-bold">    790</span> 

<span class="ansi-green-fg ansi-bold">    791</span> <span style="font-style:italic;color:rgb(175,0,0)">Parameters</span>

<span class="ansi-green-fg">   (...)</span>

<span class="ansi-green-fg ansi-bold">    805</span> 

<span class="ansi-green-fg ansi-bold">    806</span> <span style="font-style:italic;color:rgb(175,0,0)">"""</span>

<span class="ansi-green-fg ansi-bold">    807</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> utils<span style="color:rgb(98,98,98)">.</span>open(model_file, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">rb</span><span style="color:rgb(175,0,0)">'</span>) <span style="font-weight:bold;color:rgb(0,135,0)">as</span> fin:

<span class="ansi-green-fg">--&gt; 808</span>     m <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">gensim</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">models</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_fasttext_bin</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">load</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">fin</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">encoding</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">encoding</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">full_model</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">full_model</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-green-fg ansi-bold">    810</span> model <span style="color:rgb(98,98,98)">=</span> FastText(

<span class="ansi-green-fg ansi-bold">    811</span>     vector_size<span style="color:rgb(98,98,98)">=</span>m<span style="color:rgb(98,98,98)">.</span>dim,

<span class="ansi-green-fg ansi-bold">    812</span>     window<span style="color:rgb(98,98,98)">=</span>m<span style="color:rgb(98,98,98)">.</span>ws,

<span class="ansi-green-fg">   (...)</span>

<span class="ansi-green-fg ansi-bold">    821</span>     max_n<span style="color:rgb(98,98,98)">=</span>m<span style="color:rgb(98,98,98)">.</span>maxn,

<span class="ansi-green-fg ansi-bold">    822</span> )

<span class="ansi-green-fg ansi-bold">    823</span> model<span style="color:rgb(98,98,98)">.</span>corpus_total_words <span style="color:rgb(98,98,98)">=</span> m<span style="color:rgb(98,98,98)">.</span>ntokens



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/gensim/models/_fasttext_bin.py:345</span>, in <span class="ansi-cyan-fg">load</span><span class="ansi-blue-fg">(fin, encoding, full_model)</span>

<span class="ansi-green-fg ansi-bold">    342</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> new_format:

<span class="ansi-green-fg ansi-bold">    343</span>     model<span style="color:rgb(98,98,98)">.</span>update(dim<span style="color:rgb(98,98,98)">=</span>magic, ws<span style="color:rgb(98,98,98)">=</span>version)

<span class="ansi-green-fg">--&gt; 345</span> raw_vocab, vocab_size, nwords, ntokens <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_load_vocab</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">fin</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">new_format</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">encoding</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">encoding</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-green-fg ansi-bold">    346</span> model<span style="color:rgb(98,98,98)">.</span>update(raw_vocab<span style="color:rgb(98,98,98)">=</span>raw_vocab, vocab_size<span style="color:rgb(98,98,98)">=</span>vocab_size, nwords<span style="color:rgb(98,98,98)">=</span>nwords, ntokens<span style="color:rgb(98,98,98)">=</span>ntokens)

<span class="ansi-green-fg ansi-bold">    348</span> vectors_ngrams <span style="color:rgb(98,98,98)">=</span> _load_matrix(fin, new_format<span style="color:rgb(98,98,98)">=</span>new_format)



File <span class="ansi-green-fg">/media/naman/NewVolume/Ubuntu/mytff/lib/python3.11/site-packages/gensim/models/_fasttext_bin.py:198</span>, in <span class="ansi-cyan-fg">_load_vocab</span><span class="ansi-blue-fg">(fin, new_format, encoding)</span>

<span class="ansi-green-fg ansi-bold">    196</span> <span style="font-style:italic;color:rgb(95,135,135)"># Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)</span>

<span class="ansi-green-fg ansi-bold">    197</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> nlabels <span style="color:rgb(98,98,98)">&gt;</span> <span style="color:rgb(98,98,98)">0</span>:

<span class="ansi-green-fg">--&gt; 198</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">NotImplementedError</span>(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">Supervised fastText models are not supported</span><span style="color:rgb(175,0,0)">"</span>)

<span class="ansi-green-fg ansi-bold">    199</span> logger<span style="color:rgb(98,98,98)">.</span>info(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">loading </span><span style="font-weight:bold;color:rgb(175,95,135)">%s</span><span style="color:rgb(175,0,0)"> words for fastText model from </span><span style="font-weight:bold;color:rgb(175,95,135)">%s</span><span style="color:rgb(175,0,0)">"</span>, vocab_size, fin<span style="color:rgb(98,98,98)">.</span>name)

<span class="ansi-green-fg ansi-bold">    201</span> ntokens <span style="color:rgb(98,98,98)">=</span> _struct_unpack(fin, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">@q</span><span style="color:rgb(175,0,0)">'</span>)[<span style="color:rgb(98,98,98)">0</span>]  <span style="font-style:italic;color:rgb(95,135,135)"># number of tokens</span>



<span class="ansi-red-fg">NotImplementedError</span>: Supervised fastText models are not supported</pre>
</div>
</div>
</div>
<div id="cell-103" class="cell">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using wiki-news-model which is in .vec </span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KeyedVectors.load_word2vec_format(<span class="st">'/media/naman/NewVolume/Ubuntu/temp/fasttext/wiki-news-300d-1M.vec'</span>, binary<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-104" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(model[<span class="st">'Hockey'</span>] <span class="op">-</span> model[<span class="st">'stick'</span>] <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>model[<span class="st">'Racquet'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('Racquet', 0.9089058637619019),
 ('Racquets', 0.6725203394889832),
 ('Tennis', 0.628152072429657),
 ('Racquetball', 0.6192691326141357),
 ('Hockey', 0.6138624548912048),
 ('Rackets', 0.6138353943824768),
 ('Shuffleboard', 0.6121164560317993),
 ('Raquet', 0.5950469970703125),
 ('Hardcourt', 0.5910704731941223),
 ('Pickleball', 0.5815674662590027)]</code></pre>
</div>
</div>
<div id="cell-105" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>vec <span class="op">=</span> model[<span class="st">'Hollywood'</span>] <span class="op">-</span> model[<span class="st">'USA'</span>] <span class="op">+</span> model[<span class="st">'India'</span>]</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>model.most_similar(vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('Hollywood', 0.768816351890564),
 ('India', 0.6756782531738281),
 ('Bollywood', 0.6675699353218079),
 ('Bombay', 0.5696168541908264),
 ('Indian', 0.5634967684745789),
 ('subcontinent', 0.5614151358604431),
 ('Tinseltown', 0.5566943287849426),
 ('Mumbai', 0.5549437999725342),
 ('movie-making', 0.5469269752502441),
 ('L.A.', 0.5422771573066711)]</code></pre>
</div>
</div>
<div id="cell-106" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(model[<span class="st">'Bollywood'</span>] <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>model[<span class="st">'Tamil'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('Bollywood', 0.9446541666984558),
 ('Tamil', 0.7436086535453796),
 ('Kollywood', 0.7285736799240112),
 ('bollywood', 0.7230172157287598),
 ('Hindi', 0.710655927658081),
 ('Tollywood', 0.6885508298873901),
 ('filmi', 0.6666260957717896),
 ('tamil', 0.6634833216667175),
 ('Indian', 0.6627074480056763),
 ('Mollywood', 0.6580052375793457)]</code></pre>
</div>
</div>
<p>Gensim’s <strong>Word2Vec</strong> (and related models like <strong>FastText</strong> and <strong>Doc2Vec</strong>) uses a <strong>neural network-based architecture</strong> to generate word embeddings, primarily based on the <strong>Skip-gram</strong> and <strong>CBOW</strong> (Continuous Bag of Words) models introduced by Mikolov et al.&nbsp;(2013). Below is a detailed breakdown of its architecture and training process:</p>
</section>
</section>
<section id="core-architectures-in-gensim" class="level2">
<h2 class="anchored" data-anchor-id="core-architectures-in-gensim"><strong>1. Core Architectures in Gensim</strong></h2>
<p>Gensim supports two main Word2Vec variants: 1. <strong>Skip-gram (SG)</strong><br>
- Predicts <strong>context words</strong> given a <strong>target word</strong>.<br>
- Better for <strong>small datasets</strong> and <strong>rare words</strong>.<br>
- Activated with <code>sg=1</code> in Gensim.</p>
<ol start="2" type="1">
<li><strong>Continuous Bag of Words (CBOW)</strong>
<ul>
<li>Predicts a <strong>target word</strong> from its <strong>context words</strong>.<br>
</li>
<li>Faster and better for <strong>frequent words</strong>.<br>
</li>
<li>Activated with <code>sg=0</code> (default in Gensim).</li>
</ul></li>
</ol>
<p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*bBETsVNLyjnaFJgM9avkeQ.png"></p>
<p>Word2Vec creates a representation of each word present in our vocabulary into a vector. Words used in similar contexts or having semantic relationships are captured effectively through their closeness in the vector space- effectively speaking similar words will have similar word vectors! History. Word2vec was created, patented, and published in 2013 by a team of researchers led by Tomas Mikolov at Google.</p>
<p>Let us consider a classic example: “king”, “queen”, “man”, “girl”, “prince”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="NLP_files/figure-html/cell-110-1-image.png" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div>
<p>In a hypothetical world, vectors could then define the weight of each criteria (for example royalty, masculinity, femininity, age etc.) for each of the given words in our vocabulary.</p>
<p>What we then observe is:</p>
<ul>
<li>As expected, “king”, “queen”, “prince” have similar scores for “royalty” and “girl”, “queen” have similar scores for “femininity”.</li>
<li>An operation that removes “man” from “king”, would yield in a vector very close to “queen” ( “king”- “man” = “queen”)</li>
<li>Vectors “king” and “prince” have the same characteristics, except for age, telling us how they might possibly be semantically related to each other.</li>
</ul>
<p>Word2Vec forms word embeddings that work in a similar fashion except for the fact that the criterion we have used for each of the words are not clearly determinable. What matters to us is the semantic and syntactic relations between words which can still be determined by our model without explicitly having defining features for units of the vector.</p>
<p>Word2Vec has also shown to identify relations like country-capital over larger datasets showing us how powerful word embeddings can be.</p>
<p>Embeddings generated by word2vec can further be used in NLP tasks, such as using it as an input to a CNN to classify text!</p>
</section>
<section id="neural-network-structure" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-structure"><strong>2. Neural Network Structure</strong></h2>
<section id="input-layer" class="level3">
<h3 class="anchored" data-anchor-id="input-layer"><strong>Input Layer</strong></h3>
<ul>
<li>Words are represented as <strong>one-hot vectors</strong> (size = vocabulary <code>V</code>).<br>
</li>
<li>Example: If the vocabulary has 10,000 words, each word is a <code>10,000-dim</code> binary vector with a single <code>1</code> at its index.</li>
</ul>
</section>
<section id="hidden-layer-embedding-layer" class="level3">
<h3 class="anchored" data-anchor-id="hidden-layer-embedding-layer"><strong>Hidden Layer (Embedding Layer)</strong></h3>
<ul>
<li>A <strong>weight matrix</strong> <code>W</code> (size = <code>V × D</code>, where <code>D</code> = embedding dimension, e.g., 300).<br>
</li>
<li>This matrix stores the <strong>word embeddings</strong>.<br>
</li>
<li>Multiplying a one-hot input by <code>W</code> extracts the corresponding embedding (no activation function is used).</li>
</ul>
</section>
<section id="output-layer" class="level3">
<h3 class="anchored" data-anchor-id="output-layer"><strong>Output Layer</strong></h3>
<ul>
<li>Another weight matrix <code>W'</code> (size = <code>D × V</code>).<br>
</li>
<li>For Skip-gram: Outputs probabilities for context words.<br>
</li>
<li>For CBOW: Outputs probability for the target word.<br>
</li>
<li>Uses <strong>softmax</strong> (or approximations like <strong>negative sampling</strong>) to compute probabilities.</li>
</ul>
<p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XBaqjqpnBIXtXLzWjkuLQQ.png"></p>
</section>
</section>
<section id="training-process" class="level2">
<h2 class="anchored" data-anchor-id="training-process"><strong>3. Training Process</strong></h2>
<section id="step-1-sliding-window" class="level3">
<h3 class="anchored" data-anchor-id="step-1-sliding-window"><strong>Step 1: Sliding Window</strong></h3>
<ul>
<li>A fixed-size window (e.g., <code>window=5</code>) slides over sentences to generate <code>(target, context)</code> pairs.<br>
</li>
<li>Example:
<ul>
<li><p><strong>Sentence</strong>: <code>"the quick brown fox jumps"</code><br>
</p></li>
<li><p><strong>Pairs (Skip-gram, window=2)</strong>:</p>
<ul>
<li><code>("brown", "quick")</code>, <code>("brown", "fox")</code>, etc.</li>
</ul></li>
<li><p><strong>Sentence</strong>: <code>"The quick brown fox jumps over the lazy dog"</code></p></li>
<li><p>For the target word “fox” and window=5 (center of the window):</p>
<ul>
<li>Context words: <code>["quick", "brown", "jumps", "over"]</code></li>
</ul></li>
</ul>
(Words at the edges may have fewer context words.) ### <strong>Step 2: Forward Pass</strong></li>
</ul>
<ol type="1">
<li><strong>Input word</strong> → One-hot encoded → Multiplied by <code>W</code> → Gets embedding.<br>
</li>
<li><strong>Skip-gram</strong>: Embedding → Multiplied by <code>W'</code> → Softmax → Predicts context words.<br>
</li>
<li><strong>CBOW</strong>: Averages context word embeddings → Multiplies by <code>W'</code> → Predicts target word.</li>
</ol>
</section>
<section id="step-3-loss-calculation" class="level3">
<h3 class="anchored" data-anchor-id="step-3-loss-calculation"><strong>Step 3: Loss Calculation</strong></h3>
<ul>
<li><p>Uses <strong>negative log-likelihood loss</strong> (cross-entropy).<br>
</p></li>
<li><p>For Skip-gram:</p>
<pre><code>Loss = -Σ log(P(context_word | target_word))</code></pre></li>
</ul>
</section>
<section id="step-4-backpropagation-updates" class="level3">
<h3 class="anchored" data-anchor-id="step-4-backpropagation-updates"><strong>Step 4: Backpropagation &amp; Updates</strong></h3>
<ul>
<li>Gradients are computed and applied to <code>W</code> and <code>W'</code> using <strong>stochastic gradient descent (SGD)</strong>.<br>
</li>
<li>The hidden layer weights (<code>W</code>) become the final <strong>word embeddings</strong>.</li>
</ul>
</section>
</section>
<section id="key-optimizations-in-gensim" class="level2">
<h2 class="anchored" data-anchor-id="key-optimizations-in-gensim"><strong>4. Key Optimizations in Gensim</strong></h2>
<p>To make training efficient, Gensim implements: 1. <strong>Hierarchical Softmax</strong><br>
- Replaces softmax with a <strong>binary Huffman tree</strong> (reduces complexity from <code>O(V)</code> to <code>O(log V)</code>).<br>
- Enabled with <code>hs=1</code>.</p>
<ol start="2" type="1">
<li><strong>Negative Sampling</strong>
<ul>
<li>Instead of updating all output weights, only a few “negative” (incorrect) words are sampled.<br>
</li>
<li>Default: <code>negative=5</code> (5 negative samples per positive example).<br>
</li>
<li>Enabled with <code>hs=0</code> (default).</li>
</ul></li>
<li><strong>Subsampling Frequent Words</strong>
<ul>
<li>Discards very frequent words (e.g., “the”, “a”) with probability <code>P(w) = 1 - sqrt(t / f(w))</code>, where <code>f(w)</code> = word frequency.<br>
</li>
<li>Controlled by <code>sample</code> (e.g., <code>sample=1e-3</code>).</li>
</ul></li>
</ol>
</section>
<section id="comparison-with-fasttext-in-gensim" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-fasttext-in-gensim"><strong>6. Comparison with FastText in Gensim</strong></h2>
<p>If using <code>FastText</code> (via <code>gensim.models.FastText</code>), the architecture adds: - <strong>Subword embeddings</strong>: Words are split into n-grams (e.g., “cat” → <code>&lt;ca</code>, <code>cat</code>, <code>at&gt;</code>).<br>
- Handles <strong>out-of-vocabulary (OOV)</strong> words by averaging subword vectors.</p>
</section>
<section id="limitations-2" class="level2">
<h2 class="anchored" data-anchor-id="limitations-2"><strong>7. Limitations</strong></h2>
<ul>
<li><strong>No Contextual Awareness</strong>: Unlike BERT, embeddings are static (same word always has the same vector).<br>
</li>
<li><strong>Window Limitation</strong>: Struggles with long-range dependencies.<br>
</li>
<li><strong>No Morphological Understanding</strong>: FastText improves this via subwords.</li>
</ul>
<section id="summary-table" class="level3">
<h3 class="anchored" data-anchor-id="summary-table"><strong>Summary Table</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 64%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input</strong></td>
<td>One-hot word vectors</td>
</tr>
<tr class="even">
<td><strong>Hidden Layer</strong></td>
<td>Embedding matrix (<code>W</code>) → Stores final word vectors</td>
</tr>
<tr class="odd">
<td><strong>Output Layer</strong></td>
<td>Context/target prediction (<code>W'</code>)</td>
</tr>
<tr class="even">
<td><strong>Training</strong></td>
<td>Negative sampling/hierarchical softmax</td>
</tr>
<tr class="odd">
<td><strong>Optimizations</strong></td>
<td>Subsampling, parallel training</td>
</tr>
</tbody>
</table>
<p>Gensim’s Word2Vec is <strong>lightweight, efficient, and scalable</strong>, making it ideal for custom embeddings on large corpora. For deeper context, consider <strong>Transformer-based models</strong> (e.g., BERT).</p>
<div id="cell-115" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"cat"</span>, <span class="st">"climb"</span>, <span class="st">"tree"</span>],</span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"dog"</span>, <span class="st">"chase"</span>, <span class="st">"car"</span>],</span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"cat"</span>, <span class="st">"climb"</span>, <span class="st">"car"</span>, <span class="st">"too"</span>],</span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'Dog'</span>, <span class="st">"don't"</span>, <span class="st">'climb'</span>, <span class="st">'trees'</span>]</span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(</span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a>    sentences,</span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a>    vector_size<span class="op">=</span><span class="dv">6</span>,    <span class="co"># Embedding dimension (D)</span></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a>    window<span class="op">=</span><span class="dv">5</span>,          <span class="co"># Sliding window size</span></span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true" tabindex="-1"></a>    sg<span class="op">=</span><span class="dv">0</span>,              <span class="co"># 1=Skip-gram, 0=CBOW</span></span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true" tabindex="-1"></a>    min_count<span class="op">=</span><span class="dv">1</span>,       <span class="co"># Ignores words with freq &lt; min_count</span></span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true" tabindex="-1"></a>    negative<span class="op">=</span><span class="dv">3</span>,       <span class="co"># Number of negative samples</span></span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true" tabindex="-1"></a>    workers<span class="op">=</span><span class="dv">4</span>          <span class="co"># Parallel threads</span></span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb132-19"><a href="#cb132-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-20"><a href="#cb132-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Get word embeddings</span></span>
<span id="cb132-21"><a href="#cb132-21" aria-hidden="true" tabindex="-1"></a>cat_embedding <span class="op">=</span> model.wv[<span class="st">"cat"</span>]</span>
<span id="cb132-22"><a href="#cb132-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cat_embedding)  <span class="co"># E.g., [0.1, 0.2, 0.3, ...]</span></span>
<span id="cb132-23"><a href="#cb132-23" aria-hidden="true" tabindex="-1"></a>similar_words <span class="op">=</span> model.wv.most_similar(<span class="st">"cat"</span>, topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb132-24"><a href="#cb132-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(similar_words)  <span class="co"># E.g., [('dog', 0.92), ('tree', 0.85)]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[-0.07561022  0.10923419 -0.08100267 -0.03026696  0.047943    0.01653123]
[('car', 0.5450406670570374), ('tree', 0.3986128568649292), ('too', -0.005340195260941982), ('Dog', -0.09457076340913773), ('trees', -0.29085853695869446)]</code></pre>
</div>
</div>
<div id="cell-116" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>[x <span class="cf">for</span> x <span class="kw">in</span> <span class="st">"Dog don't climb trees"</span>.split()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['Dog', "don't", 'climb', 'trees']</code></pre>
</div>
</div>
<div id="cell-117" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>    sentences,</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>    vector_size<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>    window<span class="op">=</span><span class="dv">1</span>,    <span class="co"># Smaller window for tiny data</span></span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>    sg<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>    min_count<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a>    negative<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Fewer negatives</span></span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,  <span class="co"># Train longer (default=5)</span></span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.wv.most_similar(<span class="st">"cat"</span>, topn<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Might now show: [('climb', 0.6), ('dog', 0.55)]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('tree', 0.35345423221588135), ('car', 0.0969763919711113)]</code></pre>
</div>
</div>
<p>In <strong>CBOW (Continuous Bag of Words)</strong>, the model predicts the <strong>target word</strong> based on its <strong>context words</strong> (surrounding words within a sliding window). Here’s a step-by-step breakdown of how prediction works in CBOW, using our earlier example:</p>
<hr>
</section>
<section id="example-sentence-setup" class="level3">
<h3 class="anchored" data-anchor-id="example-sentence-setup"><strong>Example Sentence &amp; Setup</strong></h3>
<p><strong>Sentence</strong>: <code>"The king rules the land"</code><br>
<strong>Window Size</strong>: 2 (2 words left + 2 words right of target)<br>
<strong>Training Pair</strong>:<br>
- <strong>Context Words</strong>: <code>["the", "rules"]</code> (left and right of “king”)<br>
- <strong>Target Word</strong>: <code>"king"</code></p>
<section id="one-hot-encoding-1" class="level4">
<h4 class="anchored" data-anchor-id="one-hot-encoding-1"><strong>1. One-Hot Encoding</strong></h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>Index</th>
<th>One-Hot Vector</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>the</td>
<td>0</td>
<td><code>[1, 0, 0, 0]</code></td>
</tr>
<tr class="even">
<td>king</td>
<td>1</td>
<td><code>[0, 1, 0, 0]</code></td>
</tr>
<tr class="odd">
<td>rules</td>
<td>2</td>
<td><code>[0, 0, 1, 0]</code></td>
</tr>
<tr class="even">
<td>land</td>
<td>3</td>
<td><code>[0, 0, 0, 1]</code></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Input (Context)</strong>: <code>["the", "rules"]</code> → One-hot vectors <code>[1,0,0,0]</code> and <code>[0,0,1,0]</code>.<br>
</li>
<li><strong>Target</strong>: <code>"king"</code> → One-hot <code>[0,1,0,0]</code>.</li>
</ul>
</section>
</section>
<section id="neural-network-architecture-cbow" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-architecture-cbow"><strong>2. Neural Network Architecture (CBOW)</strong></h3>
<section id="weight-matrices-same-as-skip-gram-but-used-differently" class="level4">
<h4 class="anchored" data-anchor-id="weight-matrices-same-as-skip-gram-but-used-differently"><strong>Weight Matrices</strong> (Same as Skip-gram but used differently):</h4>
<ul>
<li><p><strong>Embedding Matrix (<code>W₁</code>)</strong>: <code>4 × 3</code> (vocab size × embedding dim)</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>W₁ <span class="op">=</span> [</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>],   <span class="co"># the</span></span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>],   <span class="co"># king</span></span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>],   <span class="co"># rules</span></span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">1.0</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span>]    <span class="co"># land</span></span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Context Matrix (<code>W₂</code>)</strong>: <code>3 × 4</code></p>
<div class="sourceCode" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>W₂ <span class="op">=</span> [</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>],</span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>],</span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
</section>
<section id="forward-pass-prediction-steps" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass-prediction-steps"><strong>3. Forward Pass (Prediction Steps)</strong></h3>
<section id="step-1-average-context-word-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="step-1-average-context-word-embeddings"><strong>Step 1: Average Context Word Embeddings</strong></h4>
<ul>
<li><p>Look up embeddings for <code>"the"</code> and <code>"rules"</code> from <code>W₁</code>:</p>
<ul>
<li><code>W₁["the"]</code> = <code>[0.1, 0.2, 0.3]</code><br>
</li>
<li><code>W₁["rules"]</code> = <code>[0.7, 0.8, 0.9]</code><br>
</li>
</ul></li>
<li><p><strong>Average them</strong>:</p>
<pre><code>h = ([0.1, 0.2, 0.3] + [0.7, 0.8, 0.9]) / 2 = [0.4, 0.5, 0.6]</code></pre></li>
</ul>
</section>
<section id="step-2-compute-output-scores" class="level4">
<h4 class="anchored" data-anchor-id="step-2-compute-output-scores"><strong>Step 2: Compute Output Scores</strong></h4>
<ul>
<li><p>Multiply averaged embedding <code>h</code> by <code>W₂</code>:</p>
<pre><code>z = [0.4, 0.5, 0.6] · W₂ = [0.32, 0.62, 0.92, 1.22]</code></pre>
<ul>
<li>Each value in <code>z</code> corresponds to a word in the vocabulary:
<ul>
<li><code>z[0]</code> = score for “the” = 0.32<br>
</li>
<li><code>z[1]</code> = score for “king” = 0.62<br>
</li>
<li><code>z[2]</code> = score for “rules” = 0.92<br>
</li>
<li><code>z[3]</code> = score for “land” = 1.22</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="step-3-apply-softmax-for-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="step-3-apply-softmax-for-probabilities"><strong>Step 3: Apply Softmax for Probabilities</strong></h4>
<pre><code>ŷ = softmax(z) ≈ [0.10, 0.20, 0.30, 0.40]</code></pre>
<ul>
<li>Predicted probabilities:
<ul>
<li>P(“the”) = 0.10<br>
</li>
<li>P(“king”) = 0.20<br>
</li>
<li>P(“rules”) = 0.30<br>
</li>
<li>P(“land”) = 0.40</li>
</ul></li>
</ul>
</section>
<section id="prediction" class="level4">
<h4 class="anchored" data-anchor-id="prediction"><strong>Prediction</strong>:</h4>
<p>The model predicts <code>"land"</code> (highest probability=0.4), but the correct target is <code>"king"</code> (probability=0.2).</p>
</section>
</section>
<section id="loss-calculation-training" class="level3">
<h3 class="anchored" data-anchor-id="loss-calculation-training"><strong>4. Loss Calculation &amp; Training</strong></h3>
<ul>
<li><p><strong>Cross-Entropy Loss</strong>:</p>
<pre><code>Loss = -log(ŷ[king]) = -log(0.20) ≈ 1.61</code></pre></li>
<li><p><strong>Backpropagation</strong>: Adjust <code>W₁</code> and <code>W₂</code> to increase <code>ŷ[king]</code> and decrease others.</p></li>
</ul>
</section>
<section id="key-differences-from-skip-gram" class="level3">
<h3 class="anchored" data-anchor-id="key-differences-from-skip-gram"><strong>Key Differences from Skip-gram</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 41%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>CBOW</th>
<th>Skip-gram</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input</strong></td>
<td>Average of context word embeddings</td>
<td>Single target word embedding</td>
</tr>
<tr class="even">
<td><strong>Output</strong></td>
<td>Probabilities for target word</td>
<td>Probabilities for context words</td>
</tr>
<tr class="odd">
<td><strong>Training</strong></td>
<td>Faster (batches contexts)</td>
<td>Slower (many target-context pairs)</td>
</tr>
<tr class="even">
<td><strong>Use Case</strong></td>
<td>Frequent words, large datasets</td>
<td>Rare words, small datasets</td>
</tr>
</tbody>
</table>
</section>
<section id="why-cbow-predicts-poorly-here" class="level3">
<h3 class="anchored" data-anchor-id="why-cbow-predicts-poorly-here"><strong>Why CBOW Predicts Poorly Here</strong></h3>
<ol type="1">
<li><strong>Random Initialization</strong>: Weights (<code>W₁</code>, <code>W₂</code>) are untrained.<br>
</li>
<li><strong>Small Example</strong>: With real data, after many iterations:
<ul>
<li><code>"king"</code> and <code>"rules"</code> will co-occur often, so their embeddings become similar.<br>
</li>
<li>The model learns to predict <code>"king"</code> when <code>["the", "rules"]</code> appears.</li>
</ul></li>
</ol>
</section>
<section id="final-embeddings-after-training" class="level3">
<h3 class="anchored" data-anchor-id="final-embeddings-after-training"><strong>Final Embeddings (After Training)</strong></h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>Embedding (D=3)</th>
<th>Closest Word</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>the</td>
<td><code>[0.1, 0.2, 0.3]</code></td>
<td>N/A</td>
</tr>
<tr class="even">
<td>king</td>
<td><code>[0.9, 0.8, 0.7]</code></td>
<td>rules</td>
</tr>
<tr class="odd">
<td>rules</td>
<td><code>[0.8, 0.7, 0.6]</code></td>
<td>king</td>
</tr>
<tr class="even">
<td>land</td>
<td><code>[0.2, 0.3, 0.4]</code></td>
<td>the</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Cosine Similarity</strong>:
<ul>
<li><code>sim(king, rules) ≈ 0.99</code> (high).<br>
</li>
<li><code>sim(king, land) ≈ 0.75</code> (low).</li>
</ul></li>
</ul>
</section>
<section id="summary-3" class="level3">
<h3 class="anchored" data-anchor-id="summary-3"><strong>Summary</strong></h3>
<ol type="1">
<li><strong>CBOW Predicts</strong>: Target word from averaged context embeddings.<br>
</li>
<li><strong>Training Goal</strong>: Maximize probability of the correct target.<br>
</li>
<li><strong>Real-World Use</strong>: Optimized with negative sampling/hierarchical softmax.</li>
</ol>
<p>For implementation, libraries like <strong>Gensim</strong> (<code>sg=0</code>) handle this efficiently. Would you like to see the Gensim code for CBOW?</p>
<p><img src="https://i.ytimg.com/vi/UqRCEmrv1gQ/hq720.jpg?sqp=-oaymwEhCK4FEIIDSFryq4qpAxMIARUAAAAAGAElAADIQj0AgKJD&amp;rs=AOn4CLB_CznHcAtYYdY_fH-Acgflpa7M1g"></p>
</section>
<section id="prediction-in-skip-gram-word2vec-step-by-step-example" class="level3">
<h3 class="anchored" data-anchor-id="prediction-in-skip-gram-word2vec-step-by-step-example"><strong>Prediction in Skip-gram Word2Vec: Step-by-Step Example</strong></h3>
<p>Let’s use a <strong>real sentence</strong> to see how Skip-gram predicts context words from a target word. We’ll walk through the neural network’s calculations.</p>
<section id="example-sentence-setup-1" class="level4">
<h4 class="anchored" data-anchor-id="example-sentence-setup-1"><strong>Example Sentence &amp; Setup</strong></h4>
<p><strong>Sentence</strong>: <code>"The cat climbs the tree"</code><br>
<strong>Vocabulary</strong>: <code>["the", "cat", "climbs", "tree"]</code> (4 words)<br>
<strong>Embedding Size (D)</strong>: 3 (for simplicity)<br>
<strong>Window Size</strong>: 1 (predict 1 word left/right of target)</p>
</section>
<section id="training-pairs-skip-gram" class="level4">
<h4 class="anchored" data-anchor-id="training-pairs-skip-gram"><strong>Training Pairs (Skip-gram)</strong>:</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Target</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“cat”</td>
<td>“the”</td>
</tr>
<tr class="even">
<td>“cat”</td>
<td>“climbs”</td>
</tr>
<tr class="odd">
<td>“climbs”</td>
<td>“cat”</td>
</tr>
<tr class="even">
<td>“climbs”</td>
<td>“the”</td>
</tr>
<tr class="odd">
<td>“tree”</td>
<td>“the”</td>
</tr>
</tbody>
</table>
<p>We’ll focus on the pair <code>(target="cat", context="climbs")</code>.</p>
</section>
</section>
<section id="step-1-one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="step-1-one-hot-encoding"><strong>Step 1: One-Hot Encoding</strong></h3>
<p>Assign each word a unique index:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>Index</th>
<th>One-Hot Vector</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>the</td>
<td>0</td>
<td><code>[1, 0, 0, 0]</code></td>
</tr>
<tr class="even">
<td>cat</td>
<td>1</td>
<td><code>[0, 1, 0, 0]</code></td>
</tr>
<tr class="odd">
<td>climbs</td>
<td>2</td>
<td><code>[0, 0, 1, 0]</code></td>
</tr>
<tr class="even">
<td>tree</td>
<td>3</td>
<td><code>[0, 0, 0, 1]</code></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Input (x)</strong>: One-hot for “cat” = <code>[0, 1, 0, 0]</code>.<br>
</li>
<li><strong>True Output (y)</strong>: One-hot for “climbs” = <code>[0, 0, 1, 0]</code>.</li>
</ul>
</section>
<section id="step-2-initialize-weight-matrices" class="level3">
<h3 class="anchored" data-anchor-id="step-2-initialize-weight-matrices"><strong>Step 2: Initialize Weight Matrices</strong></h3>
<section id="embedding-matrix-w₁---4-x-3-vocab-size-embedding-dim" class="level4">
<h4 class="anchored" data-anchor-id="embedding-matrix-w₁---4-x-3-vocab-size-embedding-dim"><strong>Embedding Matrix (W₁)</strong> - <code>4 x 3</code> (vocab size × embedding dim):</h4>
<div class="sourceCode" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>W₁ <span class="op">=</span> [</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>],   <span class="co"># the</span></span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>],   <span class="co"># cat</span></span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span>],   <span class="co"># climbs</span></span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span>]    <span class="co"># tree</span></span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="context-matrix-w₂---3-x-4" class="level4">
<h4 class="anchored" data-anchor-id="context-matrix-w₂---3-x-4"><strong>Context Matrix (W₂)</strong> - <code>3 x 4</code>:</h4>
<div class="sourceCode" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>W₂ <span class="op">=</span> [</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>],</span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>],</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="step-3-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="step-3-forward-pass"><strong>Step 3: Forward Pass</strong></h3>
<section id="lookup-target-word-embedding" class="level4">
<h4 class="anchored" data-anchor-id="lookup-target-word-embedding"><strong>1. Lookup Target Word Embedding</strong>:</h4>
<p>Multiply input <code>x</code> by <code>W₁</code> to get the hidden layer (<code>h</code>):</p>
<pre><code>h = x · W₁ = [0, 1, 0, 0] · W₁ = [0.4, 0.5, 0.6]  # "cat"'s embedding</code></pre>
</section>
<section id="predict-context-word-climbs" class="level4">
<h4 class="anchored" data-anchor-id="predict-context-word-climbs"><strong>2. Predict Context Word (“climbs”)</strong>:</h4>
<p>Multiply <code>h</code> by <code>W₂</code> to get output scores (<code>z</code>):</p>
<pre><code>z = h · W₂ 
   = [0.4*0.1 + 0.5*0.2 + 0.6*0.3,   # Score for "the"
      0.4*0.3 + 0.5*0.4 + 0.6*0.5,   # Score for "cat"
      0.4*0.5 + 0.5*0.6 + 0.6*0.7,   # Score for "climbs"
      0.4*0.7 + 0.5*0.8 + 0.6*0.9]    # Score for "tree"
   = [0.32, 0.62, 0.92, 1.22]</code></pre>
</section>
<section id="apply-softmax" class="level4">
<h4 class="anchored" data-anchor-id="apply-softmax"><strong>3. Apply Softmax</strong>:</h4>
<p>Convert scores to probabilities (<code>ŷ</code>):</p>
<pre><code>ŷ = softmax(z) = [e^0.32, e^0.62, e^0.92, e^1.22] / sum
   ≈ [0.10, 0.20, 0.30, 0.40]</code></pre>
<ul>
<li>The model predicts “tree” (highest prob=0.4), but the true context is “climbs” (prob=0.3).</li>
</ul>
</section>
</section>
<section id="step-4-loss-calculation-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="step-4-loss-calculation-cross-entropy"><strong>Step 4: Loss Calculation (Cross-Entropy)</strong></h3>
<pre><code>Loss = -log(ŷ[climbs]) = -log(0.30) ≈ 1.20</code></pre>
<ul>
<li>Penalizes the model for assigning low probability to the correct word.</li>
</ul>
</section>
<section id="step-5-backpropagation-updates" class="level3">
<h3 class="anchored" data-anchor-id="step-5-backpropagation-updates"><strong>Step 5: Backpropagation &amp; Updates</strong></h3>
<ol type="1">
<li><strong>Gradients</strong>: Compute how much to adjust <code>W₁</code> and <code>W₂</code> to increase <code>ŷ[climbs]</code>.<br>
</li>
<li><strong>Update Rule</strong>:
<ul>
<li>Increase similarity between <code>cat</code> and <code>climbs</code>.<br>
</li>
<li>Decrease similarity between <code>cat</code> and other words (e.g., “tree”).</li>
</ul></li>
</ol>
<section id="updated-embeddings-after-training" class="level4">
<h4 class="anchored" data-anchor-id="updated-embeddings-after-training"><strong>Updated Embeddings After Training</strong>:</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>Embedding (Learned)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cat</td>
<td>[0.5, 0.6, 0.7]</td>
</tr>
<tr class="even">
<td>climbs</td>
<td>[0.6, 0.7, 0.8]</td>
</tr>
<tr class="odd">
<td>tree</td>
<td>[0.1, 0.2, 0.3]</td>
</tr>
</tbody>
</table>
<ul>
<li>Now, <code>cat</code> and <code>climbs</code> are closer in vector space:
<ul>
<li><code>cosine_sim(cat, climbs) ≈ 0.99</code> (high).<br>
</li>
<li><code>cosine_sim(cat, tree) ≈ 0.75</code> (low).</li>
</ul></li>
</ul>
</section>
</section>
<section id="key-points" class="level3">
<h3 class="anchored" data-anchor-id="key-points"><strong>Key Points</strong></h3>
<ol type="1">
<li><strong>Skip-gram Goal</strong>: Predict context words from a target word.<br>
</li>
<li><strong>Negative Sampling</strong>: Only a few random words are updated (not all 4).<br>
</li>
<li><strong>Why “tree” was Initially Predicted</strong>: Random initialization favors higher indices without training.</li>
</ol>
<section id="example-with-negative-sampling" class="level4">
<h4 class="anchored" data-anchor-id="example-with-negative-sampling"><strong>Example with Negative Sampling</strong>:</h4>
<p>If <code>negative=2</code>, the model might: - Update <code>cat</code> to be closer to <code>climbs</code>.<br>
- Push <code>cat</code> away from 2 random words (e.g., “the”, “tree”).</p>
</section>
</section>
<section id="final-answer" class="level3">
<h3 class="anchored" data-anchor-id="final-answer"><strong>Final Answer</strong></h3>
<p>Skip-gram predicts context words by: 1. Mapping the target word to an embedding (<code>W₁</code>).<br>
2. Computing similarity scores with all context words (<code>W₂</code>).<br>
3. Using softmax (or negative sampling) to refine probabilities.<br>
4. Updating weights to maximize similarity for true pairs.</p>
<p><strong>For meaningful results</strong>, train on large corpora (e.g., Wikipedia) with proper hyperparameters:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences, vector_size<span class="op">=</span><span class="dv">100</span>, window<span class="op">=</span><span class="dv">5</span>, sg<span class="op">=</span><span class="dv">1</span>, negative<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="NLP_files/figure-html/cell-122-1-image.png" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="skip-gram-model-explanation-with-example" class="level1">
<h1>Skip-gram Model Explanation with Example</h1>
<section id="input-data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="input-data-preparation">1. Input Data Preparation</h2>
<p>We have the sentence: <code>["the", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]</code></p>
<p>With window size = 5, this means for each center word, we’ll consider 2 words before and 2 words after as context words.</p>
</section>
<section id="vocabulary-creation" class="level2">
<h2 class="anchored" data-anchor-id="vocabulary-creation">2. Vocabulary Creation</h2>
<p>First, let’s create our vocabulary (unique words):</p>
<p>Vocabulary: <code>["the", "quick", "brown", "fox", "jumps", "over", "lazy", "dog"]</code> (8 words)</p>
<p>Note: “the” appears twice but only included once in vocabulary.</p>
</section>
<section id="one-hot-encoding-2" class="level2">
<h2 class="anchored" data-anchor-id="one-hot-encoding-2">3. One-Hot Encoding</h2>
<p>Each word is represented as a one-hot vector of size 8 (vocabulary size):</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"the"</span>   <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"quick"</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"brown"</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"fox"</span>   <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb151-5"><a href="#cb151-5" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"jumps"</span> <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb151-6"><a href="#cb151-6" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"over"</span>  <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb151-7"><a href="#cb151-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"lazy"</span>  <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb151-8"><a href="#cb151-8" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> <span class="st">"dog"</span>   <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="model-architecture">4. Model Architecture</h2>
<p>Skip-gram has: - Input layer: size V (8) - Hidden layer: size N (let’s assume N=3 for this example) - Output layer: size V (8)</p>
<p>We’ll have two weight matrices: - W (input to hidden): 8x3 - W’ (hidden to output): 3x8</p>
<p>Initialize these randomly (for example, between -0.5 and 0.5).</p>
</section>
<section id="training-process-1" class="level2">
<h2 class="anchored" data-anchor-id="training-process-1">5. Training Process</h2>
<p>Let’s take one training example: Center word = “fox” (position 3)</p>
<p>With window size 5, context words are: [“quick”, “brown”, “jumps”, “over”]</p>
<section id="step-1-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="step-1-forward-pass">Step 1: Forward Pass</h3>
<ol type="1">
<li>Input vector (x): one-hot for <code>"fox" = [0, 0, 0, 1, 0, 0, 0, 0]</code></li>
<li>Hidden layer (h): <code>h = xᵀW = W[3]</code> (since x is one-hot, h is just the 3rd row of W)</li>
<li>Output layer (u): <code>u = hW'</code> (vector of size 8)</li>
<li>Apply softmax: <code>ŷ = softmax(u)</code></li>
</ol>
</section>
<section id="step-2-calculate-error" class="level3">
<h3 class="anchored" data-anchor-id="step-2-calculate-error">Step 2: Calculate Error</h3>
<p>For each context word (“quick”, “brown”, “jumps”, “over”), we calculate the error:</p>
<p>Error = Σ [ -log(ŷ[context_word_index]) ]</p>
</section>
<section id="step-3-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="step-3-backpropagation">Step 3: Backpropagation</h3>
<p>We need to calculate gradients for both weight matrices.</p>
<ol type="1">
<li><p>For W’:</p>
<p><code>∂E/∂W' = hᵀ * (ŷ - y_avg)</code></p>
<p>Where y_avg is the average of all context word one-hot vectors</p></li>
<li><p>For W:</p>
<p><code>∂E/∂W = x * ( (ŷ - y_avg) W'ᵀ )</code></p>
<p>Since x is one-hot, only the row corresponding to “fox” will be updated</p></li>
</ol>
</section>
<section id="step-4-update-weights" class="level3">
<h3 class="anchored" data-anchor-id="step-4-update-weights">Step 4: Update Weights</h3>
<p>Using learning rate η (let’s say η=0.01):</p>
<p><code>W' = W' - η * ∂E/∂W'</code></p>
<p><code>W = W - η * ∂E/∂W</code></p>
<p>Only the row of W corresponding to “fox” will be updated.</p>
</section>
</section>
<section id="multiple-epochs" class="level2">
<h2 class="anchored" data-anchor-id="multiple-epochs">6. Multiple Epochs</h2>
<p>This process repeats for all center words in the sentence, and for multiple epochs until convergence.</p>
</section>
<section id="complete-example-calculation" class="level2">
<h2 class="anchored" data-anchor-id="complete-example-calculation">Complete Example Calculation</h2>
<p>Let’s do a concrete calculation with numbers.</p>
<p>Assume initial weights (randomly initialized between -0.5 and 0.5):</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>W (<span class="dv">8</span><span class="er">x3</span>):</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>[</span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>],  <span class="co"># the</span></span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.4</span>],  <span class="co"># quick</span></span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a> [<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.1</span>],  <span class="co"># brown</span></span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">0.4</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.1</span>],  <span class="co"># fox (our center word)</span></span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a> [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>],  <span class="co"># jumps</span></span>
<span id="cb152-8"><a href="#cb152-8" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.4</span>],  <span class="co"># over</span></span>
<span id="cb152-9"><a href="#cb152-9" aria-hidden="true" tabindex="-1"></a> [<span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.3</span>],  <span class="co"># lazy</span></span>
<span id="cb152-10"><a href="#cb152-10" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.2</span>]   <span class="co"># dog</span></span>
<span id="cb152-11"><a href="#cb152-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb152-12"><a href="#cb152-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-13"><a href="#cb152-13" aria-hidden="true" tabindex="-1"></a>W<span class="st">' (3x8):</span></span>
<span id="cb152-14"><a href="#cb152-14" aria-hidden="true" tabindex="-1"></a><span class="er">[</span></span>
<span id="cb152-15"><a href="#cb152-15" aria-hidden="true" tabindex="-1"></a><span class="er"> [ 0.1, -0.2, 0.3, -0.1, 0.2, -0.3, 0.4, -0.4],</span></span>
<span id="cb152-16"><a href="#cb152-16" aria-hidden="true" tabindex="-1"></a><span class="er"> [-0.2, 0.1, -0.3, 0.2, -0.1, 0.4, -0.3, 0.2],</span></span>
<span id="cb152-17"><a href="#cb152-17" aria-hidden="true" tabindex="-1"></a><span class="er"> [ 0.3, -0.4, 0.2, -0.3, 0.1, -0.2, 0.3, -0.1]</span></span>
<span id="cb152-18"><a href="#cb152-18" aria-hidden="true" tabindex="-1"></a><span class="er">]</span></span>
<span id="cb152-19"><a href="#cb152-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-20"><a href="#cb152-20" aria-hidden="true" tabindex="-1"></a><span class="er">### Forward Pass:</span></span>
<span id="cb152-21"><a href="#cb152-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-22"><a href="#cb152-22" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> h <span class="op">=</span> W[<span class="dv">3</span>] <span class="op">=</span> [<span class="fl">0.4</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.1</span>]</span>
<span id="cb152-23"><a href="#cb152-23" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> u <span class="op">=</span> hW<span class="st">' = [0.4*0.1 + -0.2*-0.2 + 0.1*0.3, ..., 0.4*-0.4 + -0.2*0.2 + 0.1*-0.1]</span></span>
<span id="cb152-24"><a href="#cb152-24" aria-hidden="true" tabindex="-1"></a><span class="er">   = </span>[<span class="fl">0.04</span> <span class="op">+</span> <span class="fl">0.04</span> <span class="op">+</span> <span class="fl">0.03</span>, ..., <span class="op">-</span><span class="fl">0.16</span> <span class="op">-</span> <span class="fl">0.04</span> <span class="op">-</span> <span class="fl">0.01</span>]</span>
<span id="cb152-25"><a href="#cb152-25" aria-hidden="true" tabindex="-1"></a>   <span class="op">=</span> [<span class="fl">0.11</span>, <span class="op">-</span><span class="fl">0.14</span>, <span class="fl">0.19</span>, <span class="op">-</span><span class="fl">0.17</span>, <span class="fl">0.11</span>, <span class="op">-</span><span class="fl">0.24</span>, <span class="fl">0.25</span>, <span class="op">-</span><span class="fl">0.21</span>]</span>
<span id="cb152-26"><a href="#cb152-26" aria-hidden="true" tabindex="-1"></a><span class="fl">3.</span> ŷ <span class="op">=</span> softmax(u) ≈ [<span class="fl">0.13</span>, <span class="fl">0.10</span>, <span class="fl">0.14</span>, <span class="fl">0.11</span>, <span class="fl">0.13</span>, <span class="fl">0.09</span>, <span class="fl">0.16</span>, <span class="fl">0.10</span>]</span>
<span id="cb152-27"><a href="#cb152-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-28"><a href="#cb152-28" aria-hidden="true" tabindex="-1"></a><span class="co">### Context Words:</span></span>
<span id="cb152-29"><a href="#cb152-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-30"><a href="#cb152-30" aria-hidden="true" tabindex="-1"></a><span class="co">"quick"</span> (index <span class="dv">1</span>), <span class="st">"brown"</span> (index <span class="dv">2</span>), <span class="st">"jumps"</span> (index <span class="dv">4</span>), <span class="st">"over"</span> (index <span class="dv">5</span>)</span>
<span id="cb152-31"><a href="#cb152-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-32"><a href="#cb152-32" aria-hidden="true" tabindex="-1"></a>Average context vector y_avg:</span>
<span id="cb152-33"><a href="#cb152-33" aria-hidden="true" tabindex="-1"></a>([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">+</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">+</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">+</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>])<span class="op">/</span><span class="dv">4</span></span>
<span id="cb152-34"><a href="#cb152-34" aria-hidden="true" tabindex="-1"></a><span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb152-35"><a href="#cb152-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-36"><a href="#cb152-36" aria-hidden="true" tabindex="-1"></a><span class="co">### Error Calculation:</span></span>
<span id="cb152-37"><a href="#cb152-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-38"><a href="#cb152-38" aria-hidden="true" tabindex="-1"></a>EI <span class="op">=</span> <span class="op">-</span>log(ŷ[<span class="dv">1</span>]) <span class="op">-</span> log(ŷ[<span class="dv">2</span>]) <span class="op">-</span> log(ŷ[<span class="dv">4</span>]) <span class="op">-</span> log(ŷ[<span class="dv">5</span>])</span>
<span id="cb152-39"><a href="#cb152-39" aria-hidden="true" tabindex="-1"></a>   ≈ <span class="op">-</span>log(<span class="fl">0.10</span>) <span class="op">-</span> log(<span class="fl">0.14</span>) <span class="op">-</span> log(<span class="fl">0.13</span>) <span class="op">-</span> log(<span class="fl">0.09</span>)</span>
<span id="cb152-40"><a href="#cb152-40" aria-hidden="true" tabindex="-1"></a>   ≈ <span class="fl">2.30</span> <span class="op">+</span> <span class="fl">1.97</span> <span class="op">+</span> <span class="fl">2.04</span> <span class="op">+</span> <span class="fl">2.41</span> ≈ <span class="fl">8.72</span></span>
<span id="cb152-41"><a href="#cb152-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-42"><a href="#cb152-42" aria-hidden="true" tabindex="-1"></a><span class="co">### Backpropagation:</span></span>
<span id="cb152-43"><a href="#cb152-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-44"><a href="#cb152-44" aria-hidden="true" tabindex="-1"></a><span class="fl">1.</span> ∂E<span class="op">/</span>∂W<span class="st">' = hᵀ * (ŷ - y_avg)</span></span>
<span id="cb152-45"><a href="#cb152-45" aria-hidden="true" tabindex="-1"></a><span class="er">   ŷ - y_avg = </span>[<span class="fl">0.13</span>, <span class="op">-</span><span class="fl">0.15</span>, <span class="op">-</span><span class="fl">0.06</span>, <span class="fl">0.11</span>, <span class="op">-</span><span class="fl">0.12</span>, <span class="op">-</span><span class="fl">0.16</span>, <span class="fl">0.16</span>, <span class="fl">0.10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="math display">\[
\frac{\partial E}{\partial W'} = \begin{bmatrix}
0.4 \\
-0.2 \\
0.1
\end{bmatrix}
\cdot
\begin{bmatrix}
0.13 &amp; -0.15 &amp; -0.06 &amp; 0.11 &amp; -0.12 &amp; -0.16 &amp; 0.16 &amp; 0.10
\end{bmatrix}
\]</span></p>
<p>This means each element $ $ is calculated as: <span class="math display">\[ \frac{\partial E}{\partial W'_{ij}} = h_i \cdot (\hat{y}_j - y_{\text{avg},j}) \]</span></p>
<section id="resulting-gradient-matrix" class="level3">
<h3 class="anchored" data-anchor-id="resulting-gradient-matrix">Resulting Gradient Matrix:</h3>
<p><span class="math display">\[
\frac{\partial E}{\partial W'} = \begin{bmatrix}
0.4 \times 0.13 &amp; 0.4 \times -0.15 &amp; \cdots &amp; 0.4 \times 0.10 \\
-0.2 \times 0.13 &amp; -0.2 \times -0.15 &amp; \cdots &amp; -0.2 \times 0.10 \\
0.1 \times 0.13 &amp; 0.1 \times -0.15 &amp; \cdots &amp; 0.1 \times 0.10
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
= \begin{bmatrix}
0.052 &amp; -0.060 &amp; -0.024 &amp; 0.044 &amp; -0.048 &amp; -0.064 &amp; 0.064 &amp; 0.040 \\
-0.026 &amp; 0.030 &amp; 0.012 &amp; -0.022 &amp; 0.024 &amp; 0.032 &amp; -0.032 &amp; -0.020 \\
0.013 &amp; -0.015 &amp; -0.006 &amp; 0.011 &amp; -0.012 &amp; -0.016 &amp; 0.016 &amp; 0.010
\end{bmatrix}
\]</span></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="fl">2.</span> ∂E<span class="op">/</span>∂W <span class="op">=</span> x <span class="op">*</span> ( (ŷ <span class="op">-</span> y_avg) W<span class="st">'ᵀ )</span></span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a><span class="er">   Since x is one-hot for "fox", only row 3 of W will be updated.</span></span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a>   (ŷ <span class="op">-</span> y_avg) W<span class="st">'ᵀ = [0.13, -0.15, -0.06, 0.11, -0.12, -0.16, 0.16, 0.10] * W'</span>ᵀ</span>
<span id="cb153-5"><a href="#cb153-5" aria-hidden="true" tabindex="-1"></a>   <span class="op">=</span> [<span class="fl">0.062</span>, <span class="op">-</span><span class="fl">0.081</span>, <span class="fl">0.112</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="weight-update" class="level3">
<h3 class="anchored" data-anchor-id="weight-update">Weight Update:</h3>
<p>After calculating gradients, we update weights with learning rate η=0.01.</p>
<p>This is a simplified version - in practice, we might use negative sampling to make computation more efficient.</p>
</section>
</section>
<section id="final-word-vectors" class="level2">
<h2 class="anchored" data-anchor-id="final-word-vectors">Final Word Vectors</h2>
<p>After training completes, the word vectors are typically taken from the W matrix (input to hidden weights). Each row represents a word vector.</p>
<p>For example, the vector for “fox” would be the 3rd row of W.</p>
</section>
<section id="key-points-1" class="level2">
<h2 class="anchored" data-anchor-id="key-points-1">Key Points:</h2>
<ol type="1">
<li>Skip-gram predicts context words given a center word</li>
<li>Window size determines how many words around the center are considered context</li>
<li>The model learns by adjusting weights to minimize prediction error</li>
<li>Final word vectors capture semantic relationships between words</li>
</ol>
<p>Would you like me to elaborate on any specific part of this process?</p>
<hr>
<p>Let me explain <strong>CBOW (Continuous Bag of Words)</strong> in detail using the same example, contrasting it with Skip-gram to highlight the differences.</p>
</section>
<section id="cbow-vs.-skip-gram-key-difference" class="level2">
<h2 class="anchored" data-anchor-id="cbow-vs.-skip-gram-key-difference"><strong>CBOW vs.&nbsp;Skip-gram Key Difference</strong></h2>
<ul>
<li><strong>Skip-gram</strong>: Predicts context words <strong>from</strong> a center word (1 → N)</li>
<li><strong>CBOW</strong>: Predicts center word <strong>from</strong> context words (N → 1)</li>
</ul>
</section>
<section id="cbow-step-by-step-same-example" class="level2">
<h2 class="anchored" data-anchor-id="cbow-step-by-step-same-example"><strong>CBOW Step-by-Step (Same Example)</strong></h2>
<p><strong>Sentence</strong>: <code>["the", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]</code><br>
<strong>Window size=5</strong>: 2 words before + 2 words after center word<br>
<strong>Center word</strong>: “fox” (position 3)<br>
<strong>Context words</strong>: [“quick”, “brown”, “jumps”, “over”] (positions 1,2,4,5)</p>
<section id="input-representation" class="level3">
<h3 class="anchored" data-anchor-id="input-representation"><strong>1. Input Representation</strong></h3>
<ul>
<li><strong>Context words</strong> (one-hot encoded):
<ul>
<li>“quick” = <code>[0, 1, 0, 0, 0, 0, 0, 0]</code><br>
</li>
<li>“brown” = <code>[0, 0, 1, 0, 0, 0, 0, 0]</code><br>
</li>
<li>“jumps” = <code>[0, 0, 0, 0, 1, 0, 0, 0]</code><br>
</li>
<li>“over” = <code>[0, 0, 0, 0, 0, 1, 0, 0]</code></li>
</ul></li>
<li><strong>Target (center word)</strong>: “fox” = <code>[0, 0, 0, 1, 0, 0, 0, 0]</code> (one-hot)</li>
</ul>
</section>
<section id="model-architecture-1" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture-1"><strong>2. Model Architecture</strong></h3>
<ul>
<li><strong>Input layer</strong>: 4 one-hot vectors (context words) → averaged<br>
</li>
<li><strong>Hidden layer</strong>: Embedding dimension (N=3)<br>
</li>
<li><strong>Output layer</strong>: Predicts center word (softmax over vocabulary)</li>
</ul>
<p><strong>Weight matrices</strong>:<br>
- <code>W</code> (input→hidden): 8×3 (same as Skip-gram)<br>
- <code>W'</code> (hidden→output): 3×8 (same as Skip-gram)</p>
</section>
<section id="forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass"><strong>3. Forward Pass</strong></h3>
<section id="average-context-vectors" class="level4">
<h4 class="anchored" data-anchor-id="average-context-vectors"><strong>(1) Average Context Vectors</strong></h4>
<ul>
<li><p><strong>Input</strong>: Sum context word one-hot vectors, then average:</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>x_avg <span class="op">=</span> ([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">+</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>] </span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>        <span class="op">+</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">+</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a>      <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="compute-hidden-layer-h" class="level4">
<h4 class="anchored" data-anchor-id="compute-hidden-layer-h"><strong>(2) Compute Hidden Layer (h)</strong></h4>
<ul>
<li><p><strong>h = x_avg · W</strong><br>
Since <code>x_avg</code> is a weighted sum, we calculate:</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="fl">0.25</span><span class="op">*</span>W[<span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.25</span><span class="op">*</span>W[<span class="dv">2</span>] <span class="op">+</span> <span class="fl">0.25</span><span class="op">*</span>W[<span class="dv">4</span>] <span class="op">+</span> <span class="fl">0.25</span><span class="op">*</span>W[<span class="dv">5</span>]</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a>  <span class="op">=</span> <span class="fl">0.25</span><span class="op">*</span>[<span class="fl">0.2</span>,<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">0.4</span>] <span class="op">+</span> <span class="fl">0.25</span><span class="op">*</span>[<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">0.2</span>,<span class="op">-</span><span class="fl">0.1</span>] </span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> <span class="fl">0.25</span><span class="op">*</span>[<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">0.3</span>,<span class="op">-</span><span class="fl">0.2</span>] <span class="op">+</span> <span class="fl">0.25</span><span class="op">*</span>[<span class="fl">0.2</span>,<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">0.4</span>]</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.025</span>, <span class="fl">0.125</span>]  <span class="co"># Averaged hidden vector</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="compute-output-scores-u" class="level4">
<h4 class="anchored" data-anchor-id="compute-output-scores-u"><strong>(3) Compute Output Scores (u)</strong></h4>
<ul>
<li><p><strong>u = h · W’</strong></p>
<div class="sourceCode" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.025</span>, <span class="fl">0.125</span>] <span class="op">@</span> W<span class="st">'</span></span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a><span class="er">  = </span>[<span class="dv">0</span><span class="op">*</span><span class="fl">0.1</span> <span class="op">+</span> <span class="fl">0.025</span><span class="op">*-</span><span class="fl">0.2</span> <span class="op">+</span> <span class="fl">0.125</span><span class="op">*</span><span class="fl">0.3</span>,  <span class="co"># "the"</span></span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a>     <span class="dv">0</span><span class="op">*-</span><span class="fl">0.2</span> <span class="op">+</span> <span class="fl">0.025</span><span class="op">*</span><span class="fl">0.1</span> <span class="op">+</span> <span class="fl">0.125</span><span class="op">*-</span><span class="fl">0.4</span>, <span class="co"># "quick"</span></span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a>     ... ]  <span class="co"># (1×8 vector)</span></span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a>  ≈ [<span class="fl">0.0325</span>, <span class="op">-</span><span class="fl">0.0475</span>, <span class="fl">0.0175</span>, <span class="op">-</span><span class="fl">0.0375</span>, <span class="fl">0.0125</span>, <span class="op">-</span><span class="fl">0.0525</span>, <span class="fl">0.0425</span>, <span class="op">-</span><span class="fl">0.0125</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="softmax-ŷ" class="level4">
<h4 class="anchored" data-anchor-id="softmax-ŷ"><strong>(4) Softmax (ŷ)</strong></h4>
<ul>
<li><p><strong>ŷ = softmax(u)</strong></p>
<div class="sourceCode" id="cb157"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a>ŷ ≈ [<span class="fl">0.13</span>, <span class="fl">0.10</span>, <span class="fl">0.12</span>, <span class="fl">0.10</span>, <span class="fl">0.12</span>, <span class="fl">0.09</span>, <span class="fl">0.14</span>, <span class="fl">0.10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Target</strong>: “fox” (index 3, true y = <code>[0,0,0,1,0,0,0,0]</code>)</li>
</ul></li>
</ul>
</section>
</section>
<section id="loss-calculation-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="loss-calculation-cross-entropy"><strong>4. Loss Calculation (Cross-Entropy)</strong></h3>
<ul>
<li><p><strong>Error (E) = -log(ŷ[3])</strong></p>
<div class="sourceCode" id="cb158"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> <span class="op">-</span>log(<span class="fl">0.10</span>) ≈ <span class="fl">2.302</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
</section>
<section id="backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation"><strong>5. Backpropagation</strong></h3>
<section id="gradient-for-w-hiddenoutput" class="level4">
<h4 class="anchored" data-anchor-id="gradient-for-w-hiddenoutput"><strong>(1) Gradient for W’ (Hidden→Output)</strong></h4>
<ul>
<li><p><strong>∂E/∂W’ = hᵀ · (ŷ - y)</strong></p>
<div class="sourceCode" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>ŷ <span class="op">-</span> y <span class="op">=</span> [<span class="fl">0.13</span>, <span class="fl">0.10</span>, <span class="fl">0.12</span>, <span class="op">-</span><span class="fl">0.90</span>, <span class="fl">0.12</span>, <span class="fl">0.09</span>, <span class="fl">0.14</span>, <span class="fl">0.10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p><strong>Update</strong>:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a>∂E<span class="op">/</span>∂W<span class="st">' = [0; 0.025; 0.125] @ [0.13, 0.10, ..., 0.10]</span></span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a><span class="er">       = 3×8 matrix </span>(each column <span class="op">=</span> h scaled by (ŷ_j <span class="op">-</span> y_j))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
</ul>
</section>
<section id="gradient-for-w-inputhidden" class="level4">
<h4 class="anchored" data-anchor-id="gradient-for-w-inputhidden"><strong>(2) Gradient for W (Input→Hidden)</strong></h4>
<ul>
<li><strong>∂E/∂W = x_avg · (ŷ - y) · W’ᵀ</strong>
<ul>
<li><p>Only rows of W for <strong>context words</strong> (“quick”, “brown”, “jumps”, “over”) are updated.<br>
</p></li>
<li><p>Example for “quick” (index 1):</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>W[<span class="dv">1</span>] <span class="op">+=</span> <span class="op">-</span>η <span class="op">*</span> (ŷ <span class="op">-</span> y) · W<span class="st">'[:,1] * 0.25</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul></li>
</ul>
</section>
</section>
<section id="weight-update-1" class="level3">
<h3 class="anchored" data-anchor-id="weight-update-1"><strong>6. Weight Update</strong></h3>
<ul>
<li><strong>Learning rate (η=0.01)</strong>
<ul>
<li>Adjust <code>W'</code> and <code>W</code> (only rows for context words) using gradients.</li>
</ul></li>
</ul>
</section>
</section>
<section id="key-differences-from-skip-gram-1" class="level2">
<h2 class="anchored" data-anchor-id="key-differences-from-skip-gram-1"><strong>Key Differences from Skip-gram</strong></h2>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 42%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>CBOW</strong></th>
<th><strong>Skip-gram</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input</strong></td>
<td>Average of context words</td>
<td>Single center word</td>
</tr>
<tr class="even">
<td><strong>Output</strong></td>
<td>Predict center word</td>
<td>Predict context words</td>
</tr>
<tr class="odd">
<td><strong>Backprop</strong></td>
<td>Updates W for all context words</td>
<td>Updates W only for center word</td>
</tr>
<tr class="even">
<td><strong>Efficiency</strong></td>
<td>Faster (batches context)</td>
<td>Slower (multiple pairs per center)</td>
</tr>
<tr class="odd">
<td><strong>Performance</strong></td>
<td>Better for frequent words and small Dataset</td>
<td>Better for rare words and large dataset</td>
</tr>
</tbody>
</table>
</section>
<section id="final-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="final-word-embeddings"><strong>Final Word Embeddings</strong></h2>
<ul>
<li><p>After training, <strong>rows of <code>W</code></strong> become word vectors:</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="co">"fox"</span> ≈ W[<span class="dv">3</span>] <span class="op">=</span> [<span class="fl">0.4</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.1</span>]  <span class="co"># Updated via gradient descent</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
<section id="negative-sampling-in-word2vec-simplified-explanation" class="level3">
<h3 class="anchored" data-anchor-id="negative-sampling-in-word2vec-simplified-explanation"><strong>Negative Sampling in Word2Vec: Simplified Explanation</strong></h3>
<p>Negative sampling is a <strong>computational shortcut</strong> used to train Word2Vec models (both Skip-gram and CBOW) efficiently. Instead of calculating probabilities for <em>all words</em> in the vocabulary (which is slow for large vocabularies), it focuses on:</p>
<ol type="1">
<li><strong>One real “positive” word</strong> (actual context word)<br>
</li>
<li><strong>A few “negative” words</strong> (randomly selected, unrelated words)</li>
</ol>
</section>
<section id="why-do-we-need-negative-sampling" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-need-negative-sampling"><strong>Why Do We Need Negative Sampling?</strong></h3>
<ul>
<li>In vanilla Word2Vec, the output layer requires a <strong>softmax over the entire vocabulary</strong> (e.g., 1M words → 1M calculations per training step).<br>
</li>
<li>This is <strong>too slow</strong> for practical training.<br>
</li>
<li>Negative sampling <strong>approximates</strong> the softmax by only updating a small subset of words.</li>
</ul>
</section>
<section id="how-it-works-step-by-step" class="level3">
<h3 class="anchored" data-anchor-id="how-it-works-step-by-step"><strong>How It Works (Step-by-Step)</strong></h3>
<section id="for-each-training-example" class="level4">
<h4 class="anchored" data-anchor-id="for-each-training-example"><strong>1. For Each Training Example:</strong></h4>
<ul>
<li><strong>Positive Pair</strong>: <code>(target_word, true_context_word)</code><br>
Example: <code>("fox", "jumps")</code><br>
</li>
<li><strong>Negative Samples</strong>: Randomly select <code>k</code> words that <em>do not</em> appear in the context.<br>
Example: If <code>k=3</code>, negative words might be <code>["the", "apple", "car"]</code>.</li>
</ul>
</section>
<section id="weight-updates" class="level4">
<h4 class="anchored" data-anchor-id="weight-updates"><strong>2. Weight Updates:</strong></h4>
<ul>
<li><strong>Push</strong> the target word (<code>"fox"</code>) <strong>closer</strong> to the true context word (<code>"jumps"</code>).<br>
</li>
<li><strong>Push</strong> the target word (<code>"fox"</code>) <strong>away</strong> from negative words (<code>"the"</code>, <code>"apple"</code>, <code>"car"</code>).</li>
</ul>
</section>
<section id="mathematical-intuition" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-intuition"><strong>3. Mathematical Intuition</strong>:</h4>
<p>The loss function becomes: <span class="math display">\[
   \text{Loss} = -\log(\sigma(\mathbf{v}_{\text{fox}} \cdot \mathbf{v}_{\text{jumps}})) - \sum_{i=1}^k \log(\sigma(-\mathbf{v}_{\text{fox}} \cdot \mathbf{v}_{\text{negative}_i}))
   \]</span> - <span class="math inline">\(\sigma\)</span> = sigmoid function<br>
- First term: Maximize similarity with true context.<br>
- Second term: Minimize similarity with negative samples.</p>
</section>
</section>
<section id="how-are-negative-words-selected" class="level3">
<h3 class="anchored" data-anchor-id="how-are-negative-words-selected"><strong>How Are Negative Words Selected?</strong></h3>
<p>Words are sampled based on their <strong>frequency in the training data</strong>, but with a twist:<br>
<span class="math display">\[
P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=1}^V f(w_j)^{3/4}}
\]</span><br>
- <span class="math inline">\(f(w_i)\)</span> = frequency of word <span class="math inline">\(w_i\)</span> in the corpus.<br>
- The <span class="math inline">\(^{3/4}\)</span> power <strong>reduces the dominance of very frequent words</strong> (e.g., “the”, “a”).</p>
<p><strong>Example</strong>:<br>
If the corpus has:<br>
- “the” (1000 occurrences)<br>
- “fox” (10 occurrences)<br>
Then:<br>
<span class="math display">\[
P(\text{the}) \propto 1000^{3/4} \approx 177.8 \\
P(\text{fox}) \propto 10^{3/4} \approx 5.6 \\
\]</span><br>
So “the” is ~32x more likely to be sampled than “fox”, but much less than its raw frequency (100x).</p>
</section>
<section id="example-with-calculations" class="level3">
<h3 class="anchored" data-anchor-id="example-with-calculations"><strong>Example with Calculations</strong></h3>
<p>Let’s train on <code>("fox", "jumps")</code> with negative samples <code>["the", "apple"]</code> (<code>k=2</code>).</p>
<section id="embeddings-simplified-to-2d" class="level4">
<h4 class="anchored" data-anchor-id="embeddings-simplified-to-2d"><strong>Embeddings</strong> (simplified to 2D):</h4>
<ul>
<li>“fox”: <span class="math inline">\([0.4, 0.6]\)</span><br>
</li>
<li>“jumps”: <span class="math inline">\([0.8, 0.2]\)</span><br>
</li>
<li>“the”: <span class="math inline">\([0.1, 0.9]\)</span><br>
</li>
<li>“apple”: <span class="math inline">\([0.5, 0.3]\)</span></li>
</ul>
</section>
<section id="positive-update-fox-jumps" class="level4">
<h4 class="anchored" data-anchor-id="positive-update-fox-jumps"><strong>1. Positive Update (“fox” → “jumps”)</strong>:</h4>
<ul>
<li>Similarity: <span class="math inline">\(0.4 \times 0.8 + 0.6 \times 0.2 = 0.44\)</span><br>
</li>
<li>Sigmoid: <span class="math inline">\(\sigma(0.44) \approx 0.61\)</span><br>
</li>
<li><strong>Gradient</strong>: <span class="math inline">\(1 - 0.61 = 0.39\)</span><br>
</li>
<li>Adjust “fox” and “jumps” to increase similarity.</li>
</ul>
</section>
<section id="negative-update-fox-the" class="level4">
<h4 class="anchored" data-anchor-id="negative-update-fox-the"><strong>2. Negative Update (“fox” → “the”)</strong>:</h4>
<ul>
<li>Similarity: <span class="math inline">\(0.4 \times 0.1 + 0.6 \times 0.9 = 0.58\)</span><br>
</li>
<li>Sigmoid: <span class="math inline">\(\sigma(-0.58) \approx 0.36\)</span><br>
</li>
<li><strong>Gradient</strong>: <span class="math inline">\(0 - 0.36 = -0.36\)</span><br>
</li>
<li>Adjust “fox” and “the” to decrease similarity.</li>
</ul>
</section>
<section id="negative-update-fox-apple" class="level4">
<h4 class="anchored" data-anchor-id="negative-update-fox-apple"><strong>3. Negative Update (“fox” → “apple”)</strong>:</h4>
<ul>
<li>Similarity: <span class="math inline">\(0.4 \times 0.5 + 0.6 \times 0.3 = 0.38\)</span><br>
</li>
<li>Sigmoid: <span class="math inline">\(\sigma(-0.38) \approx 0.41\)</span><br>
</li>
<li><strong>Gradient</strong>: <span class="math inline">\(0 - 0.41 = -0.41\)</span><br>
</li>
<li>Adjust “fox” and “apple” to decrease similarity.</li>
</ul>
</section>
<section id="final-weight-updates" class="level4">
<h4 class="anchored" data-anchor-id="final-weight-updates"><strong>Final Weight Updates</strong>:</h4>
<ul>
<li>“fox” += <span class="math inline">\(\eta \times [\text{positive grad} \times \text{jumps} + \text{negative grads} \times \text{negatives}]\)</span><br>
</li>
<li>Analogous updates for context words.</li>
</ul>
</section>
</section>
<section id="why-does-this-work" class="level3">
<h3 class="anchored" data-anchor-id="why-does-this-work"><strong>Why Does This Work?</strong></h3>
<ul>
<li><strong>Efficiency</strong>: Only <span class="math inline">\(k+1\)</span> words updated per step (vs.&nbsp;entire vocabulary).<br>
</li>
<li><strong>Quality</strong>: Frequent words are sampled more but downweighted by <span class="math inline">\(^{3/4}\)</span>.<br>
</li>
<li><strong>Empirical Result</strong>: Works almost as well as full softmax but much faster.</li>
</ul>
</section>
<section id="key-parameters-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="key-parameters-in-practice"><strong>Key Parameters in Practice</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 69%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Typical Value</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>k</code> (negative)</td>
<td>5-20</td>
<td>Higher = better accuracy but slower. Mikolov recommends 5 for small datasets, 15 for large ones.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(^{3/4}\)</span> power</td>
<td>Fixed</td>
<td>Reduces oversampling of frequent words.</td>
</tr>
<tr class="odd">
<td>Subsampling</td>
<td><span class="math inline">\(10^{-5}\)</span></td>
<td>Drops very frequent words (e.g., “the”) during training.</td>
</tr>
</tbody>
</table>
</section>
<section id="comparison-to-full-softmax" class="level3">
<h3 class="anchored" data-anchor-id="comparison-to-full-softmax"><strong>Comparison to Full Softmax</strong></h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Updates per Step</th>
<th>Quality</th>
<th>Speed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Full Softmax</td>
<td>Entire vocabulary</td>
<td>Best</td>
<td>Slow</td>
</tr>
<tr class="even">
<td>Negative Sampling</td>
<td>k+1 words</td>
<td>Very Good</td>
<td>Fast</td>
</tr>
<tr class="odd">
<td>Hierarchical Softmax</td>
<td><span class="math inline">\(\log(V)\)</span></td>
<td>Good</td>
<td>Medium</td>
</tr>
</tbody>
</table>
</section>
<section id="when-to-use-negative-sampling" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-negative-sampling"><strong>When to Use Negative Sampling?</strong></h3>
<ul>
<li>Always for large vocabularies (e.g., &gt;10K words).<br>
</li>
<li>Use with both Skip-gram and CBOW.<br>
</li>
<li>Combine with <strong>subsampling</strong> of frequent words for best results.</li>
</ul>
<p>Negative sampling is why Word2Vec trains quickly while still learning high-quality</p>
<div id="cell-126" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb163-4"><a href="#cb163-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb163-5"><a href="#cb163-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-127" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"/media/naman/NewVolume/Data_Science/csv_data/Corona_NLP_train.csv"</span>, encoding<span class="op">=</span><span class="st">'latin1'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-128" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb165"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">UserName</th>
<th data-quarto-table-cell-role="th">ScreenName</th>
<th data-quarto-table-cell-role="th">Location</th>
<th data-quarto-table-cell-role="th">TweetAt</th>
<th data-quarto-table-cell-role="th">OriginalTweet</th>
<th data-quarto-table-cell-role="th">Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>3799</td>
<td>48751</td>
<td>London</td>
<td>16-03-2020</td>
<td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>
<td>Neutral</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>3800</td>
<td>48752</td>
<td>UK</td>
<td>16-03-2020</td>
<td>advice Talk to your neighbours family to excha...</td>
<td>Positive</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>3801</td>
<td>48753</td>
<td>Vagabonds</td>
<td>16-03-2020</td>
<td>Coronavirus Australia: Woolworths to give elde...</td>
<td>Positive</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3802</td>
<td>48754</td>
<td>NaN</td>
<td>16-03-2020</td>
<td>My food stock is not the only one which is emp...</td>
<td>Positive</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>3803</td>
<td>48755</td>
<td>NaN</td>
<td>16-03-2020</td>
<td>Me, ready to go at supermarket during the #COV...</td>
<td>Extremely Negative</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="cell-129" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb166"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>df.OriginalTweet[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8'</code></pre>
</div>
</div>
<div id="cell-130" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a>df.Sentiment.unique()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',
       'Extremely Positive'], dtype=object)</code></pre>
</div>
</div>
<div id="cell-131" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>df.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(41157, 6)</code></pre>
</div>
</div>
<div id="cell-132" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a>stop_wrds <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-133" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb173"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>texts<span class="op">=</span>[]</span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df)):</span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'@\w+'</span>, <span class="st">''</span>, df[<span class="st">'OriginalTweet'</span>][i])</span>
<span id="cb173-4"><a href="#cb173-4" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.lower()</span>
<span id="cb173-5"><a href="#cb173-5" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> text.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop_wrds]</span>
<span id="cb173-6"><a href="#cb173-6" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="st">" "</span>.join(text) </span>
<span id="cb173-7"><a href="#cb173-7" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'https?://\S+'</span>, <span class="st">''</span>, text)</span>
<span id="cb173-8"><a href="#cb173-8" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'[^a-zA-Z]'</span>,<span class="st">' '</span>,text)</span>
<span id="cb173-9"><a href="#cb173-9" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.split()</span>
<span id="cb173-10"><a href="#cb173-10" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="st">" "</span>.join(text)    </span>
<span id="cb173-11"><a href="#cb173-11" aria-hidden="true" tabindex="-1"></a>    texts.append(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-134" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> pd.DataFrame(texts, columns<span class="op">=</span>[<span class="st">'text'</span>])</span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">'length'</span>] <span class="op">=</span> dataset[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x.split()))</span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">'Sentiment'</span>] <span class="op">=</span> df[<span class="st">'Sentiment'</span>]</span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(41157, 3)</code></pre>
</div>
</div>
<div id="cell-135" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.query(<span class="st">"length&gt;3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-136" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb177"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a>dataset.sample(<span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">length</th>
<th data-quarto-table-cell-role="th">Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">21841</td>
<td>coronavirus man terror charge coughing superma...</td>
<td>9</td>
<td>Negative</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">27381</td>
<td>heartbreaking scenes thirteen year old ismail ...</td>
<td>28</td>
<td>Extremely Negative</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">32485</td>
<td>rise covid means people look face masks added ...</td>
<td>22</td>
<td>Positive</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">27046</td>
<td>food get supermarkets coronavirus</td>
<td>4</td>
<td>Neutral</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">36918</td>
<td>finnish study finds virus droplets linger amp ...</td>
<td>33</td>
<td>Neutral</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15055</td>
<td>it s frustrating trying plan weekly meals groc...</td>
<td>12</td>
<td>Extremely Negative</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">31210</td>
<td>basic commodity prices goods grocery shop incr...</td>
<td>17</td>
<td>Positive</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="cell-137" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a>dataset.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(40831, 3)</code></pre>
</div>
</div>
<div id="cell-138" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> <span class="bu">list</span>(dataset[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.split(<span class="st">' '</span>)))</span>
<span id="cb180-3"><a href="#cb180-3" aria-hidden="true" tabindex="-1"></a>w2v <span class="op">=</span> Word2Vec(sentences, vector_size<span class="op">=</span><span class="dv">100</span>, workers<span class="op">=</span><span class="dv">4</span>, sg<span class="op">=</span><span class="dv">0</span>, negative<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb180-4"><a href="#cb180-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-5"><a href="#cb180-5" aria-hidden="true" tabindex="-1"></a><span class="co"># printing words vocabulary</span></span>
<span id="cb180-6"><a href="#cb180-6" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">list</span>(w2v.wv.key_to_index)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-139" class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb181"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(words[:<span class="dv">10</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['covid', 'coronavirus', 'prices', 'food', 'supermarket', 'store', 'grocery', 's', 'people', 'amp']</code></pre>
</div>
</div>
<div id="cell-140" class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb183"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(words))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>10520</code></pre>
</div>
</div>
<div id="cell-141" class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb185"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w2v.wv[<span class="st">'computer'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[-2.07698673e-01  6.12112843e-02  1.19350202e-01 -3.61303426e-02
 -3.96921225e-02 -3.92291367e-01 -8.85745436e-02  2.96499252e-01
 -1.80189952e-01 -2.67454863e-01 -1.96686268e-01 -1.24954894e-01
  9.54386815e-02  1.76839992e-01  4.50490322e-03 -2.32539922e-01
  9.23527330e-02 -2.96658963e-01  7.79727921e-02 -5.89567959e-01
  1.28139764e-01 -1.25212446e-02  2.47814000e-01 -2.22039029e-01
 -5.41404895e-02  1.14116259e-01 -3.18993688e-01  3.43159102e-02
 -2.27671698e-01 -1.67719498e-01  9.86263230e-02  1.82044476e-01
  3.10156625e-02 -2.41357654e-01 -7.15568662e-02  1.35081142e-01
 -4.47852500e-02 -2.00150698e-01 -2.60598421e-01 -5.00763595e-01
 -1.58197522e-01 -3.43155041e-02 -1.51660040e-01 -1.17778234e-01
  1.75426155e-01 -6.07219934e-02 -1.65209100e-01 -6.43974766e-02
  1.11663081e-01  1.14305876e-01  4.22266312e-02 -2.44805038e-01
 -9.83127579e-02 -3.08964215e-02 -1.71505243e-01  6.94744214e-02
 -7.71416798e-02 -4.84658740e-02 -2.08919033e-01  8.34747553e-02
  8.19998458e-02 -4.39432859e-02 -1.37404218e-01  1.27804596e-02
 -1.68417573e-01  1.71730042e-01  8.59507695e-02  2.98987150e-01
 -2.41160959e-01  4.57592010e-01 -3.75977568e-02  1.87043026e-01
  4.28610414e-01  4.50243801e-02  2.53926426e-01  7.65921995e-02
 -5.20501658e-02 -9.33884904e-02 -4.09226716e-01  6.72520092e-03
 -3.03442121e-01 -2.65504961e-04 -1.91432118e-01  3.59919310e-01
 -5.33821434e-02 -3.91633762e-03 -1.03366099e-01  9.90526378e-02
  2.39067271e-01  1.75136015e-01  1.92214206e-01  8.78731683e-02
  6.02720007e-02  1.35731593e-01  4.51122671e-01  2.29124680e-01
 -4.53242334e-03 -1.47995353e-01  3.59882899e-02  5.46112880e-02]</code></pre>
</div>
</div>
<div id="cell-142" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb187"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a>w2v.wv.similarity(<span class="st">'vladimir'</span>, <span class="st">'putin'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.9099802</code></pre>
</div>
</div>
<div id="cell-143" class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb189"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>w2v.wv.similarity(<span class="st">'vladimir'</span>, <span class="st">'modi'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.919796</code></pre>
</div>
</div>
<div id="cell-144" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb191"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w2v.wv.most_similar(<span class="st">'india'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('nigeria', 0.8200169801712036), ('asia', 0.8106220364570618), ('collapse', 0.804771900177002), ('europe', 0.8036248683929443), ('oilprice', 0.7981177568435669), ('realestate', 0.7968825697898865), ('bitcoin', 0.7958385348320007), ('worldwide', 0.7921236753463745), ('crash', 0.7859154939651489), ('epidemic', 0.7819210290908813)]</code></pre>
</div>
</div>
<div id="cell-145" class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb193"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w2v.wv.most_similar(<span class="st">'covid'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('coronavirus', 0.7000964283943176), ('corona', 0.6295691132545471), ('coronavirusupdate', 0.6275039911270142), ('virus', 0.6267353892326355), ('coronaviruspandemic', 0.617009699344635), ('coronacrisis', 0.6096126437187195), ('disease', 0.6022200584411621), ('coronavirusindia', 0.6005639433860779), ('coronavirusoutbreak', 0.5993783473968506), ('stayhome', 0.5993613600730896)]</code></pre>
</div>
</div>
<div id="cell-146" class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w2v.wv.most_similar(<span class="st">'pay'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('paying', 0.8663848638534546), ('paid', 0.8542090654373169), ('bills', 0.8490726351737976), ('hazard', 0.8444725871086121), ('wage', 0.8366014361381531), ('minimum', 0.8074957132339478), ('rent', 0.8000009059906006), ('benefits', 0.7753042578697205), ('wages', 0.7469033598899841), ('laid', 0.7415756583213806)]</code></pre>
</div>
</div>
<div id="cell-147" class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb197"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w2v.wv.most_similar(<span class="st">'vaccine'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('bucks', 0.925228476524353), ('bathandbodyworks', 0.909714937210083), ('skin', 0.9096536636352539), ('uganda', 0.9068837761878967), ('apnea', 0.9067789316177368), ('vera', 0.9060773253440857), ('pcs', 0.9012918472290039), ('poo', 0.9008862376213074), ('lord', 0.9000052809715271), ('bubbles', 0.8995580077171326)]</code></pre>
</div>
</div>
<div id="cell-148" class="cell" data-execution_count="157">
<div class="sourceCode cell-code" id="cb199"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>w2v.wv.most_similar(<span class="st">"india"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('nigeria', 0.8307377099990845),
 ('crash', 0.8283155560493469),
 ('oilprice', 0.8025673627853394),
 ('bitcoin', 0.7994877099990845),
 ('falling', 0.7933363914489746),
 ('slump', 0.7931302785873413),
 ('epidemic', 0.7925467491149902),
 ('oott', 0.7852712869644165),
 ('collapse', 0.7840865850448608),
 ('asia', 0.7806269526481628)]</code></pre>
</div>
</div>
<div id="cell-149" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb201"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a>w2v.wv.doesnt_match([<span class="st">'grocery'</span>, <span class="st">'covid'</span>, <span class="st">'vaccine'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'grocery'</code></pre>
</div>
</div>
<div id="cell-150" class="cell" data-execution_count="147">
<div class="sourceCode cell-code" id="cb203"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a>w2v.wv.most_similar(w2v.wv[<span class="st">'hospital'</span>] <span class="op">+</span> w2v.wv[<span class="st">'covid'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('unsung', 0.78270423412323),
 ('frontlines', 0.7687802910804749),
 ('capes', 0.735917866230011),
 ('covid', 0.7318629622459412),
 ('thankyou', 0.7203244566917419),
 ('frontline', 0.7140740156173706),
 ('essentialworkers', 0.7127604484558105),
 ('exposed', 0.7095414996147156),
 ('savelives', 0.7069511413574219),
 ('heroes', 0.7049961090087891)]</code></pre>
</div>
</div>
<div id="cell-151" class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb205"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a>w2v.wv.most_similar(w2v.wv[<span class="st">'covid'</span>] <span class="op">+</span> w2v.wv[<span class="st">'shopping'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('shopping', 0.8625062704086304),
 ('ordering', 0.7435927391052246),
 ('orders', 0.6714648604393005),
 ('book', 0.6348423957824707),
 ('shop', 0.6214817762374878),
 ('deliveries', 0.6186297535896301),
 ('platforms', 0.6042692065238953),
 ('anxiety', 0.600202202796936),
 ('homedelivery', 0.593807578086853),
 ('pickup', 0.5881509780883789)]</code></pre>
</div>
</div>
<div id="cell-152" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb207"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(w2v.wv.key_to_index)[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['covid',
 'coronavirus',
 'prices',
 'food',
 'supermarket',
 'store',
 'grocery',
 's',
 'people',
 'amp']</code></pre>
</div>
</div>
<div id="cell-153" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb209"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb209-1"><a href="#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb209-2"><a href="#cb209-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb209-3"><a href="#cb209-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb209-4"><a href="#cb209-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-5"><a href="#cb209-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> display_pca(model, words<span class="op">=</span><span class="va">None</span>, sample<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb209-6"><a href="#cb209-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> words<span class="op">==</span><span class="va">None</span>:</span>
<span id="cb209-7"><a href="#cb209-7" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> np.random.choice(<span class="bu">list</span>(w2v.wv.key_to_index), sample)</span>
<span id="cb209-8"><a href="#cb209-8" aria-hidden="true" tabindex="-1"></a>    word_vectors <span class="op">=</span> np.array([model.wv[word] <span class="cf">for</span> word <span class="kw">in</span> words])</span>
<span id="cb209-9"><a href="#cb209-9" aria-hidden="true" tabindex="-1"></a>    twodim <span class="op">=</span> PCA().fit_transform(word_vectors)[:,:<span class="dv">2</span>]</span>
<span id="cb209-10"><a href="#cb209-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-11"><a href="#cb209-11" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb209-12"><a href="#cb209-12" aria-hidden="true" tabindex="-1"></a>    plt.scatter(twodim[:,<span class="dv">0</span>], twodim[:,<span class="dv">1</span>], edgecolors<span class="op">=</span><span class="st">'k'</span>, c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb209-13"><a href="#cb209-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, (x,y) <span class="kw">in</span> <span class="bu">zip</span>(words, twodim):</span>
<span id="cb209-14"><a href="#cb209-14" aria-hidden="true" tabindex="-1"></a>        plt.text(x<span class="op">+</span><span class="fl">0.05</span>, y<span class="op">+</span><span class="fl">0.05</span>, word)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-154" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb210"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a>display_pca(w2v,[<span class="st">'coronavirus'</span>, <span class="st">'covid'</span>, <span class="st">'virus'</span>, <span class="st">'corona'</span>,<span class="st">'disease'</span>, <span class="st">'saudiarabia'</span>,  <span class="st">'doctor'</span>, <span class="st">'hospital'</span>, <span class="st">'pakistan'</span>, <span class="st">'kenya'</span>, <span class="st">'pay'</span>, <span class="st">'paying'</span>, <span class="st">'paid'</span>, <span class="st">'wages'</span>, <span class="st">'raise'</span>, <span class="st">'bills'</span>, <span class="st">'rent'</span>, <span class="st">'charge'</span>, <span class="st">'india'</span>, <span class="st">'lockdown'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NLP_files/figure-html/cell-105-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-155" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb211"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a>display_pca(w2v,<span class="bu">list</span>(w2v.wv.key_to_index)[:<span class="dv">40</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NLP_files/figure-html/cell-106-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-156" class="cell">
<div class="sourceCode cell-code" id="cb212"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>w2v.save(<span class="st">"/media/naman/NewVolume/Ubuntu/temp/gensim_data/ourmodel.model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-157" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb213"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec.load(<span class="st">"/media/naman/NewVolume/Ubuntu/temp/gensim_data/ourmodel.model"</span>)</span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a>model.train([[<span class="st">"hello"</span>, <span class="st">"world"</span>]], total_examples<span class="op">=</span><span class="dv">1</span>, epochs<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(2, 2)</code></pre>
</div>
</div>
<div id="cell-158" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb215"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb215-1"><a href="#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.wv.key_to_index)[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['covid',
 'coronavirus',
 'prices',
 'food',
 'supermarket',
 'store',
 'grocery',
 's',
 'people',
 'amp']</code></pre>
</div>
</div>
</section>
</section>
<section id="exporting-pre-trained-embeddings-savingloading-our-model" class="level2">
<h2 class="anchored" data-anchor-id="exporting-pre-trained-embeddings-savingloading-our-model">Exporting pre trained embeddings + Saving/Loading our model</h2>
<p>Gensim comes with several already pre-trained models, in the Gensim-data repository. We can import the downloader from the gensim library. We can use the following method to print the list of pre-trained models trained on large datasets available to us. This also includes models like GloVe and fastext other than word2vec.</p>
<div id="cell-160" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb217"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader</span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(gensim.downloader.info()[<span class="st">'models'</span>].keys()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']</code></pre>
</div>
</div>
<div id="cell-161" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb219"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a>model4 <span class="op">=</span> gensim.downloader.load(<span class="st">'glove-wiki-gigaword-300'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[==================================================] 100.0% 376.1/376.1MB downloaded</code></pre>
</div>
</div>
<div id="cell-162" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb221"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb221-1"><a href="#cb221-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using a pretrained model</span></span>
<span id="cb221-2"><a href="#cb221-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec, KeyedVectors</span>
<span id="cb221-3"><a href="#cb221-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KeyedVectors.load_word2vec_format(<span class="st">"/media/naman/NewVolume/Ubuntu/temp/gensim_data/word2vec-google-news-300/word2vec-google-news-300.gz"</span>, binary<span class="op">=</span><span class="va">True</span>, limit<span class="op">=</span><span class="dv">500000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-163" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb222"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(model[<span class="st">'india'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('india', 1.0000001192092896),
 ('indian', 0.6967039704322815),
 ('usa', 0.6836211085319519),
 ('pakistan', 0.6815167665481567),
 ('chennai', 0.6675504446029663),
 ('america', 0.6589399576187134),
 ('sri_lanka', 0.64982008934021),
 ('canada', 0.6490967869758606),
 ('australia', 0.6368584036827087),
 ('mexico', 0.6239137649536133)]</code></pre>
</div>
</div>
<div id="cell-164" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb224"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb224-1"><a href="#cb224-1" aria-hidden="true" tabindex="-1"></a>model.most_similar(model[<span class="st">'india'</span>] <span class="op">-</span> <span class="fl">1.5</span> <span class="op">*</span> model[<span class="st">'delhi'</span>] <span class="op">+</span> model[<span class="st">'berlin'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('berlin', 0.5434370636940002),
 ('german', 0.3894408047199249),
 ('ca', 0.34584057331085205),
 ('germany', 0.34512007236480713),
 ('Deutsches', 0.33249467611312866),
 ('nv', 0.32953745126724243),
 ('european', 0.3244131803512573),
 ('swedish', 0.3223515450954437),
 ('nero', 0.3210062086582184),
 ('gnu', 0.3156096339225769)]</code></pre>
</div>
</div>
<section id="what-is-text-classification" class="level3">
<h3 class="anchored" data-anchor-id="what-is-text-classification"><strong>What is Text Classification?</strong></h3>
<p><strong>Text classification</strong> is the process of automatically assigning predefined categories (labels) to text based on its content. It’s a fundamental task in <strong>Natural Language Processing (NLP)</strong> and is used to organize, filter, and analyze text data efficiently.</p>
</section>
<section id="how-does-it-work" class="level3">
<h3 class="anchored" data-anchor-id="how-does-it-work"><strong>How Does It Work?</strong></h3>
<ol type="1">
<li><strong>Input</strong>: Raw text (e.g., an email, tweet, or product review).<br>
</li>
<li><strong>Processing</strong>: The text is cleaned (remove stopwords, punctuation) and converted into numerical features (e.g., word embeddings).<br>
</li>
<li><strong>Model Training</strong>: A machine learning model (like Naive Bayes, LSTM, or BERT) learns patterns from labeled examples.<br>
</li>
<li><strong>Prediction</strong>: The model assigns a category to new, unseen text.</li>
</ol>
</section>
<section id="real-life-examples-of-text-classification" class="level3">
<h3 class="anchored" data-anchor-id="real-life-examples-of-text-classification"><strong>Real-Life Examples of Text Classification</strong></h3>
<section id="email-spam-filtering" class="level4">
<h4 class="anchored" data-anchor-id="email-spam-filtering">1️⃣ <strong>Email Spam Filtering</strong></h4>
<ul>
<li><strong>Goal</strong>: Classify emails as <strong>“spam” or “not spam”</strong>.<br>
</li>
<li><strong>How it works</strong>:
<ul>
<li>The model analyzes keywords (e.g., “free offer,” “lottery winner”) and sender reputation.<br>
</li>
<li>Example: Gmail’s spam filter uses NLP to block phishing emails.</li>
</ul></li>
</ul>
</section>
<section id="sentiment-analysis" class="level4">
<h4 class="anchored" data-anchor-id="sentiment-analysis">2️⃣ <strong>Sentiment Analysis</strong></h4>
<ul>
<li><strong>Goal</strong>: Detect emotions in text (positive/negative/neutral).<br>
</li>
<li><strong>Applications</strong>:
<ul>
<li><strong>Twitter/X</strong>: Classifies tweets about a product as 😊 (positive) or 😠 (negative).<br>
</li>
<li><strong>Customer Support</strong>: Automatically flags angry complaints for priority handling.</li>
</ul></li>
</ul>
</section>
<section id="news-categorization" class="level4">
<h4 class="anchored" data-anchor-id="news-categorization">3️⃣ <strong>News Categorization</strong></h4>
<ul>
<li><strong>Goal</strong>: Tag articles by topic (e.g., sports, politics, tech).<br>
</li>
<li><strong>Example</strong>:
<ul>
<li>BBC News uses AI to auto-categorize breaking news into sections like <strong>“World”</strong> or <strong>“Business”</strong>.</li>
</ul></li>
</ul>
</section>
<section id="intent-detection-in-chatbots" class="level4">
<h4 class="anchored" data-anchor-id="intent-detection-in-chatbots">4️⃣ <strong>Intent Detection in Chatbots</strong></h4>
<ul>
<li><strong>Goal</strong>: Understand user queries (e.g., “refund request” vs.&nbsp;“track order”).<br>
</li>
<li><strong>Example</strong>:
<ul>
<li>Siri/Alexa classify voice commands into actions like <strong>“play music”</strong> or <strong>“set alarm”</strong>.</li>
</ul></li>
</ul>
</section>
<section id="medical-diagnosis-from-patient-notes" class="level4">
<h4 class="anchored" data-anchor-id="medical-diagnosis-from-patient-notes">5️⃣ <strong>Medical Diagnosis from Patient Notes</strong></h4>
<ul>
<li><strong>Goal</strong>: Predict diseases from symptoms described in text.<br>
</li>
<li><strong>Example</strong>:
<ul>
<li>AI models scan doctor’s notes to classify conditions like <strong>“diabetes”</strong> or <strong>“COVID-19”</strong>.</li>
</ul></li>
</ul>
</section>
<section id="legal-document-tagging" class="level4">
<h4 class="anchored" data-anchor-id="legal-document-tagging">6️⃣ <strong>Legal Document Tagging</strong></h4>
<ul>
<li><strong>Goal</strong>: Sort contracts into categories (e.g., <strong>“NDA”</strong>, <strong>“Lease Agreement”</strong>).<br>
</li>
<li><strong>Used by</strong>: Law firms to automate document management.</li>
</ul>
</section>
<section id="social-media-moderation" class="level4">
<h4 class="anchored" data-anchor-id="social-media-moderation">7️⃣ <strong>Social Media Moderation</strong></h4>
<ul>
<li><strong>Goal</strong>: Detect <strong>hate speech, fake news, or NSFW content</strong>.<br>
</li>
<li><strong>Example</strong>:
<ul>
<li>Facebook uses NLP to flag harmful posts automatically.</li>
</ul></li>
</ul>
</section>
</section>
<section id="common-algorithms-for-text-classification" class="level3">
<h3 class="anchored" data-anchor-id="common-algorithms-for-text-classification"><strong>Common Algorithms for Text Classification</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 42%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Algorithm</strong></th>
<th><strong>Best For</strong></th>
<th><strong>Example Use Case</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Naive Bayes</strong></td>
<td>Simple, fast, small datasets</td>
<td>Spam detection</td>
</tr>
<tr class="even">
<td><strong>Logistic Regression</strong></td>
<td>Interpretable, linear problems</td>
<td>Sentiment analysis</td>
</tr>
<tr class="odd">
<td><strong>SVM</strong></td>
<td>High accuracy, medium datasets</td>
<td>News categorization</td>
</tr>
<tr class="even">
<td><strong>LSTM/RNN</strong></td>
<td>Sequential text (e.g., reviews)</td>
<td>Intent detection</td>
</tr>
<tr class="odd">
<td><strong>BERT/Transformers</strong></td>
<td>State-of-the-art, complex tasks</td>
<td>Medical diagnosis from notes</td>
</tr>
</tbody>
</table>
<p>We can use APIs too for NLP tasks like AWS which can classify based on our text sent to them.</p>
</section>
<section id="why-is-text-classification-important" class="level3">
<h3 class="anchored" data-anchor-id="why-is-text-classification-important"><strong>Why is Text Classification Important?</strong></h3>
<ul>
<li><strong>Saves time</strong>: Automates manual tagging (e.g., sorting 10,000 customer emails).<br>
</li>
<li><strong>Improves accuracy</strong>: Reduces human bias in labeling.<br>
</li>
<li><strong>Scalability</strong>: Handles massive text data (e.g., all tweets per second).</li>
</ul>
</section>
</section>
<section id="making-a-sentiment-analysis-model-using-ml-word2vec" class="level2">
<h2 class="anchored" data-anchor-id="making-a-sentiment-analysis-model-using-ml-word2vec">Making a sentiment analysis model using ML word2vec</h2>
<div id="cell-167" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb226"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb226-2"><a href="#cb226-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb226-3"><a href="#cb226-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb226-4"><a href="#cb226-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb226-5"><a href="#cb226-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-168" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb227"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb227-1"><a href="#cb227-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"/media/naman/NewVolume/Data_Science/csv_data/Corona_NLP_train.csv"</span>, encoding<span class="op">=</span><span class="st">'latin1'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-169" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb228"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb228-1"><a href="#cb228-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">UserName</th>
<th data-quarto-table-cell-role="th">ScreenName</th>
<th data-quarto-table-cell-role="th">Location</th>
<th data-quarto-table-cell-role="th">TweetAt</th>
<th data-quarto-table-cell-role="th">OriginalTweet</th>
<th data-quarto-table-cell-role="th">Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>3799</td>
<td>48751</td>
<td>London</td>
<td>16-03-2020</td>
<td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>
<td>Neutral</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>3800</td>
<td>48752</td>
<td>UK</td>
<td>16-03-2020</td>
<td>advice Talk to your neighbours family to excha...</td>
<td>Positive</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>3801</td>
<td>48753</td>
<td>Vagabonds</td>
<td>16-03-2020</td>
<td>Coronavirus Australia: Woolworths to give elde...</td>
<td>Positive</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3802</td>
<td>48754</td>
<td>NaN</td>
<td>16-03-2020</td>
<td>My food stock is not the only one which is emp...</td>
<td>Positive</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>3803</td>
<td>48755</td>
<td>NaN</td>
<td>16-03-2020</td>
<td>Me, ready to go at supermarket during the #COV...</td>
<td>Extremely Negative</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="cell-170" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb229"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb229-1"><a href="#cb229-1" aria-hidden="true" tabindex="-1"></a>df.OriginalTweet[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8'</code></pre>
</div>
</div>
<div id="cell-171" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb231"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb231-1"><a href="#cb231-1" aria-hidden="true" tabindex="-1"></a>df.Sentiment.unique()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',
       'Extremely Positive'], dtype=object)</code></pre>
</div>
</div>
<div id="cell-172" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb233"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb233-1"><a href="#cb233-1" aria-hidden="true" tabindex="-1"></a>df.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(41157, 6)</code></pre>
</div>
</div>
<div id="cell-173" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb235"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb235-1"><a href="#cb235-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb235-2"><a href="#cb235-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb235-3"><a href="#cb235-3" aria-hidden="true" tabindex="-1"></a>stop_wrds <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-174" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb236"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb236-1"><a href="#cb236-1" aria-hidden="true" tabindex="-1"></a>texts<span class="op">=</span>[]</span>
<span id="cb236-2"><a href="#cb236-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df)):</span>
<span id="cb236-3"><a href="#cb236-3" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'@\w+'</span>, <span class="st">''</span>, df[<span class="st">'OriginalTweet'</span>][i])</span>
<span id="cb236-4"><a href="#cb236-4" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.lower()</span>
<span id="cb236-5"><a href="#cb236-5" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> text.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop_wrds]</span>
<span id="cb236-6"><a href="#cb236-6" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="st">" "</span>.join(text) </span>
<span id="cb236-7"><a href="#cb236-7" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'https?://\S+'</span>, <span class="st">''</span>, text)</span>
<span id="cb236-8"><a href="#cb236-8" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'[^a-zA-Z]'</span>,<span class="st">' '</span>,text)</span>
<span id="cb236-9"><a href="#cb236-9" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.split()</span>
<span id="cb236-10"><a href="#cb236-10" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="st">" "</span>.join(text)    </span>
<span id="cb236-11"><a href="#cb236-11" aria-hidden="true" tabindex="-1"></a>    texts.append(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-175" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb237"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb237-1"><a href="#cb237-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> pd.DataFrame(texts, columns<span class="op">=</span>[<span class="st">'text'</span>])</span>
<span id="cb237-2"><a href="#cb237-2" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">'length'</span>] <span class="op">=</span> dataset[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x.split()))</span>
<span id="cb237-3"><a href="#cb237-3" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">'Sentiment'</span>] <span class="op">=</span> df[<span class="st">'Sentiment'</span>]</span>
<span id="cb237-4"><a href="#cb237-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(41157, 3)</code></pre>
</div>
</div>
<div id="cell-176" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb239"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb239-1"><a href="#cb239-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.query(<span class="st">"length&gt;3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-177" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb240"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb240-1"><a href="#cb240-1" aria-hidden="true" tabindex="-1"></a>dataset.sample(<span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">length</th>
<th data-quarto-table-cell-role="th">Sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">5669</td>
<td>church jesus christ latter day saints closes d...</td>
<td>16</td>
<td>Neutral</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">31819</td>
<td>social distancing accelerate trend toward home...</td>
<td>9</td>
<td>Neutral</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6168</td>
<td>states classifying grocery store workers emerg...</td>
<td>11</td>
<td>Positive</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">26677</td>
<td>i m extremely grateful covid hit time video ch...</td>
<td>18</td>
<td>Positive</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">35117</td>
<td>pandemic ends economic recovery slower compare...</td>
<td>23</td>
<td>Extremely Negative</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">33039</td>
<td>new consumer covid behaviours outlast pandemic...</td>
<td>12</td>
<td>Neutral</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2847</td>
<td>man hoarded bottles hand sanitizer nothing wro...</td>
<td>23</td>
<td>Positive</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="cell-178" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb241"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb241-1"><a href="#cb241-1" aria-hidden="true" tabindex="-1"></a>dataset.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(40831, 3)</code></pre>
</div>
</div>
<div id="cell-179" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb243"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb243-1"><a href="#cb243-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb243-2"><a href="#cb243-2" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> <span class="bu">list</span>(dataset[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.split(<span class="st">' '</span>)))</span>
<span id="cb243-3"><a href="#cb243-3" aria-hidden="true" tabindex="-1"></a>w2v <span class="op">=</span> Word2Vec(sentences, vector_size<span class="op">=</span><span class="dv">70</span>, workers<span class="op">=</span><span class="dv">4</span>, sg<span class="op">=</span><span class="dv">0</span>, negative<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb243-4"><a href="#cb243-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb243-5"><a href="#cb243-5" aria-hidden="true" tabindex="-1"></a><span class="co"># printing words vocabulary</span></span>
<span id="cb243-6"><a href="#cb243-6" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">list</span>(w2v.wv.key_to_index)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-180" class="cell">
<div class="sourceCode cell-code" id="cb244"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb244-1"><a href="#cb244-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1</code></pre>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>