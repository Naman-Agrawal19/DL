{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why Regularization in ANN?**\n",
    "Regularization in Artificial Neural Networks (ANNs) is essential to prevent **overfitting**, which occurs when the model learns the noise in the training data instead of the actual pattern. Overfitting leads to poor generalization, meaning the model performs well on training data but fails on unseen data.\n",
    "\n",
    "Regularization **adds constraints or penalties to the modelâ€™s parameters** to reduce the complexity of the learned function and improve generalization.\n",
    "\n",
    "\n",
    "### **How Do We Regularize an ANN?**\n",
    "There are multiple ways to regularize an ANN:\n",
    "1. **L1 and L2 Regularization** (also known as Lasso and Ridge regression for linear models)\n",
    "   - **L1 (Lasso) Regularization**: Adds a penalty proportional to the absolute value of weights.\n",
    "   - **L2 (Ridge) Regularization**: Adds a penalty proportional to the square of weights.\n",
    "   - **Elastic Net**: A combination of L1 and L2.\n",
    "\n",
    "2. **Dropout**  \n",
    "   - Randomly drops neurons during training to prevent reliance on specific neurons.\n",
    "\n",
    "3. **Early Stopping**  \n",
    "   - Stops training when validation loss stops decreasing, preventing overfitting.\n",
    "\n",
    "4. **Data Augmentation**  \n",
    "   - Increases training data artificially (especially in image processing).\n",
    "\n",
    "5. **Batch Normalization**  \n",
    "   - Normalizes activations during training to reduce dependency on initial weights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mathematics Behind Regularization**\n",
    "#### **Example: Small ANN with One Hidden Layer**\n",
    "Let's consider a small ANN with:\n",
    "- **Input layer**: 2 neurons\n",
    "- **Hidden layer**: 2 neurons (ReLU activation)\n",
    "- **Output layer**: 1 neuron (Sigmoid activation)\n",
    "\n",
    "Let:\n",
    "- $X = [x_1, x_2]$ be the input features.\n",
    "- $W^{(1)}$ and $W^{(2)}$ be weight matrices.\n",
    "- $b^{(1)}$ and $b^{(2)}$ be bias terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Inputs\n",
    "        direction LR\n",
    "        style Inputs fill:#2e7c92,stroke:#64b5f6,stroke-width:2px\n",
    "        x1[x<sub>11</sub>]\n",
    "        x2[x<sub>12</sub>]\n",
    "    end\n",
    "    \n",
    "    subgraph \"hidden-layer\" [\"Hidden \\nLayer 1\"]\n",
    "        direction LR\n",
    "        style hidden-layer fill:#2c5c36,stroke:#85ff9f,stroke-width:2px\n",
    "        h1(b<sub>11</sub>)\n",
    "        h2(b<sub>12</sub>)\n",
    "    end\n",
    "    \n",
    "    subgraph Output\n",
    "        direction LR\n",
    "        style Output fill:#e78383,stroke:#f8cc52,stroke-width:2px\n",
    "        y(b<sub>21</sub>)\n",
    "    end\n",
    "\n",
    "\n",
    "    x1 --> |w<sub>11</sub><sup>1</sup>| h1\n",
    "    x1 --> |w<sub>12</sub><sup>1</sup>| h2\n",
    "\n",
    "    x2 --> |w<sub>21</sub><sup>1</sup>| h1\n",
    "    x2 --> |w<sub>22</sub><sup>1</sup>| h2\n",
    "\n",
    "    h1 --> |w<sub>11</sub><sup>2</sup>| y\n",
    "    h2 --> |w<sub>21</sub><sup>2</sup>| y\n",
    "\n",
    "    y --> Out([\"Prediction\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Forward Propagation Equations**\n",
    "1. Compute hidden layer activation:\n",
    "   $$\n",
    "   Z^{(1)} = W^{(1)}X + b^{(1)}\n",
    "   $$\n",
    "   $$\n",
    "   A^{(1)} = ReLU(Z^{(1)})\n",
    "   $$\n",
    "2. Compute output layer activation:\n",
    "   $$\n",
    "   Z^{(2)} = W^{(2)}A^{(1)} + b^{(2)}\n",
    "   $$\n",
    "   $$\n",
    "   \\hat{y} = \\sigma(Z^{(2)})\n",
    "   $$\n",
    "\n",
    "##### **Loss Function (Cross-Entropy for Binary Classification)**\n",
    "$$\n",
    "L = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log \\hat{y_i} + (1 - y_i) \\log (1 - \\hat{y_i}) \\right]\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Adding Regularization (L2)**\n",
    "L2 regularization (also called **Weight Decay**) penalizes large weights by adding a term to the loss function:\n",
    "\n",
    "$$\n",
    "L_{reg} = L + \\frac{\\lambda}{2m} \\sum ||W||^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$ is the regularization strength (hyperparameter).\n",
    "- $m$ is the number of training examples.\n",
    "- $||W||^2$ is the sum of squared weights.\n",
    "\n",
    "\n",
    "To perform gradient descent, we need to compute the derivative of $L_{reg}$ w.r.t. $W$.\n",
    "$$\n",
    "\\frac {\\partial L_{reg}}{\\partial W} = \\frac{\\partial L}{\\partial W} + \\frac{\\partial}{\\partial W} \\left( \\frac{\\lambda}{2m} \\sum W^2 \\right)\n",
    "$$\n",
    "\n",
    "##### **First term: Gradient of standard loss $L$**\n",
    "The derivative of the original loss function $L$ w.r.t. $W$ is:\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "##### **Second term: Gradient of regularization term**\n",
    "The derivative of the regularization term:\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "Using the power rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} W^2 = 2W\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W} \\left( \\frac{\\lambda}{2m} \\sum W^2 \\right) = \\frac{\\lambda}{2m} \\cdot 2W = \\frac{\\lambda}{m} W\n",
    "$$\n",
    "\n",
    "#### **Step 3: Update Rule in Gradient Descent**\n",
    "Now, the updated weight equation in gradient descent becomes:\n",
    "\n",
    "$$\n",
    "W = W - \\alpha \\left( \\frac{\\partial L}{\\partial W} + \\frac{\\lambda}{m} W \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the learning rate,\n",
    "- $\\frac{\\partial L}{\\partial W}$ is the gradient of the original loss function,\n",
    "- $\\frac{\\lambda}{m} W$ is the regularization term.\n",
    "\n",
    "\n",
    "##### **Effect on Gradient Descent**\n",
    "Regularization modifies weight updates in gradient descent:\n",
    "\n",
    "$$\n",
    "W = W - \\alpha \\left( \\frac{\\partial L}{\\partial W} + \\frac{\\lambda}{m} W \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $\\frac{\\partial L}{\\partial W}$ is the usual gradient.\n",
    "- $\\frac{\\lambda}{m} W$ is the regularization term, which **shrinks the weights**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Derivation of $\\frac{\\lambda}{m} W$ Term**\n",
    "\n",
    "#### **Step 1: Define the Regularized Loss Function**\n",
    "The standard loss function (without regularization) for a classification task (e.g., binary cross-entropy) is:\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log \\hat{y_i} + (1 - y_i) \\log (1 - \\hat{y_i}) \\right]\n",
    "$$\n",
    "\n",
    "Now, we **add L2 regularization**, which penalizes large weights by adding a term:\n",
    "\n",
    "$$\n",
    "L_{reg} = L + \\frac{\\lambda}{2m} \\sum ||W||^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$ is the regularization strength (hyperparameter),\n",
    "- $m$ is the number of training examples,\n",
    "- $||W||^2 = \\sum W^2$ is the sum of squared weights.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "- Regularization prevents overfitting by **penalizing large weights**.\n",
    "- L2 regularization (Weight Decay) adds **$||W||^2$** to the loss function.\n",
    "- L1 regularization forces some weights to be **zero**, leading to sparsity.\n",
    "- Dropout **randomly deactivates neurons** during training to prevent reliance on specific features.\n",
    "- Regularization modifies weight updates to **prevent extreme values**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
